{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install evaluate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT_FlsY0JKVd",
        "outputId": "50dcbefd-a5ef-4d0d-8e27-63c7b09cdffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 evaluate-0.4.3 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UnwwO3KJKb1",
        "outputId": "717f404b-e992-4c18-d125-2bb7da8891ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import evaluate\n",
        "import argparse\n",
        "import random\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "import re\n",
        "import traceback\n",
        "\n",
        "# CRITICAL: Set environment variables BEFORE importing PyTorch\n",
        "os.environ[\"FORCE_TORCH_SDPA_KERNEL\"] = \"0\"\n",
        "os.environ[\"PYTORCH_DISABLE_SDPA\"] = \"1\"\n",
        "os.environ[\"PYTORCH_NO_CUDA_MEMORY_EFFICIENCY_WARNING\"] = \"1\"\n",
        "\n",
        "# Data Handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "\n",
        "# Deep Learning - PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Transformers Library\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    MBartForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    set_seed as transformers_set_seed,\n",
        "    GenerationConfig\n",
        ")\n",
        "\n",
        "# Evaluation Metric\n",
        "import sacrebleu\n",
        "\n",
        "# Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab environment.\")\n",
        "\n",
        "# For custom collator\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "from transformers.utils import PaddingStrategy\n",
        "\n",
        "# === Configuration & Argument Parsing ===\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# === Global Variables ===\n",
        "global_tokenizer = None  # Will be set after tokenizer initialization"
      ],
      "metadata": {
        "id": "2elTnbleBCVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Train mBART model for Saudi Sign Language Translation\")\n",
        "\n",
        "    # Update paths for Colab environment\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"/content/drive/MyDrive/saudi/saudi-signfor-all-competition\",\n",
        "                      help=\"Directory containing H5/CSV/JSON data files (Colab Path)\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"/content/drive/MyDrive/saudi/output\",\n",
        "                      help=\"Directory to save trained models and logs (Colab Path)\")\n",
        "    parser.add_argument(\"--results_dir\", type=str, default=\"/content/drive/MyDrive/saudi/results\",\n",
        "                      help=\"Directory to save evaluation results and predictions (Colab Path)\")\n",
        "\n",
        "    # File names exactly as provided\n",
        "    parser.add_argument(\"--train_h5_file\", type=str, default=\"SSL.keypoints.train_signers_train_sentences.0.h5\",\n",
        "                       help=\"Filename of training H5 file (within data_dir)\")\n",
        "    parser.add_argument(\"--train_csv_file\", type=str, default=\"SSL.keypoints.train_signers_train_sentences.csv\",\n",
        "                       help=\"Filename of training CSV file (within data_dir)\")\n",
        "    parser.add_argument(\"--test_h5_file\", type=str, default=\"SSL.keypoints.test_signers_test_sentences.h5\",\n",
        "                       help=\"Filename of test H5 file (within data_dir)\")\n",
        "    parser.add_argument(\"--test_json_file\", type=str, default=\"SSL.keypoints.test_signers_test_sentences.json\",\n",
        "                       help=\"Filename of test JSON file (within data_dir)\")\n",
        "\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed for reproducibility\")\n",
        "    parser.add_argument(\"--mbart_model_name\", type=str, default=\"facebook/mbart-large-50-many-to-many-mmt\", help=\"Pre-trained mBART model name\")\n",
        "    parser.add_argument(\"--target_lang\", type=str, default=\"ar_AR\", help=\"mBART language code for Arabic\")\n",
        "    parser.add_argument(\"--feature_dim\", type=int, default=208, help=\"Keypoint feature dimension (check H5 files)\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=15, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--warmup_steps\", type=int, default=500, help=\"Number of warmup steps for learning rate scheduler\")\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"Weight decay for optimizer\")\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log training info every N steps\")\n",
        "    parser.add_argument(\"--save_total_limit\", type=int, default=2, help=\"Maximum number of checkpoints to save\")\n",
        "    parser.add_argument(\"--fp16\", action=\"store_true\", default=True, help=\"Enable FP16 training\")\n",
        "\n",
        "    # A100 GPU optimizations\n",
        "    parser.add_argument(\"--gradient_checkpointing\", action=\"store_true\", default=False, help=\"Enable gradient checkpointing\")\n",
        "    parser.add_argument(\"--max_seq_length\", type=int, default=512, help=\"Max sequence length for keypoints\")\n",
        "    parser.add_argument(\"--max_label_length\", type=int, default=128, help=\"Max sequence length for target Arabic text\")\n",
        "    parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"Training batch size per device\")\n",
        "    parser.add_argument(\"--eval_batch_size\", type=int, default=16, help=\"Evaluation batch size per device\")\n",
        "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, help=\"Number of steps to accumulate gradients\")\n",
        "    parser.add_argument(\"--num_beams\", type=int, default=5, help=\"Number of beams for generation during evaluation\")\n",
        "\n",
        "    # Use parse_known_args to ignore extra arguments from environments like Jupyter\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    if unknown:\n",
        "        print(f\"Ignoring unrecognized arguments: {unknown}\")\n",
        "\n",
        "    # Calculate effective batch size\n",
        "    num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "    effective_batch_size = args.train_batch_size * args.gradient_accumulation_steps * num_devices\n",
        "    print(f\"Effective batch size: {effective_batch_size} (train_bs={args.train_batch_size} * grad_accum={args.gradient_accumulation_steps} * devices={num_devices})\")\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(args.data_dir, exist_ok=True)\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "    os.makedirs(args.results_dir, exist_ok=True)\n",
        "\n",
        "    return args\n"
      ],
      "metadata": {
        "id": "yVjeD1ZrBIcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def set_seed(seed_value):\n",
        "    \"\"\"Set random seed for reproducibility across libraries.\"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "    transformers_set_seed(seed_value)\n",
        "\n",
        "def check_and_prompt_for_file(file_path, file_type):\n",
        "    \"\"\"Check if a file exists and prompt for an alternative path if not found.\"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"Found {file_type} file: {file_path}\")\n",
        "        return file_path\n",
        "\n",
        "    print(f\"ERROR: {file_type} file not found at {file_path}\")\n",
        "\n",
        "    user_input = None\n",
        "    while True:\n",
        "        user_input = input(f\"Please enter an alternative path for the {file_type} file or type 'skip' to bypass: \")\n",
        "\n",
        "        if user_input.lower() == 'skip':\n",
        "            print(f\"Skipping {file_type} file\")\n",
        "            return None\n",
        "\n",
        "        if os.path.exists(user_input):\n",
        "            print(f\"Found {file_type} file at: {user_input}\")\n",
        "            return user_input\n",
        "        else:\n",
        "            print(f\"File not found at: {user_input}\")"
      ],
      "metadata": {
        "id": "dhIGitKpBIfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kH1SNdbyCiYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DgaDMvkJCszT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hXo5gCOQCs1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qmLYBsyrCs3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preload_h5_info(file_path):\n",
        "    \"\"\"\n",
        "    Scans an H5 file and extracts key information:\n",
        "    - List of valid keys\n",
        "    - Mapping of keys to sequence lengths\n",
        "    - Feature dimension\n",
        "    \"\"\"\n",
        "    keys = []\n",
        "    lengths = {}\n",
        "    feature_dim = None\n",
        "    if not file_path or not os.path.exists(file_path):\n",
        "        print(f\"Error: H5 path {file_path} invalid or not found.\")\n",
        "        return keys, lengths, feature_dim\n",
        "    try:\n",
        "        with h5py.File(file_path, \"r\") as f:\n",
        "            print(f\"Scanning HDF5 {file_path}...\")\n",
        "            valid_keys = list(f.keys())\n",
        "            for key in tqdm(valid_keys, desc=\"Preloading HDF5 info\"):\n",
        "                try:\n",
        "                    item = f[key]\n",
        "                    seq_len = 0\n",
        "                    current_dim = 0\n",
        "\n",
        "                    # Handle potential group structure\n",
        "                    if isinstance(item, h5py.Group):\n",
        "                        data_keys = list(item.keys())\n",
        "                        if data_keys:\n",
        "                            data = item[data_keys[0]]\n",
        "                            if isinstance(data, h5py.Dataset):\n",
        "                                seq_len = data.shape[0]\n",
        "                                if len(data.shape) > 1:\n",
        "                                    current_dim = data.shape[1]\n",
        "                        else:\n",
        "                             continue # Skip empty group\n",
        "                    elif isinstance(item, h5py.Dataset):\n",
        "                        seq_len = item.shape[0]\n",
        "                        if len(item.shape) > 1:\n",
        "                            current_dim = item.shape[1]\n",
        "                    else:\n",
        "                        continue # Skip if not a group or dataset\n",
        "\n",
        "                    if seq_len > 0:\n",
        "                        keys.append(key)\n",
        "                        lengths[key] = seq_len\n",
        "                        if feature_dim is None and current_dim > 0:\n",
        "                            feature_dim = current_dim\n",
        "                        elif feature_dim is not None and current_dim != feature_dim and current_dim > 0:\n",
        "                             pass\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing key {key}: {e}\")\n",
        "\n",
        "            print(f\"Found {len(keys)} non-empty keys in HDF5.\")\n",
        "            if feature_dim is not None:\n",
        "                print(f\"Detected primary feature dimension: {feature_dim}\")\n",
        "            else:\n",
        "                print(\"Warning: Could not detect feature dimension from H5 file.\")\n",
        "\n",
        "            return keys, lengths, feature_dim\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening HDF5 {file_path}: {e}\")\n",
        "        return keys, lengths, feature_dim\n"
      ],
      "metadata": {
        "id": "ZxbatS6-ConE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_signer_sentence_ids(key):\n",
        "    \"\"\"\n",
        "    Extract signer ID and sentence ID from the key format (e.g., '00_0001').\n",
        "\n",
        "    Args:\n",
        "        key (str): The key in format 'XX_YYYY'\n",
        "\n",
        "    Returns:\n",
        "        tuple: (signer_id, sentence_id) or (None, None) if format doesn't match\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if '_' in key:\n",
        "            parts = key.split('_')\n",
        "            if len(parts) == 2:\n",
        "                signer_id = parts[0]\n",
        "                sentence_id = parts[1]\n",
        "                return signer_id, sentence_id\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None, None\n"
      ],
      "metadata": {
        "id": "-AKMihUUCopp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_validation_splits_with_strict_separation(train_h5_keys, id_to_sentence_map, train_h5_lengths, max_seq_length, seed=42):\n",
        "    \"\"\"\n",
        "    Create validation splits with STRICT separation from training data.\n",
        "    This ensures validation data is NEVER used in training.\n",
        "\n",
        "    Args:\n",
        "        train_h5_keys: List of keys from training H5 file\n",
        "        id_to_sentence_map: Mapping from keys to sentence translations\n",
        "        train_h5_lengths: Mapping from keys to sequence lengths\n",
        "        max_seq_length: Maximum sequence length to include\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of validation splits and metadata\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Creating Validation Splits with STRICT Separation ---\")\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Filter keys by length and valid translations\n",
        "    valid_keys = [key for key in train_h5_keys\n",
        "                  if key in id_to_sentence_map\n",
        "                  and id_to_sentence_map[key]\n",
        "                  and isinstance(id_to_sentence_map[key], str)\n",
        "                  and id_to_sentence_map[key].strip()\n",
        "                  and key in train_h5_lengths\n",
        "                  and train_h5_lengths[key] <= max_seq_length]\n",
        "\n",
        "    print(f\"Valid keys after filtering: {len(valid_keys)}\")\n",
        "\n",
        "    # Extract signer and sentence information\n",
        "    signer_to_keys = {}\n",
        "    sentence_to_keys = {}\n",
        "    key_info = {}\n",
        "\n",
        "    for key in valid_keys:\n",
        "        signer_id, sentence_id = extract_signer_sentence_ids(key)\n",
        "        sentence = id_to_sentence_map[key]\n",
        "\n",
        "        if signer_id is not None:\n",
        "            if signer_id not in signer_to_keys:\n",
        "                signer_to_keys[signer_id] = []\n",
        "            signer_to_keys[signer_id].append(key)\n",
        "\n",
        "        if sentence not in sentence_to_keys:\n",
        "            sentence_to_keys[sentence] = []\n",
        "        sentence_to_keys[sentence].append(key)\n",
        "\n",
        "        key_info[key] = {\n",
        "            'signer_id': signer_id,\n",
        "            'sentence_id': sentence_id,\n",
        "            'sentence': sentence\n",
        "        }\n",
        "\n",
        "    unique_signers = list(signer_to_keys.keys())\n",
        "    unique_sentences = list(sentence_to_keys.keys())\n",
        "\n",
        "    print(f\"Found {len(unique_signers)} unique signers and {len(unique_sentences)} unique sentences\")\n",
        "\n",
        "    # Create validation splits with NO OVERLAP with training\n",
        "    splits = {}\n",
        "\n",
        "    # Split 1: Standard hold-out (completely random keys)\n",
        "    train_standard, val_standard = train_test_split(\n",
        "        valid_keys, test_size=0.1, random_state=seed)\n",
        "\n",
        "    # Ensure strict separation\n",
        "    assert set(train_standard).isdisjoint(set(val_standard)), \"ERROR: Train and validation sets overlap!\"\n",
        "\n",
        "    splits['standard'] = {\n",
        "        'name': 'Standard Random Split',\n",
        "        'description': '10% of data randomly selected, completely held out from training',\n",
        "        'train_keys': train_standard,\n",
        "        'val_keys': val_standard,\n",
        "        'challenge_level': 1\n",
        "    }\n",
        "\n",
        "    print(f\"Standard split: {len(train_standard)} train, {len(val_standard)} validation samples\")\n",
        "    print(f\"Verified: Train and validation sets have NO overlap\")\n",
        "\n",
        "    # Split 2: Unseen sentences\n",
        "    if len(unique_sentences) >= 10:\n",
        "        # Hold out 10% of sentences completely\n",
        "        val_sentence_count = max(int(len(unique_sentences) * 0.1), 5)  # At least 5 sentences\n",
        "        val_sentences = set(random.sample(unique_sentences, k=val_sentence_count))\n",
        "\n",
        "        # Ensure sentences in validation are NEVER in training\n",
        "        val_unseen_sentence_keys = []\n",
        "        train_seen_sentence_keys = []\n",
        "\n",
        "        for key in valid_keys:\n",
        "            sentence = id_to_sentence_map[key]\n",
        "            if sentence in val_sentences:\n",
        "                val_unseen_sentence_keys.append(key)\n",
        "            else:\n",
        "                train_seen_sentence_keys.append(key)\n",
        "\n",
        "        # Verify strict separation\n",
        "        assert set(train_seen_sentence_keys).isdisjoint(set(val_unseen_sentence_keys)), \"ERROR: Train and unseen sentence validation sets overlap!\"\n",
        "\n",
        "        splits['unseen_sentences'] = {\n",
        "            'name': 'Unseen Sentences Split',\n",
        "            'description': f'Hold out {len(val_sentences)} complete sentences (never seen during training)',\n",
        "            'train_keys': train_seen_sentence_keys,\n",
        "            'val_keys': val_unseen_sentence_keys,\n",
        "            'challenge_level': 2,\n",
        "            'val_sentences': val_sentences  # Store these for reference\n",
        "        }\n",
        "\n",
        "        print(f\"Unseen sentences split: {len(train_seen_sentence_keys)} train, {len(val_unseen_sentence_keys)} validation samples\")\n",
        "        print(f\"Verified: Train and validation sets have NO overlap\")\n",
        "\n",
        "    # Split 3: Unseen signers\n",
        "    if len(unique_signers) >= 5:\n",
        "        # Hold out 20% of signers completely\n",
        "        val_signer_count = max(int(len(unique_signers) * 0.2), 2)  # At least 2 signers\n",
        "        val_signers = set(random.sample(unique_signers, k=val_signer_count))\n",
        "\n",
        "        # Ensure signers in validation are NEVER in training\n",
        "        val_unseen_signer_keys = []\n",
        "        train_seen_signer_keys = []\n",
        "\n",
        "        for key in valid_keys:\n",
        "            signer_id, _ = extract_signer_sentence_ids(key)\n",
        "            if signer_id in val_signers:\n",
        "                val_unseen_signer_keys.append(key)\n",
        "            else:\n",
        "                train_seen_signer_keys.append(key)\n",
        "\n",
        "        # Verify strict separation\n",
        "        assert set(train_seen_signer_keys).isdisjoint(set(val_unseen_signer_keys)), \"ERROR: Train and unseen signer validation sets overlap!\"\n",
        "\n",
        "        splits['unseen_signers'] = {\n",
        "            'name': 'Unseen Signers Split',\n",
        "            'description': f'Hold out {len(val_signers)} complete signers (never seen during training)',\n",
        "            'train_keys': train_seen_signer_keys,\n",
        "            'val_keys': val_unseen_signer_keys,\n",
        "            'challenge_level': 3,\n",
        "            'val_signers': val_signers  # Store these for reference\n",
        "        }\n",
        "\n",
        "        print(f\"Unseen signers split: {len(train_seen_signer_keys)} train, {len(val_unseen_signer_keys)} validation samples\")\n",
        "        print(f\"Verified: Train and validation sets have NO overlap\")\n",
        "\n",
        "    # Split 4: Test-like challenge (both unseen signers and sentences)\n",
        "    if 'unseen_signers' in splits and 'unseen_sentences' in splits:\n",
        "        # Extract a different set of signers and sentences\n",
        "        # We need to ensure we don't reuse the ones already held out\n",
        "        used_val_signers = set(splits['unseen_signers']['val_signers'])\n",
        "        used_val_sentences = set(splits['unseen_sentences']['val_sentences'])\n",
        "\n",
        "        remaining_signers = [s for s in unique_signers if s not in used_val_signers]\n",
        "        remaining_sentences = [s for s in unique_sentences if s not in used_val_sentences]\n",
        "\n",
        "        if len(remaining_signers) >= 2 and len(remaining_sentences) >= 5:\n",
        "            challenge_signer_count = min(val_signer_count, len(remaining_signers))\n",
        "            challenge_sentence_count = min(val_sentence_count, len(remaining_sentences))\n",
        "\n",
        "            challenge_signers = set(random.sample(remaining_signers, k=challenge_signer_count))\n",
        "            challenge_sentences = set(random.sample(remaining_sentences, k=challenge_sentence_count))\n",
        "\n",
        "            # Find keys that have EITHER challenge signers OR challenge sentences\n",
        "            # This creates a strict train/val separation while ensuring enough validation samples\n",
        "            val_challenge_keys = []\n",
        "            train_challenge_keys = []\n",
        "\n",
        "            for key in valid_keys:\n",
        "                signer_id, _ = extract_signer_sentence_ids(key)\n",
        "                sentence = id_to_sentence_map[key]\n",
        "\n",
        "                # If this key has either a challenge signer or sentence, it goes to validation\n",
        "                if (signer_id in challenge_signers) or (sentence in challenge_sentences):\n",
        "                    val_challenge_keys.append(key)\n",
        "                else:\n",
        "                    train_challenge_keys.append(key)\n",
        "\n",
        "            # Verify strict separation\n",
        "            assert set(train_challenge_keys).isdisjoint(set(val_challenge_keys)), \"ERROR: Train and challenge validation sets overlap!\"\n",
        "\n",
        "            # Only create this split if we have enough validation samples\n",
        "            if len(val_challenge_keys) >= 10:\n",
        "                splits['challenge'] = {\n",
        "                    'name': 'Test-Like Challenge Split',\n",
        "                    'description': 'Both unseen signers and unseen sentences (strictly held out from training)',\n",
        "                    'train_keys': train_challenge_keys,\n",
        "                    'val_keys': val_challenge_keys,\n",
        "                    'challenge_level': 4,\n",
        "                    'val_signers': challenge_signers,\n",
        "                    'val_sentences': challenge_sentences\n",
        "                }\n",
        "\n",
        "                print(f\"Challenge split: {len(train_challenge_keys)} train, {len(val_challenge_keys)} validation samples\")\n",
        "                print(f\"Verified: Train and validation sets have NO overlap\")\n",
        "\n",
        "    return splits, key_info\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xrvCb6VHCiaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_split_separation(splits):\n",
        "    \"\"\"\n",
        "    Verify that each split maintains strict separation between its own training and validation data.\n",
        "\n",
        "    Args:\n",
        "        splits: Dictionary of validation splits\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all individual splits have strict separation, False otherwise\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Verifying Individual Split Separation ---\")\n",
        "    all_valid = True\n",
        "\n",
        "    # Check each split for strict separation between its own train and validation sets\n",
        "    for name, split_info in splits.items():\n",
        "        train_keys = set(split_info['train_keys'])\n",
        "        val_keys = set(split_info['val_keys'])\n",
        "\n",
        "        # Verify no overlap between train and val within this split\n",
        "        if not train_keys.isdisjoint(val_keys):\n",
        "            overlap = len(train_keys.intersection(val_keys))\n",
        "            print(f\"ERROR: Split '{name}' has {overlap} overlapping keys between train and validation!\")\n",
        "            all_valid = False\n",
        "        else:\n",
        "            print(f\"✓ Split '{name}' has strict separation between train and validation sets\")\n",
        "\n",
        "    # Now provide info about cross-split overlap (not an error, just informational)\n",
        "    print(\"\\n--- Cross-Split Overlap Information (Expected) ---\")\n",
        "    split_names = list(splits.keys())\n",
        "    for i, name1 in enumerate(split_names):\n",
        "        for j in range(i+1, len(split_names)):\n",
        "            name2 = split_names[j]\n",
        "            train_keys1 = set(splits[name1]['train_keys'])\n",
        "            val_keys1 = set(splits[name1]['val_keys'])\n",
        "            train_keys2 = set(splits[name2]['train_keys'])\n",
        "            val_keys2 = set(splits[name2]['val_keys'])\n",
        "\n",
        "            train_train_overlap = len(train_keys1.intersection(train_keys2))\n",
        "            val_val_overlap = len(val_keys1.intersection(val_keys2))\n",
        "\n",
        "            # Show overlap info for training and validation sets\n",
        "            print(f\"Info: Splits '{name1}' and '{name2}':\")\n",
        "            print(f\"  - {train_train_overlap} overlapping training keys\")\n",
        "            print(f\"  - {val_val_overlap} overlapping validation keys\")\n",
        "\n",
        "            # Also show cross-overlap (informational only)\n",
        "            train1_val2_overlap = len(train_keys1.intersection(val_keys2))\n",
        "            train2_val1_overlap = len(train_keys2.intersection(val_keys1))\n",
        "\n",
        "            print(f\"  - {train1_val2_overlap} keys in both '{name1}' train and '{name2}' validation\")\n",
        "            print(f\"  - {train2_val1_overlap} keys in both '{name2}' train and '{name1}' validation\")\n",
        "\n",
        "    if all_valid:\n",
        "        print(\"\\n✓ All individual splits maintain strict separation between their own train and validation sets!\")\n",
        "        print(\"NOTE: The overlap between different splits is normal and expected.\")\n",
        "        print(\"The training will use only ONE split (the most challenging one) for both training and validation.\")\n",
        "    else:\n",
        "        print(\"\\n✗ One or more splits have internal train/validation overlap!\")\n",
        "\n",
        "    return all_valid"
      ],
      "metadata": {
        "id": "W-XhCotZCidE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_prepare_test_data(test_h5_path, test_json_path, test_h5_keys=None, test_h5_lengths=None):\n",
        "    \"\"\"\n",
        "    Load and prepare the official test dataset\n",
        "\n",
        "    Args:\n",
        "        test_h5_path: Path to test H5 file\n",
        "        test_json_path: Path to test JSON file\n",
        "        test_h5_keys: List of keys from test H5 file (if already loaded)\n",
        "        test_h5_lengths: Mapping from keys to sequence lengths (if already loaded)\n",
        "\n",
        "    Returns:\n",
        "        list: List of valid test keys\n",
        "    \"\"\"\n",
        "    test_ids_from_json = []\n",
        "\n",
        "    # Load test IDs from JSON\n",
        "    if test_json_path and os.path.exists(test_json_path):\n",
        "        try:\n",
        "            with open(test_json_path, \"r\") as f:\n",
        "                test_data_json = json.load(f)\n",
        "                if isinstance(test_data_json, list) and test_data_json and isinstance(test_data_json[0], dict) and \"id\" in test_data_json[0]:\n",
        "                    test_ids_from_json = [item[\"id\"] for item in test_data_json]\n",
        "                    print(f\"Loaded {len(test_ids_from_json)} test IDs from {test_json_path}\")\n",
        "                else:\n",
        "                    print(f\"Warning: Test JSON format not recognized or empty at {test_json_path}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading test JSON {test_json_path}: {e}\")\n",
        "\n",
        "    # If we don't have keys/lengths yet, load them from H5\n",
        "    if test_h5_keys is None or test_h5_lengths is None:\n",
        "        if test_h5_path and os.path.exists(test_h5_path):\n",
        "            test_h5_keys, test_h5_lengths, _ = preload_h5_info(test_h5_path)\n",
        "        else:\n",
        "            print(f\"Warning: Test H5 file not found at {test_h5_path}\")\n",
        "            return []\n",
        "\n",
        "    # Find keys that exist in both H5 and JSON (if JSON was loaded)\n",
        "    valid_test_keys = []\n",
        "    if test_ids_from_json:\n",
        "        valid_test_keys = [key for key in test_h5_keys if key in test_ids_from_json and key in test_h5_lengths]\n",
        "    else:\n",
        "        valid_test_keys = [key for key in test_h5_keys if key in test_h5_lengths]\n",
        "\n",
        "    print(f\"Found {len(valid_test_keys)} valid test keys for evaluation\")\n",
        "    return valid_test_keys"
      ],
      "metadata": {
        "id": "LXwmo5jICifv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_datasets_from_splits(splits, train_h5_path, id_to_sentence_map, train_h5_lengths,\n",
        "                               tokenizer, max_seq_length, max_label_length, feature_dim, target_lang,\n",
        "                               test_h5_path=None, test_h5_lengths=None):\n",
        "    \"\"\"\n",
        "    Create training and validation datasets from the splits.\n",
        "\n",
        "    Args:\n",
        "        splits: Dictionary of validation splits\n",
        "        train_h5_path: Path to training H5 file\n",
        "        id_to_sentence_map: Mapping from keys to sentence translations\n",
        "        train_h5_lengths: Mapping from keys to sequence lengths\n",
        "        tokenizer: The tokenizer\n",
        "        max_seq_length: Maximum sequence length\n",
        "        max_label_length: Maximum label length\n",
        "        feature_dim: Feature dimension\n",
        "        target_lang: Target language\n",
        "        test_h5_path: Path to test H5 file\n",
        "        test_h5_lengths: Mapping from test keys to sequence lengths\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of datasets for each split\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Creating Datasets from Splits ---\")\n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    for split_name, split_info in splits.items():\n",
        "        print(f\"\\nCreating datasets for '{split_info['name']}' split:\")\n",
        "\n",
        "        train_keys = split_info['train_keys']\n",
        "        val_keys = split_info['val_keys']\n",
        "\n",
        "        # Determine if this is using test data\n",
        "        is_test_split = 'test' in split_name and test_h5_path is not None\n",
        "        h5_path = test_h5_path if is_test_split else train_h5_path\n",
        "        h5_lengths = test_h5_lengths if is_test_split else train_h5_lengths\n",
        "\n",
        "        # For test validation, we don't have translations\n",
        "        empty_map = {}\n",
        "\n",
        "        # Create training dataset\n",
        "        train_dataset = None\n",
        "        if not is_test_split and train_keys:\n",
        "            try:\n",
        "                train_dataset = SignDataset(\n",
        "                    h5_file_path=h5_path,\n",
        "                    data_keys_to_use=train_keys,\n",
        "                    id_to_sentence_map=id_to_sentence_map,\n",
        "                    tokenizer=tokenizer,\n",
        "                    h5_lengths_map=h5_lengths,\n",
        "                    max_seq_length=max_seq_length,\n",
        "                    max_label_length=max_label_length,\n",
        "                    feature_dim=feature_dim,\n",
        "                    target_lang=target_lang,\n",
        "                    is_test_set=False\n",
        "                )\n",
        "                print(f\"  Created training dataset with {len(train_dataset)} samples\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error creating training dataset: {e}\")\n",
        "\n",
        "        # Create validation dataset\n",
        "        val_dataset = None\n",
        "        if val_keys:\n",
        "            try:\n",
        "                val_dataset = SignDataset(\n",
        "                    h5_file_path=h5_path,\n",
        "                    data_keys_to_use=val_keys,\n",
        "                    # Use empty map for test validation to mark as test data\n",
        "                    id_to_sentence_map=empty_map if is_test_split else id_to_sentence_map,\n",
        "                    tokenizer=tokenizer,\n",
        "                    h5_lengths_map=h5_lengths,\n",
        "                    max_seq_length=max_seq_length,\n",
        "                    max_label_length=max_label_length,\n",
        "                    feature_dim=feature_dim,\n",
        "                    target_lang=target_lang,\n",
        "                    is_test_set=is_test_split\n",
        "                )\n",
        "                print(f\"  Created validation dataset with {len(val_dataset)} samples\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error creating validation dataset: {e}\")\n",
        "\n",
        "        datasets[split_name] = {\n",
        "            'train': train_dataset,\n",
        "            'validation': val_dataset,\n",
        "            'info': split_info\n",
        "        }\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# --- Custom Data Collator --- #\n",
        "@dataclass\n",
        "class CustomDataCollator:\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None # Not used for features here, padding done in dataset\n",
        "    pad_to_multiple_of: Optional[int] = None # Applied to labels\n",
        "    label_pad_token_id: int = -100\n",
        "    return_tensors: str = \"pt\"\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        input_features = [f[\"input_features\"] for f in features]\n",
        "        attention_mask = [f[\"attention_mask\"] for f in features]\n",
        "        labels = [f[\"labels\"] for f in features] # These are the ones to pad dynamically\n",
        "        decoder_attention_mask = [f[\"decoder_attention_mask\"] for f in features]\n",
        "\n",
        "        batch_input_features = torch.stack(input_features)\n",
        "        batch_attention_mask = torch.stack(attention_mask)\n",
        "\n",
        "        max_label_len = max(len(lab) for lab in labels)\n",
        "        if self.pad_to_multiple_of is not None:\n",
        "            max_label_len = (\n",
        "                (max_label_len + self.pad_to_multiple_of - 1)\n",
        "                // self.pad_to_multiple_of\n",
        "                * self.pad_to_multiple_of\n",
        "            )\n",
        "\n",
        "        padded_labels = []\n",
        "        padded_decoder_attention_mask = []\n",
        "        for lab, dec_attn in zip(labels, decoder_attention_mask):\n",
        "            remainder = max_label_len - len(lab)\n",
        "            padded_labels.append(F.pad(lab, (0, remainder), value=self.label_pad_token_id))\n",
        "            padded_decoder_attention_mask.append(F.pad(dec_attn, (0, remainder), value=0))\n",
        "\n",
        "        batch_labels = torch.stack(padded_labels)\n",
        "        batch_decoder_attention_mask = torch.stack(padded_decoder_attention_mask)\n",
        "\n",
        "        batch = {\n",
        "            \"input_features\": batch_input_features,\n",
        "            \"attention_mask\": batch_attention_mask,\n",
        "            \"labels\": batch_labels,\n",
        "            \"decoder_attention_mask\": batch_decoder_attention_mask,\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "# --- Model Definition --- #\n",
        "class VisualHead(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "        print(f\"Initialized VisualHead: Linear({input_dim} -> {output_dim})\")\n",
        "\n",
        "    def forward(self, features):\n",
        "        # The issue is here - it should use self.projection, not self.visual_head\n",
        "        projected_features = self.projection(features)\n",
        "        return projected_features\n",
        "\n",
        "class SignTranslationModel(nn.Module):\n",
        "    def __init__(self, feature_dim, mbert_model_name, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        print(f\"Initializing SignTranslationModel:\")\n",
        "        print(f\"  Loading Base mBART Model From: {mbert_model_name}\")\n",
        "        print(f\"  Feature Dim (Input): {feature_dim}\")\n",
        "\n",
        "        try:\n",
        "            # CRUCIAL CHANGE: Set config before loading model\n",
        "            from transformers import MBartConfig\n",
        "            config = MBartConfig.from_pretrained(mbert_model_name)\n",
        "\n",
        "            # Disable SDPA in config before loading\n",
        "            config._attn_implementation = \"eager\"\n",
        "\n",
        "            # Now load with our config\n",
        "            self.mbart = MBartForConditionalGeneration.from_pretrained(\n",
        "                mbert_model_name,\n",
        "                config=config\n",
        "            )\n",
        "            print(f\"  Base mBART model loaded successfully with eager attention.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR loading base mBART model from {mbert_model_name}: {e}\")\n",
        "            raise\n",
        "\n",
        "        self.model_hidden_dim = self.mbart.config.d_model\n",
        "        print(f\"  Model Hidden Dim (d_model): {self.model_hidden_dim}\")\n",
        "\n",
        "        self.visual_head = VisualHead(feature_dim, self.model_hidden_dim, dropout_rate)\n",
        "\n",
        "        # Initialize generation_config\n",
        "        self.generation_config = None  # Will be set later in training setup\n",
        "\n",
        "    def forward(self, input_features, attention_mask, labels=None, decoder_attention_mask=None, **kwargs):\n",
        "        input_features = input_features.float()\n",
        "        try:\n",
        "            inputs_embeds = self.visual_head(input_features)\n",
        "        except Exception as e:\n",
        "            print(f\"VisualHead projection error: {e}\\nInput shape: {input_features.shape}, Input dtype: {input_features.dtype}\")\n",
        "            raise\n",
        "\n",
        "        try:\n",
        "            outputs = self.mbart(\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                decoder_attention_mask=decoder_attention_mask,\n",
        "                return_dict=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"mBART forward error: {e}\")\n",
        "            print(f\"  inputs_embeds shape: {inputs_embeds.shape if inputs_embeds is not None else 'None'}, dtype: {inputs_embeds.dtype if inputs_embeds is not None else 'None'}\")\n",
        "            print(f\"  attention_mask shape: {attention_mask.shape if attention_mask is not None else 'None'}, dtype: {attention_mask.dtype if attention_mask is not None else 'None'}\")\n",
        "            print(f\"  labels shape: {labels.shape if labels is not None else 'None'}, dtype: {labels.dtype if labels is not None else 'None'}\")\n",
        "            print(f\"  decoder_attention_mask shape: {decoder_attention_mask.shape if decoder_attention_mask is not None else 'None'}, dtype: {decoder_attention_mask.dtype if decoder_attention_mask is not None else 'None'}\")\n",
        "            raise\n",
        "        return outputs\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_features, attention_mask, **generate_kwargs):\n",
        "        self.eval()\n",
        "        input_features = input_features.float() # Ensure float input\n",
        "        inputs_embeds = self.visual_head(input_features)\n",
        "\n",
        "        # ATTENTION MASK FIX - Ensure correct shape for generate\n",
        "        # During beam search, the attention mask shape can cause issues\n",
        "        # We need to ensure it's correctly shaped for the mBART model\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Get batch size\n",
        "            batch_size = attention_mask.size(0)\n",
        "\n",
        "            # Ensure attention mask is the correct shape by reshaping\n",
        "            attention_mask = attention_mask.view(batch_size, -1)\n",
        "\n",
        "            # Debug log the shape\n",
        "            # print(f\"Generate: Attention mask reshaped to {attention_mask.shape}\")\n",
        "\n",
        "        decoder_start_token_id = self.mbart.config.forced_bos_token_id # Default\n",
        "        if \"tgt_lang\" in generate_kwargs:\n",
        "            try:\n",
        "                lang_code = generate_kwargs[\"tgt_lang\"]\n",
        "                lang_token_id = global_tokenizer.lang_code_to_id.get(lang_code)\n",
        "                if lang_token_id is not None:\n",
        "                   decoder_start_token_id = lang_token_id\n",
        "                else:\n",
        "                    print(f\"Warning: Target language '{lang_code}' not found in tokenizer lang_code_to_id. Using default BOS: {decoder_start_token_id}.\")\n",
        "            except Exception as e:\n",
        "                 print(f\"Warning: Error accessing tokenizer for target language '{generate_kwargs['tgt_lang']}': {e}. Using default BOS: {decoder_start_token_id}\")\n",
        "\n",
        "        # Use the model's generation_config if available, otherwise use defaults\n",
        "        if hasattr(self, 'generation_config') and self.generation_config is not None:\n",
        "            # Clone and update the generation config\n",
        "            from copy import deepcopy\n",
        "            gen_config = deepcopy(self.generation_config)\n",
        "\n",
        "            # Update with any kwargs provided\n",
        "            for k, v in generate_kwargs.items():\n",
        "                if hasattr(gen_config, k):\n",
        "                    setattr(gen_config, k, v)\n",
        "\n",
        "            # Extra safety check for critical parameters\n",
        "            if \"max_length\" in generate_kwargs:\n",
        "                gen_config.max_length = generate_kwargs[\"max_length\"]\n",
        "            if \"num_beams\" in generate_kwargs:\n",
        "                gen_config.num_beams = generate_kwargs[\"num_beams\"]\n",
        "            if \"decoder_start_token_id\" in generate_kwargs:\n",
        "                gen_config.decoder_start_token_id = generate_kwargs[\"decoder_start_token_id\"]\n",
        "            else:\n",
        "                gen_config.decoder_start_token_id = decoder_start_token_id\n",
        "\n",
        "            # Run generation with the config object\n",
        "            try:\n",
        "                return self.mbart.generate(\n",
        "                    inputs_embeds=inputs_embeds,\n",
        "                    attention_mask=attention_mask,\n",
        "                    generation_config=gen_config\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Generate failed with generation_config: {e}\")\n",
        "                print(\"Falling back to kwargs method...\")\n",
        "\n",
        "        # Fall back to kwargs method\n",
        "        final_generate_kwargs = {\n",
        "            \"max_length\": generate_kwargs.get(\"max_length\", self.mbart.config.max_length),\n",
        "            \"num_beams\": generate_kwargs.get(\"num_beams\", self.mbart.config.num_beams),\n",
        "            \"early_stopping\": generate_kwargs.get(\"early_stopping\", True),\n",
        "            \"decoder_start_token_id\": decoder_start_token_id,\n",
        "        }\n",
        "        for k, v in generate_kwargs.items():\n",
        "            if k not in [\"tgt_lang\", \"max_length\", \"num_beams\", \"early_stopping\", \"decoder_start_token_id\", \"forced_decoder_ids\"]:\n",
        "                 final_generate_kwargs[k] = v\n",
        "\n",
        "        return self.mbart.generate(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            **final_generate_kwargs\n",
        "        )\n",
        "\n",
        "    def save_pretrained(self, save_dir):\n",
        "        \"\"\"Save model to the specified directory.\"\"\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Save mBART config\n",
        "        self.mbart.config.save_pretrained(save_dir)\n",
        "\n",
        "        # Save model state_dict\n",
        "        torch.save(self.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "\n",
        "        # Save generation config if it exists\n",
        "        if hasattr(self, 'generation_config') and self.generation_config is not None:\n",
        "            self.generation_config.save_pretrained(save_dir)\n",
        "\n",
        "        print(f\"Model saved to {save_dir}\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, load_dir, device=None):\n",
        "        \"\"\"Load model from the specified directory.\"\"\"\n",
        "        # Load config\n",
        "        from transformers import MBartConfig\n",
        "        config = MBartConfig.from_pretrained(load_dir)\n",
        "\n",
        "        # Determine feature_dim (needed to initialize the model)\n",
        "        # This might be stored in the config or we need to infer it\n",
        "        feature_dim = getattr(config, \"feature_dim\", 208)  # Default to 208 if not found\n",
        "\n",
        "        # Create new model instance\n",
        "        model = cls(\n",
        "            feature_dim=feature_dim,\n",
        "            mbert_model_name=load_dir\n",
        "        )\n",
        "\n",
        "        # Load state_dict\n",
        "        state_dict_path = os.path.join(load_dir, \"pytorch_model.bin\")\n",
        "        if os.path.exists(state_dict_path):\n",
        "            state_dict = torch.load(state_dict_path, map_location=\"cpu\")\n",
        "            model.load_state_dict(state_dict)\n",
        "            print(f\"Model weights loaded from {state_dict_path}\")\n",
        "        else:\n",
        "            print(f\"Warning: No model weights found at {state_dict_path}\")\n",
        "\n",
        "        # Load generation config if it exists\n",
        "        generation_config_path = os.path.join(load_dir, \"generation_config.json\")\n",
        "        if os.path.exists(generation_config_path):\n",
        "            model.generation_config = GenerationConfig.from_pretrained(load_dir)\n",
        "            print(f\"Generation config loaded from {generation_config_path}\")\n",
        "\n",
        "        # Move to device if specified\n",
        "        if device is not None:\n",
        "            model = model.to(device)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
        "        \"\"\"Enable gradient checkpointing for memory efficiency.\"\"\"\n",
        "        print(\"Enabling gradient checkpointing on underlying mBART model...\")\n",
        "        if hasattr(self.mbart, \"gradient_checkpointing_enable\"):\n",
        "            self.mbart.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
        "            print(\"Gradient checkpointing enabled on mBART.\")\n",
        "        else:\n",
        "            print(\"Warning: Underlying mBART model does not have 'gradient_checkpointing_enable' method.\")\n",
        "\n",
        "# --- Evaluation Metrics --- #\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "global_tokenizer = None # To be set after tokenizer initialization\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    \"\"\"Process predictions and labels for evaluation.\"\"\"\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"Compute BLEU score for evaluation.\"\"\"\n",
        "    global global_tokenizer\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0] # Predictions are often tuples\n",
        "\n",
        "    labels = np.where(labels != -100, labels, global_tokenizer.pad_token_id)\n",
        "\n",
        "    try:\n",
        "        decoded_preds = global_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding predictions: {e}\")\n",
        "        decoded_preds = [\"<DECODE_ERROR>\"] * len(preds)\n",
        "\n",
        "    try:\n",
        "        decoded_labels = global_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding labels: {e}\")\n",
        "        decoded_labels = [\"<DECODE_ERROR>\"] * len(labels)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    try:\n",
        "        if not isinstance(decoded_preds, list) or not all(isinstance(p, str) for p in decoded_preds):\n",
        "             raise ValueError(\"Decoded predictions are not in the expected format (List[str]).\")\n",
        "        if not isinstance(decoded_labels, list) or not all(isinstance(ref, list) and all(isinstance(s, str) for s in ref) for ref in decoded_labels):\n",
        "            raise ValueError(\"Decoded labels are not in the expected format (List[List[str]]).\")\n",
        "\n",
        "        result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "        result = {\"bleu\": result[\"score\"]}\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing BLEU: {e}\")\n",
        "        result = {\"bleu\": 0.0} # Default to 0 on error\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != global_tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "def train_and_evaluate_with_strict_separation(model, datasets, args, tokenizer, data_collator):\n",
        "    \"\"\"\n",
        "    Train and evaluate the model with strict separation between training and validation.\n",
        "\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        datasets: Dictionary of datasets for each split\n",
        "        args: Training arguments\n",
        "        tokenizer: The tokenizer\n",
        "        data_collator: The data collator\n",
        "\n",
        "    Returns:\n",
        "        The trained model\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Training and Evaluation with Strict Separation ===\")\n",
        "\n",
        "    # Create results directory\n",
        "    os.makedirs(args.results_dir, exist_ok=True)\n",
        "    results_file = os.path.join(args.results_dir, \"training_results.txt\")\n",
        "\n",
        "    # Select primary split for training (highest challenge level with valid datasets)\n",
        "    available_splits = [(name, info['info']['challenge_level'])\n",
        "                       for name, info in datasets.items()\n",
        "                       if info['train'] is not None and info['validation'] is not None]\n",
        "\n",
        "    if not available_splits:\n",
        "        print(\"ERROR: No valid splits found for training. Aborting.\")\n",
        "        return model\n",
        "\n",
        "    # Sort by challenge level (descending)\n",
        "    available_splits.sort(key=lambda x: x[1], reverse=True)\n",
        "    primary_split = available_splits[0][0]\n",
        "\n",
        "    primary_train = datasets[primary_split]['train']\n",
        "    primary_validation = datasets[primary_split]['validation']\n",
        "    split_info = datasets[primary_split]['info']\n",
        "\n",
        "    print(f\"\\nUsing '{split_info['name']}' as primary split for training.\")\n",
        "    print(f\"Challenge level: {split_info['challenge_level']}\")\n",
        "    print(f\"Description: {split_info['description']}\")\n",
        "    print(f\"Training samples: {len(primary_train)}\")\n",
        "    print(f\"Validation samples: {len(primary_validation)}\")\n",
        "\n",
        "    # Setup logging\n",
        "    with open(results_file, \"w\") as f:\n",
        "        f.write(\"=== Sign Language Translation Training Results ===\\n\\n\")\n",
        "        f.write(f\"Model: mBART with Visual Head\\n\")\n",
        "        f.write(f\"Primary split: {split_info['name']}\\n\")\n",
        "        f.write(f\"Challenge level: {split_info['challenge_level']}\\n\")\n",
        "        f.write(f\"Description: {split_info['description']}\\n\")\n",
        "        f.write(f\"Training samples: {len(primary_train)}\\n\")\n",
        "        f.write(f\"Validation samples: {len(primary_validation)}\\n\\n\")\n",
        "\n",
        "        f.write(\"Other available splits:\\n\")\n",
        "        for name, info in datasets.items():\n",
        "            if name != primary_split and info['train'] is not None and info['validation'] is not None:\n",
        "                f.write(f\"- {info['info']['name']}: {len(info['train'])} train, {len(info['validation'])} validation\\n\")\n",
        "                f.write(f\"  {info['info']['description']}\\n\")\n",
        "\n",
        "        f.write(\"\\n=== TRAINING PROGRESS ===\\n\")\n",
        "\n",
        "    # Setup target language tokens\n",
        "    lang_token_id = tokenizer.lang_code_to_id.get(args.target_lang)\n",
        "    if lang_token_id is None:\n",
        "        print(f\"Warning: Target language code '{args.target_lang}' not found in tokenizer's lang_code_to_id map.\")\n",
        "        lang_token_id = model.mbart.config.decoder_start_token_id\n",
        "        print(f\"Using fallback decoder_start_token_id: {lang_token_id}\")\n",
        "\n",
        "    # Generation config\n",
        "    generation_config = GenerationConfig(\n",
        "        max_length=args.max_label_length,\n",
        "        num_beams=args.num_beams,\n",
        "        early_stopping=True,\n",
        "        decoder_start_token_id=lang_token_id,\n",
        "        forced_bos_token_id=lang_token_id,\n",
        "    )\n",
        "\n",
        "    model.generation_config = generation_config\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        num_train_epochs=args.epochs,\n",
        "        per_device_train_batch_size=args.train_batch_size,\n",
        "        per_device_eval_batch_size=args.eval_batch_size,\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        warmup_steps=args.warmup_steps,\n",
        "        weight_decay=args.weight_decay,\n",
        "        logging_dir=os.path.join(args.output_dir, \"logs\"),\n",
        "        logging_steps=args.logging_steps,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=args.save_total_limit,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"bleu\",\n",
        "        greater_is_better=True,\n",
        "        predict_with_generate=True,\n",
        "        generation_max_length=args.max_label_length,\n",
        "        generation_num_beams=args.num_beams,\n",
        "        fp16=args.fp16 and torch.cuda.is_available(),\n",
        "        gradient_checkpointing=args.gradient_checkpointing,\n",
        "        report_to=[\"tensorboard\"],\n",
        "        dataloader_num_workers=2,\n",
        "        dataloader_pin_memory=True if torch.cuda.is_available() else False,\n",
        "        save_safetensors=False,\n",
        "    )\n",
        "\n",
        "    # Initialize trainer with primary validation dataset\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=primary_train,\n",
        "        eval_dataset=primary_validation,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Early stopping setup\n",
        "    early_stopping_patience = 3\n",
        "    best_epoch = 0\n",
        "    best_bleu = 0\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\n=== Starting Training ===\")\n",
        "\n",
        "    # Check for existing checkpoints\n",
        "    checkpoint_dir = args.output_dir\n",
        "    resume_from_checkpoint = False\n",
        "\n",
        "    # Look for checkpoint directories\n",
        "    if os.path.exists(checkpoint_dir):\n",
        "        checkpoint_dirs = [d for d in os.listdir(checkpoint_dir)\n",
        "                          if os.path.isdir(os.path.join(checkpoint_dir, d))\n",
        "                          and d.startswith(\"checkpoint-\")]\n",
        "\n",
        "        if checkpoint_dirs:\n",
        "            # Sort checkpoints by step number\n",
        "            checkpoint_dirs.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
        "            latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_dirs[-1])\n",
        "            print(f\"\\nFound existing checkpoint: {latest_checkpoint}\")\n",
        "            resume_choice = input(\"Do you want to resume training from this checkpoint? (y/n): \").lower()\n",
        "\n",
        "            if resume_choice == 'y':\n",
        "                resume_from_checkpoint = latest_checkpoint\n",
        "                print(f\"Will resume training from: {resume_from_checkpoint}\")\n",
        "            else:\n",
        "                print(\"Starting training from scratch.\")\n",
        "        else:\n",
        "            print(\"No checkpoints found. Starting training from scratch.\")\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"\\n--- Epoch {epoch + 1}/{args.epochs} ---\")\n",
        "\n",
        "        # Train for one epoch - modified to support checkpoint resumption\n",
        "        train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "        # Only use checkpoint for first epoch, then set to False\n",
        "        if resume_from_checkpoint:\n",
        "            resume_from_checkpoint = False\n",
        "\n",
        "        train_metrics = train_result.metrics\n",
        "\n",
        "        print(f\"Training loss: {train_metrics['train_loss']:.4f}\")\n",
        "\n",
        "        # Evaluate on all validation splits\n",
        "        with open(results_file, \"a\") as f:\n",
        "            f.write(f\"\\nEpoch {epoch + 1}\\n\")\n",
        "            f.write(f\"Training loss: {train_metrics['train_loss']:.4f}\\n\")\n",
        "\n",
        "        # Track best model\n",
        "        current_bleu = None\n",
        "\n",
        "        # Evaluate on each split's validation set\n",
        "        for split_name, split_data in datasets.items():\n",
        "            val_dataset = split_data['validation']\n",
        "\n",
        "            if val_dataset is None:\n",
        "                continue\n",
        "\n",
        "            split_desc = split_data['info']['name']\n",
        "            is_test_split = 'test' in split_name\n",
        "\n",
        "            if is_test_split:\n",
        "                # For test validation, we can only generate predictions, not evaluate\n",
        "                print(f\"\\nGenerating predictions for '{split_desc}' validation\")\n",
        "                predictions = trainer.predict(\n",
        "                    test_dataset=val_dataset,\n",
        "                    metric_key_prefix=f\"val_{split_name}\",\n",
        "                )\n",
        "\n",
        "                with open(results_file, \"a\") as f:\n",
        "                    f.write(f\"{split_desc}: Generated predictions only (no reference translations)\\n\")\n",
        "            else:\n",
        "                # For regular validation, evaluate with metrics\n",
        "                print(f\"\\nEvaluating on '{split_desc}' validation\")\n",
        "                val_metrics = trainer.evaluate(\n",
        "                    eval_dataset=val_dataset,\n",
        "                    metric_key_prefix=f\"val_{split_name}\",\n",
        "                )\n",
        "\n",
        "                bleu_score = val_metrics[f\"val_{split_name}_bleu\"]\n",
        "                print(f\"{split_desc} BLEU: {bleu_score:.2f}\")\n",
        "\n",
        "                with open(results_file, \"a\") as f:\n",
        "                    f.write(f\"{split_desc} BLEU: {bleu_score:.2f}\\n\")\n",
        "\n",
        "                # Track primary split for early stopping\n",
        "                if split_name == primary_split:\n",
        "                    current_bleu = bleu_score\n",
        "\n",
        "        # Check early stopping on primary validation\n",
        "        if current_bleu is not None:\n",
        "            if current_bleu > best_bleu:\n",
        "                best_bleu = current_bleu\n",
        "                best_epoch = epoch + 1\n",
        "                no_improvement_count = 0\n",
        "\n",
        "                # Save best model\n",
        "                best_model_dir = os.path.join(args.output_dir, \"best_model\")\n",
        "                os.makedirs(best_model_dir, exist_ok=True)\n",
        "                model.save_pretrained(best_model_dir)\n",
        "                tokenizer.save_pretrained(best_model_dir)\n",
        "\n",
        "                print(f\"New best model saved with BLEU: {best_bleu:.2f}\")\n",
        "            else:\n",
        "                no_improvement_count += 1\n",
        "                print(f\"No improvement for {no_improvement_count} epochs (best: {best_bleu:.2f} at epoch {best_epoch})\")\n",
        "\n",
        "        # Check early stopping\n",
        "        if no_improvement_count >= early_stopping_patience:\n",
        "            print(f\"\\nEarly stopping after {epoch + 1} epochs!\")\n",
        "            with open(results_file, \"a\") as f:\n",
        "                f.write(f\"\\nEarly stopping triggered. Best model was at epoch {best_epoch} with BLEU: {best_bleu:.2f}\\n\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n=== Training Complete ===\")\n",
        "    print(f\"Best model was at epoch {best_epoch} with BLEU: {best_bleu:.2f}\")\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    best_model_path = os.path.join(args.output_dir, \"best_model\")\n",
        "    if os.path.exists(best_model_path):\n",
        "        print(f\"\\nLoading best model from epoch {best_epoch}...\")\n",
        "        model = model.from_pretrained(best_model_path, device=device)\n",
        "\n",
        "    # Final evaluation on all validation splits\n",
        "    print(\"\\n=== Final Evaluation ===\")\n",
        "\n",
        "    with open(results_file, \"a\") as f:\n",
        "        f.write(\"\\n=== FINAL EVALUATION ===\\n\")\n",
        "\n",
        "    for split_name, split_data in datasets.items():\n",
        "        val_dataset = split_data['validation']\n",
        "\n",
        "        if val_dataset is None:\n",
        "            continue\n",
        "\n",
        "        split_desc = split_data['info']['name']\n",
        "        is_test_split = 'test' in split_name\n",
        "\n",
        "        if is_test_split:\n",
        "            print(f\"\\nFinal prediction generation for '{split_desc}'\")\n",
        "            predict_results = trainer.predict(\n",
        "                test_dataset=val_dataset,\n",
        "                metric_key_prefix=f\"final_{split_name}\",\n",
        "            )\n",
        "\n",
        "            with open(results_file, \"a\") as f:\n",
        "                f.write(f\"Final {split_desc}: Generated predictions only\\n\")\n",
        "        else:\n",
        "            print(f\"\\nFinal evaluation on '{split_desc}'\")\n",
        "            val_metrics = trainer.evaluate(\n",
        "                eval_dataset=val_dataset,\n",
        "                metric_key_prefix=f\"final_{split_name}\",\n",
        "            )\n",
        "\n",
        "            bleu_score = val_metrics[f\"final_{split_name}_bleu\"]\n",
        "            print(f\"Final {split_desc} BLEU: {bleu_score:.2f}\")\n",
        "\n",
        "            with open(results_file, \"a\") as f:\n",
        "                f.write(f\"Final {split_desc} BLEU: {bleu_score:.2f}\\n\")\n",
        "\n",
        "    # Generate predictions for official test set if available\n",
        "    if 'official_test' in datasets and datasets['official_test']['validation'] is not None:\n",
        "        test_dataset = datasets['official_test']['validation']\n",
        "\n",
        "        print(\"\\n=== Generating Official Test Set Predictions ===\")\n",
        "\n",
        "        predict_results = trainer.predict(\n",
        "            test_dataset=test_dataset,\n",
        "            metric_key_prefix=\"test\",\n",
        "        )\n",
        "\n",
        "        # Save predictions\n",
        "        predictions = predict_results.predictions\n",
        "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "        decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "        cleaned_predictions = [pred.strip() for pred in decoded_predictions]\n",
        "\n",
        "        # Create submission format\n",
        "        output_prediction_file = os.path.join(args.results_dir, \"test_predictions.txt\")\n",
        "        with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            writer.write(\"ID\\tPrediction\\n\")\n",
        "            for idx, prediction in enumerate(cleaned_predictions):\n",
        "                if idx < len(test_dataset.data_info):\n",
        "                    sample_key = test_dataset.data_info[idx][0]\n",
        "                    writer.write(f\"{sample_key}\\t{prediction}\\n\")\n",
        "\n",
        "        print(f\"Test predictions saved to {output_prediction_file}\")\n",
        "\n",
        "        # Create JSON format for submission\n",
        "        submission_file = os.path.join(args.results_dir, \"submission.json\")\n",
        "        submission_data = []\n",
        "\n",
        "        for idx, prediction in enumerate(cleaned_predictions):\n",
        "            if idx < len(test_dataset.data_info):\n",
        "                sample_key = test_dataset.data_info[idx][0]\n",
        "                submission_data.append({\n",
        "                    \"id\": sample_key,\n",
        "                    \"translation\": prediction\n",
        "                })\n",
        "\n",
        "        with open(submission_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(submission_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Submission file saved to {submission_file}\")\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "7yaIZGyUKoLJ",
        "outputId": "edaf9189-8e37-4541-9b8f-3aa53bfb2d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataclass' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-db2c28eaf615>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# --- Custom Data Collator --- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mdataclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCustomDataCollator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPreTrainedTokenizerBase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataclass' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zGtLUqrWKoNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvjy4tlgA475"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# === NEW: Main Class for Streamlined Execution ===\n",
        "class SignLanguageTranslationSystem:\n",
        "    def __init__(self):\n",
        "        self.args = None\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.train_h5_keys = []\n",
        "        self.train_h5_lengths = {}\n",
        "        self.test_h5_keys = []\n",
        "        self.test_h5_lengths = {}\n",
        "        self.id_to_sentence_map = {}\n",
        "        self.feature_dim = None\n",
        "        self.splits = {}\n",
        "        self.datasets = {}\n",
        "        self.data_collator = None\n",
        "        self.mount_completed = False\n",
        "        self.training_completed = False\n",
        "\n",
        "    def mount_drive(self):\n",
        "        \"\"\"Mount Google Drive if in Colab environment\"\"\"\n",
        "        if self.mount_completed:\n",
        "            print(\"Drive already mounted. Skipping.\")\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            drive.mount('/content/drive')\n",
        "            print(\"Google Drive mounted successfully.\")\n",
        "            self.mount_completed = True\n",
        "        except Exception as e:\n",
        "            print(f\"Error mounting Google Drive: {e}\")\n",
        "            print(\"Proceeding without Drive mount. Ensure data/output paths are accessible.\")\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\"Initialize arguments and setup paths\"\"\"\n",
        "        # Parse arguments\n",
        "        self.args = parse_args()\n",
        "        print(f\"Using device: {device}\")\n",
        "        set_seed(self.args.seed)\n",
        "        print(f\"Seed set to: {self.args.seed}\")\n",
        "\n",
        "        # Create output directories\n",
        "        os.makedirs(self.args.output_dir, exist_ok=True)\n",
        "        os.makedirs(self.args.results_dir, exist_ok=True)\n",
        "\n",
        "        # Construct paths\n",
        "        self.TRAIN_H5_PATH = os.path.join(self.args.data_dir, self.args.train_h5_file)\n",
        "        self.TRAIN_CSV_PATH = os.path.join(self.args.data_dir, self.args.train_csv_file)\n",
        "        self.TEST_H5_PATH = os.path.join(self.args.data_dir, self.args.test_h5_file)\n",
        "        self.TEST_JSON_PATH = os.path.join(self.args.data_dir, self.args.test_json_file)\n",
        "\n",
        "        # Check paths\n",
        "        self.TRAIN_H5_PATH = check_and_prompt_for_file(self.TRAIN_H5_PATH, \"training H5\")\n",
        "        self.TRAIN_CSV_PATH = check_and_prompt_for_file(self.TRAIN_CSV_PATH, \"training CSV\")\n",
        "        self.TEST_H5_PATH = check_and_prompt_for_file(self.TEST_H5_PATH, \"test H5\")\n",
        "        self.TEST_JSON_PATH = check_and_prompt_for_file(self.TEST_JSON_PATH, \"test JSON\")\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load CSV and H5 file data\"\"\"\n",
        "        # Load CSV data\n",
        "        try:\n",
        "            if self.TRAIN_CSV_PATH:\n",
        "                train_df = pd.read_csv(self.TRAIN_CSV_PATH)\n",
        "                print(f\"Loaded training CSV: {self.TRAIN_CSV_PATH}\")\n",
        "                print(f\"Training samples in CSV: {len(train_df)}\")\n",
        "            else:\n",
        "                print(\"No CSV path provided. Training will not be possible.\")\n",
        "                train_df = pd.DataFrame(columns=[\"ID\", \"Translation\"])\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR reading training CSV: {e}\")\n",
        "            print(\"Creating empty dataframe. Training will not be possible.\")\n",
        "            train_df = pd.DataFrame(columns=[\"ID\", \"Translation\"])\n",
        "\n",
        "        # Create ID to sentence map\n",
        "        if \"ID\" in train_df.columns and \"Translation\" in train_df.columns:\n",
        "            self.id_to_sentence_map = pd.Series(train_df.Translation.values, index=train_df.ID).to_dict()\n",
        "            print(f\"Created ID-to-sentence map with {len(self.id_to_sentence_map)} entries.\")\n",
        "        else:\n",
        "            print(f\"WARNING: Could not find \\\"ID\\\" and \\\"Translation\\\" columns in CSV. Please check column names.\")\n",
        "            if len(train_df.columns) >= 2:\n",
        "                # Try to use first two columns as fallback\n",
        "                column_names = train_df.columns.tolist()\n",
        "                print(f\"Attempting to use first two columns as fallback: {column_names[0]} and {column_names[1]}\")\n",
        "                self.id_to_sentence_map = pd.Series(train_df[column_names[1]].values, index=train_df[column_names[0]]).to_dict()\n",
        "                print(f\"Created fallback ID-to-sentence map with {len(self.id_to_sentence_map)} entries.\")\n",
        "            else:\n",
        "                print(\"No suitable columns found for ID-to-sentence map.\")\n",
        "\n",
        "        # Load H5 file info\n",
        "        detected_feature_dim = None\n",
        "\n",
        "        if self.TRAIN_H5_PATH:\n",
        "            self.train_h5_keys, self.train_h5_lengths, detected_feature_dim = preload_h5_info(self.TRAIN_H5_PATH)\n",
        "        else:\n",
        "            print(\"No training H5 path provided. Training will not be possible.\")\n",
        "\n",
        "        if self.TEST_H5_PATH:\n",
        "            self.test_h5_keys, self.test_h5_lengths, _ = preload_h5_info(self.TEST_H5_PATH)\n",
        "        else:\n",
        "            print(\"No test H5 path provided. Evaluation will not be possible.\")\n",
        "\n",
        "        # Analyze sequence lengths\n",
        "        if self.train_h5_lengths:\n",
        "            self._analyze_sequence_lengths()\n",
        "\n",
        "        # Set feature dimension\n",
        "        self.feature_dim = self._determine_feature_dim(detected_feature_dim)\n",
        "\n",
        "    def _analyze_sequence_lengths(self):\n",
        "        \"\"\"Analyze training sequence lengths\"\"\"\n",
        "        lengths = list(self.train_h5_lengths.values())\n",
        "        print(\"\\n--- Training Sequence Length Analysis ---\")\n",
        "        print(f\"Total sequences found in H5: {len(lengths)}\")\n",
        "        if lengths:\n",
        "            print(f\"Min length: {np.min(lengths)}\")\n",
        "            print(f\"Max length: {np.max(lengths)}\")\n",
        "            print(f\"Mean length: {np.mean(lengths):.2f}\")\n",
        "            print(f\"Median length: {np.median(lengths)}\")\n",
        "            print(f\"90th percentile: {np.percentile(lengths, 90):.2f}\")\n",
        "            print(f\"95th percentile: {np.percentile(lengths, 95):.2f}\")\n",
        "            print(f\"99th percentile: {np.percentile(lengths, 99):.2f}\")\n",
        "            num_exceeding = sum(l > self.args.max_seq_length for l in lengths)\n",
        "            print(f\"Num sequences > {self.args.max_seq_length}: {num_exceeding} ({num_exceeding/len(lengths)*100:.2f}%)\")\n",
        "            print(f\"--> Based on this, {len(lengths) - num_exceeding} sequences ({100 - num_exceeding/len(lengths)*100:.2f}%) will be used with max_seq_length={self.args.max_seq_length}.\")\n",
        "        else:\n",
        "            print(\"No valid sequence lengths found to analyze.\")\n",
        "        print(\"--- End Analysis ---\\n\")\n",
        "\n",
        "    def _determine_feature_dim(self, detected_feature_dim):\n",
        "        \"\"\"Determine the feature dimension to use\"\"\"\n",
        "        if not self.train_h5_keys:\n",
        "            print(f\"WARNING: No valid keys found in training H5 file. Training may not be possible.\")\n",
        "        else:\n",
        "            print(f\"Found {len(self.train_h5_keys)} valid keys in training H5 file.\")\n",
        "\n",
        "        final_feature_dim = self.args.feature_dim\n",
        "        if detected_feature_dim is not None and detected_feature_dim != self.args.feature_dim:\n",
        "            print(f\"WARNING: Manually set feature_dim ({self.args.feature_dim}) differs from detected dimension ({detected_feature_dim}).\")\n",
        "            feature_choice = input(f\"Use detected dimension {detected_feature_dim} instead of {self.args.feature_dim}? (y/n): \")\n",
        "            if feature_choice.lower() == 'y':\n",
        "                final_feature_dim = detected_feature_dim\n",
        "                print(f\"Using detected feature dimension: {final_feature_dim}\")\n",
        "            else:\n",
        "                print(f\"Using manually set feature dimension: {self.args.feature_dim}\")\n",
        "        elif detected_feature_dim is None:\n",
        "            print(f\"WARNING: Could not detect feature dimension. Using manually set value: {self.args.feature_dim}. Verify this is correct!\")\n",
        "        else:\n",
        "            final_feature_dim = detected_feature_dim\n",
        "            print(f\"Using feature dimension: {final_feature_dim}\")\n",
        "\n",
        "        return final_feature_dim\n",
        "\n",
        "    def initialize_tokenizer(self):\n",
        "        \"\"\"Initialize the tokenizer\"\"\"\n",
        "        global global_tokenizer\n",
        "\n",
        "        print(\"\\nInitializing tokenizer...\")\n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                self.args.mbart_model_name,\n",
        "                src_lang=\"en_XX\",\n",
        "                tgt_lang=self.args.target_lang\n",
        "            )\n",
        "            global_tokenizer = self.tokenizer  # Set global tokenizer\n",
        "            print(f\"Tokenizer loaded: {self.args.mbart_model_name}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading tokenizer: {e}\")\n",
        "            print(\"Attempting to proceed, but this may cause issues.\")\n",
        "\n",
        "    def create_datasets(self):\n",
        "        \"\"\"Create validation splits and datasets\"\"\"\n",
        "        # Create validation splits with strict separation\n",
        "        self.splits, key_info = create_validation_splits_with_strict_separation(\n",
        "            self.train_h5_keys,\n",
        "            self.id_to_sentence_map,\n",
        "            self.train_h5_lengths,\n",
        "            self.args.max_seq_length,\n",
        "            seed=self.args.seed\n",
        "        )\n",
        "\n",
        "        # Verify strict separation between all splits\n",
        "        valid_splits = verify_split_separation(self.splits)\n",
        "\n",
        "        if not valid_splits:\n",
        "            print(\"WARNING: Split separation verification failed! Some validation data may leak into training.\")\n",
        "            proceed = input(\"Do you want to continue anyway? (y/n): \")\n",
        "            if proceed.lower() != 'y':\n",
        "                print(\"Exiting script.\")\n",
        "                sys.exit(1)\n",
        "\n",
        "        # Load test data (if available)\n",
        "        test_keys = load_and_prepare_test_data(\n",
        "            self.TEST_H5_PATH,\n",
        "            self.TEST_JSON_PATH,\n",
        "            self.test_h5_keys,\n",
        "            self.test_h5_lengths\n",
        "        )\n",
        "\n",
        "        # Add official test split as a separate dataset (will be test-only, no training)\n",
        "        if test_keys:\n",
        "            self.splits['official_test'] = {\n",
        "                'name': 'Official Test Set',\n",
        "                'description': 'Official test set for final submission',\n",
        "                'train_keys': [],  # No training on test data\n",
        "                'val_keys': test_keys,\n",
        "                'challenge_level': 5  # Highest challenge level\n",
        "            }\n",
        "\n",
        "        # Create datasets from splits\n",
        "        self.datasets = create_datasets_from_splits(\n",
        "            splits=self.splits,\n",
        "            train_h5_path=self.TRAIN_H5_PATH,\n",
        "            id_to_sentence_map=self.id_to_sentence_map,\n",
        "            train_h5_lengths=self.train_h5_lengths,\n",
        "            tokenizer=self.tokenizer,\n",
        "            max_seq_length=self.args.max_seq_length,\n",
        "            max_label_length=self.args.max_label_length,\n",
        "            feature_dim=self.feature_dim,\n",
        "            target_lang=self.args.target_lang,\n",
        "            test_h5_path=self.TEST_H5_PATH,\n",
        "            test_h5_lengths=self.test_h5_lengths\n",
        "        )\n",
        "\n",
        "    def initialize_model(self):\n",
        "        \"\"\"Initialize the model\"\"\"\n",
        "        print(\"\\n--- Initializing Model ---\")\n",
        "        try:\n",
        "            self.model = SignTranslationModel(\n",
        "                feature_dim=self.feature_dim,\n",
        "                mbert_model_name=self.args.mbart_model_name\n",
        "            )\n",
        "\n",
        "            # Apply gradient checkpointing only if explicitly enabled\n",
        "            if self.args.gradient_checkpointing:\n",
        "                print(\"Gradient Checkpointing explicitly enabled via argument.\")\n",
        "                self.model.gradient_checkpointing_enable()\n",
        "            else:\n",
        "                print(\"Gradient Checkpointing is DISABLED (intended for A100/faster training).\")\n",
        "\n",
        "            # Explicitly set attention implementation again after model is ready\n",
        "            print(\"Setting attention implementation to eager on model...\")\n",
        "            if hasattr(self.model.mbart.config, \"_attn_implementation\"):\n",
        "                self.model.mbart.config._attn_implementation = \"eager\"\n",
        "            if hasattr(self.model.mbart, \"_attn_implementation\"):\n",
        "                self.model.mbart._attn_implementation = \"eager\"\n",
        "\n",
        "            # Move model to device after configuration\n",
        "            self.model.to(device)\n",
        "            print(f\"Model instantiated and moved to {device}.\")\n",
        "\n",
        "            if hasattr(self.model, 'mbart') and hasattr(self.model.mbart, 'get_memory_footprint'):\n",
        "                print(f\"Model memory footprint (approx): {self.model.mbart.get_memory_footprint() / 1e9:.2f} GB\")\n",
        "            else:\n",
        "                print(\"Could not retrieve model memory footprint.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing model: {e}\")\n",
        "            print(\"Model initialization failed. Cannot continue with training.\")\n",
        "            sys.exit(1)\n",
        "\n",
        "        # Clear cache\n",
        "        print(\"Running GC and clearing CUDA cache before starting training...\")\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        print(\"Cache cleared.\")\n",
        "\n",
        "        # Create data collator\n",
        "        self.data_collator = CustomDataCollator(tokenizer=self.tokenizer, label_pad_token_id=-100)\n",
        "        print(\"Custom Data Collator created.\")\n",
        "\n",
        "    def train_model(self):\n",
        "        \"\"\"Train the model\"\"\"\n",
        "        if self.training_completed:\n",
        "            print(\"\\nTraining already completed. Skipping.\")\n",
        "            return\n",
        "\n",
        "        if not self.datasets:\n",
        "            print(\"Cannot train: no valid datasets created\")\n",
        "            return\n",
        "\n",
        "        # Train and evaluate with our enhanced function\n",
        "        self.model = train_and_evaluate_with_strict_separation(\n",
        "            model=self.model,\n",
        "            datasets=self.datasets,\n",
        "            args=self.args,\n",
        "            tokenizer=self.tokenizer,\n",
        "            data_collator=self.data_collator\n",
        "        )\n",
        "\n",
        "        self.training_completed = True\n",
        "        print(\"\\nScript execution complete!\")\n",
        "\n",
        "    def evaluate_model(self):\n",
        "        \"\"\"Evaluate the model on test data\"\"\"\n",
        "        if not self.training_completed:\n",
        "            print(\"Cannot evaluate: training not completed\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== Additional Evaluation of Best Model ===\")\n",
        "        # Load best model\n",
        "        best_model_path = os.path.join(self.args.output_dir, \"best_model\")\n",
        "        if not os.path.exists(best_model_path):\n",
        "            print(f\"Best model not found at {best_model_path}\")\n",
        "            return\n",
        "\n",
        "        # Custom evaluation code can be added here\n",
        "        print(\"Model evaluation complete!\")\n",
        "\n",
        "    def generate_predictions(self):\n",
        "        \"\"\"Generate predictions for test data\"\"\"\n",
        "        if not self.training_completed:\n",
        "            print(\"Cannot generate predictions: training not completed\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n=== Generating Test Predictions with Best Model ===\")\n",
        "        # Load best model\n",
        "        best_model_path = os.path.join(self.args.output_dir, \"best_model\")\n",
        "        if not os.path.exists(best_model_path):\n",
        "            print(f\"Best model not found at {best_model_path}\")\n",
        "            return\n",
        "\n",
        "        # Generate predictions\n",
        "        # Custom prediction code can be added here\n",
        "\n",
        "        print(\"Test predictions generated!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Saudi Sign Language Translation Code\n",
        "# For SignforAll 2025 Challenge - With Strict Validation\n",
        "\n",
        "# Standard Libraries\n",
        "import os\n",
        "import sys\n",
        "import evaluate\n",
        "import argparse\n",
        "import random\n",
        "import yaml\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "import re\n",
        "import traceback\n",
        "\n",
        "# CRITICAL: Set environment variables BEFORE importing PyTorch\n",
        "os.environ[\"FORCE_TORCH_SDPA_KERNEL\"] = \"0\"\n",
        "os.environ[\"PYTORCH_DISABLE_SDPA\"] = \"1\"\n",
        "os.environ[\"PYTORCH_NO_CUDA_MEMORY_EFFICIENCY_WARNING\"] = \"1\"\n",
        "\n",
        "# Data Handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import h5py\n",
        "\n",
        "# Deep Learning - PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Transformers Library\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    MBartForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    set_seed as transformers_set_seed,\n",
        "    GenerationConfig\n",
        ")\n",
        "\n",
        "# Evaluation Metric\n",
        "import sacrebleu\n",
        "\n",
        "# Google Drive\n",
        "try:\n",
        "    from google.colab import drive\n",
        "except ImportError:\n",
        "    print(\"Not running in Google Colab environment.\")\n",
        "\n",
        "# For custom collator\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n",
        "from transformers.utils import PaddingStrategy\n",
        "\n",
        "# --- Configuration & Argument Parsing ---\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Train mBART model for Saudi Sign Language Translation\")\n",
        "\n",
        "    # Update paths for Colab environment\n",
        "    parser.add_argument(\"--data_dir\", type=str, default=\"/content/drive/MyDrive/saudi/saudi-signfor-all-competition\",\n",
        "                      help=\"Directory containing H5/CSV/JSON data files (Colab Path)\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"/content/drive/MyDrive/saudi/output\",\n",
        "                      help=\"Directory to save trained models and logs (Colab Path)\")\n",
        "    parser.add_argument(\"--results_dir\", type=str, default=\"/content/drive/MyDrive/saudi/results\",\n",
        "                      help=\"Directory to save evaluation results and predictions (Colab Path)\")\n",
        "\n",
        "    # File names exactly as provided\n",
        "    parser.add_argument(\"--train_h5_file\", type=str, default=\"SSL.keypoints.train_signers_train_sentences.0.h5\",\n",
        "                       help=\"Filename of training H5 file (within data_dir)\")\n",
        "    parser.add_argument(\"--train_csv_file\", type=str, default=\"SSL.keypoints.train_signers_train_sentences.csv\",\n",
        "                       help=\"Filename of training CSV file (within data_dir)\")\n",
        "    parser.add_argument(\"--test_h5_file\", type=str, default=\"SSL.keypoints.test_signers_test_sentences.h5\",\n",
        "                       help=\"Filename of test H5 file (within data_dir)\")\n",
        "    parser.add_argument(\"--test_json_file\", type=str, default=\"SSL.keypoints.test_signers_test_sentences.json\",\n",
        "                       help=\"Filename of test JSON file (within data_dir)\")\n",
        "\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed for reproducibility\")\n",
        "    parser.add_argument(\"--mbart_model_name\", type=str, default=\"facebook/mbart-large-50-many-to-many-mmt\", help=\"Pre-trained mBART model name\")\n",
        "    parser.add_argument(\"--target_lang\", type=str, default=\"ar_AR\", help=\"mBART language code for Arabic\")\n",
        "    parser.add_argument(\"--feature_dim\", type=int, default=208, help=\"Keypoint feature dimension (check H5 files)\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=15, help=\"Number of training epochs\")\n",
        "    parser.add_argument(\"--warmup_steps\", type=int, default=500, help=\"Number of warmup steps for learning rate scheduler\")\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.01, help=\"Weight decay for optimizer\")\n",
        "    parser.add_argument(\"--logging_steps\", type=int, default=100, help=\"Log training info every N steps\")\n",
        "    parser.add_argument(\"--save_total_limit\", type=int, default=2, help=\"Maximum number of checkpoints to save\")\n",
        "    parser.add_argument(\"--fp16\", action=\"store_true\", default=True, help=\"Enable FP16 training\")\n",
        "\n",
        "    # A100 GPU optimizations\n",
        "    parser.add_argument(\"--gradient_checkpointing\", action=\"store_true\", default=False, help=\"Enable gradient checkpointing\")\n",
        "    parser.add_argument(\"--max_seq_length\", type=int, default=512, help=\"Max sequence length for keypoints\")\n",
        "    parser.add_argument(\"--max_label_length\", type=int, default=128, help=\"Max sequence length for target Arabic text\")\n",
        "    parser.add_argument(\"--train_batch_size\", type=int, default=4, help=\"Training batch size per device\")\n",
        "    parser.add_argument(\"--eval_batch_size\", type=int, default=16, help=\"Evaluation batch size per device\")\n",
        "    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, help=\"Number of steps to accumulate gradients\")\n",
        "    parser.add_argument(\"--num_beams\", type=int, default=5, help=\"Number of beams for generation during evaluation\")\n",
        "\n",
        "    # Use parse_known_args to ignore extra arguments from environments like Jupyter\n",
        "    args, unknown = parser.parse_known_args()\n",
        "    if unknown:\n",
        "        print(f\"Ignoring unrecognized arguments: {unknown}\")\n",
        "\n",
        "    # Calculate effective batch size\n",
        "    num_devices = torch.cuda.device_count() if torch.cuda.is_available() else 1\n",
        "    effective_batch_size = args.train_batch_size * args.gradient_accumulation_steps * num_devices\n",
        "    print(f\"Effective batch size: {effective_batch_size} (train_bs={args.train_batch_size} * grad_accum={args.gradient_accumulation_steps} * devices={num_devices})\")\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(args.data_dir, exist_ok=True)\n",
        "    os.makedirs(args.output_dir, exist_ok=True)\n",
        "    os.makedirs(args.results_dir, exist_ok=True)\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def set_seed(seed_value):\n",
        "    \"\"\"Set random seed for reproducibility across libraries.\"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed_value)\n",
        "    transformers_set_seed(seed_value)\n",
        "\n",
        "def check_and_prompt_for_file(file_path, file_type):\n",
        "    \"\"\"Check if a file exists and prompt for an alternative path if not found.\"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"Found {file_type} file: {file_path}\")\n",
        "        return file_path\n",
        "\n",
        "    print(f\"ERROR: {file_type} file not found at {file_path}\")\n",
        "\n",
        "    user_input = None\n",
        "    while True:\n",
        "        user_input = input(f\"Please enter an alternative path for the {file_type} file or type 'skip' to bypass: \")\n",
        "\n",
        "        if user_input.lower() == 'skip':\n",
        "            print(f\"Skipping {file_type} file\")\n",
        "            return None\n",
        "\n",
        "        if os.path.exists(user_input):\n",
        "            print(f\"Found {file_type} file at: {user_input}\")\n",
        "            return user_input\n",
        "        else:\n",
        "            print(f\"File not found at: {user_input}\")\n",
        "\n",
        "def preload_h5_info(file_path):\n",
        "    \"\"\"\n",
        "    Scans an H5 file and extracts key information:\n",
        "    - List of valid keys\n",
        "    - Mapping of keys to sequence lengths\n",
        "    - Feature dimension\n",
        "    \"\"\"\n",
        "    keys = []\n",
        "    lengths = {}\n",
        "    feature_dim = None\n",
        "    if not file_path or not os.path.exists(file_path):\n",
        "        print(f\"Error: H5 path {file_path} invalid or not found.\")\n",
        "        return keys, lengths, feature_dim\n",
        "    try:\n",
        "        with h5py.File(file_path, \"r\") as f:\n",
        "            print(f\"Scanning HDF5 {file_path}...\")\n",
        "            valid_keys = list(f.keys())\n",
        "            for key in tqdm(valid_keys, desc=\"Preloading HDF5 info\"):\n",
        "                try:\n",
        "                    item = f[key]\n",
        "                    seq_len = 0\n",
        "                    current_dim = 0\n",
        "\n",
        "                    # Handle potential group structure\n",
        "                    if isinstance(item, h5py.Group):\n",
        "                        data_keys = list(item.keys())\n",
        "                        if data_keys:\n",
        "                            data = item[data_keys[0]]\n",
        "                            if isinstance(data, h5py.Dataset):\n",
        "                                seq_len = data.shape[0]\n",
        "                                if len(data.shape) > 1:\n",
        "                                    current_dim = data.shape[1]\n",
        "                        else:\n",
        "                             continue # Skip empty group\n",
        "                    elif isinstance(item, h5py.Dataset):\n",
        "                        seq_len = item.shape[0]\n",
        "                        if len(item.shape) > 1:\n",
        "                            current_dim = item.shape[1]\n",
        "                    else:\n",
        "                        continue # Skip if not a group or dataset\n",
        "\n",
        "                    if seq_len > 0:\n",
        "                        keys.append(key)\n",
        "                        lengths[key] = seq_len\n",
        "                        if feature_dim is None and current_dim > 0:\n",
        "                            feature_dim = current_dim\n",
        "                        elif feature_dim is not None and current_dim != feature_dim and current_dim > 0:\n",
        "                             pass\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing key {key}: {e}\")\n",
        "\n",
        "            print(f\"Found {len(keys)} non-empty keys in HDF5.\")\n",
        "            if feature_dim is not None:\n",
        "                print(f\"Detected primary feature dimension: {feature_dim}\")\n",
        "            else:\n",
        "                print(\"Warning: Could not detect feature dimension from H5 file.\")\n",
        "\n",
        "            return keys, lengths, feature_dim\n",
        "    except Exception as e:\n",
        "        print(f\"Error opening HDF5 {file_path}: {e}\")\n",
        "        return keys, lengths, feature_dim\n",
        "\n",
        "def extract_signer_sentence_ids(key):\n",
        "    \"\"\"\n",
        "    Extract signer ID and sentence ID from the key format (e.g., '00_0001').\n",
        "\n",
        "    Args:\n",
        "        key (str): The key in format 'XX_YYYY'\n",
        "\n",
        "    Returns:\n",
        "        tuple: (signer_id, sentence_id) or (None, None) if format doesn't match\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if '_' in key:\n",
        "            parts = key.split('_')\n",
        "            if len(parts) == 2:\n",
        "                signer_id = parts[0]\n",
        "                sentence_id = parts[1]\n",
        "                return signer_id, sentence_id\n",
        "    except Exception:\n",
        "        pass\n",
        "    return None, None\n",
        "\n",
        "def create_validation_splits_with_strict_separation(train_h5_keys, id_to_sentence_map, train_h5_lengths, max_seq_length, seed=42):\n",
        "    \"\"\"\n",
        "    Create validation splits with STRICT separation from training data.\n",
        "    This ensures validation data is NEVER used in training.\n",
        "\n",
        "    Args:\n",
        "        train_h5_keys: List of keys from training H5 file\n",
        "        id_to_sentence_map: Mapping from keys to sentence translations\n",
        "        train_h5_lengths: Mapping from keys to sequence lengths\n",
        "        max_seq_length: Maximum sequence length to include\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of validation splits and metadata\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Creating Validation Splits with STRICT Separation ---\")\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Filter keys by length and valid translations\n",
        "    valid_keys = [key for key in train_h5_keys\n",
        "                  if key in id_to_sentence_map\n",
        "                  and id_to_sentence_map[key]\n",
        "                  and isinstance(id_to_sentence_map[key], str)\n",
        "                  and id_to_sentence_map[key].strip()\n",
        "                  and key in train_h5_lengths\n",
        "                  and train_h5_lengths[key] <= max_seq_length]\n",
        "\n",
        "    print(f\"Valid keys after filtering: {len(valid_keys)}\")\n",
        "\n",
        "    # Extract signer and sentence information\n",
        "    signer_to_keys = {}\n",
        "    sentence_to_keys = {}\n",
        "    key_info = {}\n",
        "\n",
        "    for key in valid_keys:\n",
        "        signer_id, sentence_id = extract_signer_sentence_ids(key)\n",
        "        sentence = id_to_sentence_map[key]\n",
        "\n",
        "        if signer_id is not None:\n",
        "            if signer_id not in signer_to_keys:\n",
        "                signer_to_keys[signer_id] = []\n",
        "            signer_to_keys[signer_id].append(key)\n",
        "\n",
        "        if sentence not in sentence_to_keys:\n",
        "            sentence_to_keys[sentence] = []\n",
        "        sentence_to_keys[sentence].append(key)\n",
        "\n",
        "        key_info[key] = {\n",
        "            'signer_id': signer_id,\n",
        "            'sentence_id': sentence_id,\n",
        "            'sentence': sentence\n",
        "        }\n",
        "\n",
        "    unique_signers = list(signer_to_keys.keys())\n",
        "    unique_sentences = list(sentence_to_keys.keys())\n",
        "\n",
        "    print(f\"Found {len(unique_signers)} unique signers and {len(unique_sentences)} unique sentences\")\n",
        "\n",
        "    # Create validation splits with NO OVERLAP with training\n",
        "    splits = {}\n",
        "\n",
        "    # Split 1: Standard hold-out (completely random keys)\n",
        "    train_standard, val_standard = train_test_split(\n",
        "        valid_keys, test_size=0.1, random_state=seed)\n",
        "\n",
        "    # Ensure strict separation\n",
        "    assert set(train_standard).isdisjoint(set(val_standard)), \"ERROR: Train and validation sets overlap!\"\n",
        "\n",
        "    splits['standard'] = {\n",
        "        'name': 'Standard Random Split',\n",
        "        'description': '10% of data randomly selected, completely held out from training',\n",
        "        'train_keys': train_standard,\n",
        "        'val_keys': val_standard,\n",
        "        'challenge_level': 1\n",
        "    }\n",
        "\n",
        "    print(f\"Standard split: {len(train_standard)} train, {len(val_standard)} validation samples\")\n",
        "    print(f\"Verified: Train and validation sets have NO overlap\")\n",
        "\n",
        "    # Split 2: Unseen sentences\n",
        "    if len(unique_sentences) >= 10:\n",
        "        # Hold out 10% of sentences completely\n",
        "        val_sentence_count = max(int(len(unique_sentences) * 0.1), 5)  # At least 5 sentences\n",
        "        val_sentences = set(random.sample(unique_sentences, k=val_sentence_count))\n",
        "\n",
        "        # Ensure sentences in validation are NEVER in training\n",
        "        val_unseen_sentence_keys = []\n",
        "        train_seen_sentence_keys = []\n",
        "\n",
        "        for key in valid_keys:\n",
        "            sentence = id_to_sentence_map[key]\n",
        "            if sentence in val_sentences:\n",
        "                val_unseen_sentence_keys.append(key)\n",
        "            else:\n",
        "                train_seen_sentence_keys.append(key)\n",
        "\n",
        "        # Verify strict separation\n",
        "        assert set(train_seen_sentence_keys).isdisjoint(set(val_unseen_sentence_keys)), \"ERROR: Train and unseen sentence validation sets overlap!\"\n",
        "\n",
        "        splits['unseen_sentences'] = {\n",
        "            'name': 'Unseen Sentences Split',\n",
        "            'description': f'Hold out {len(val_sentences)} complete sentences (never seen during training)',\n",
        "            'train_keys': train_seen_sentence_keys,\n",
        "            'val_keys': val_unseen_sentence_keys,\n",
        "            'challenge_level': 2,\n",
        "            'val_sentences': val_sentences  # Store these for reference\n",
        "        }\n",
        "\n",
        "        print(f\"Unseen sentences split: {len(train_seen_sentence_keys)} train, {len(val_unseen_sentence_keys)} validation samples\")\n",
        "        print(f\"Verified: Train and validation sets have NO overlap\")\n",
        "\n",
        "    # Split 3: Unseen signers\n",
        "    if len(unique_signers) >= 5:\n",
        "        # Hold out 20% of signers completely\n",
        "        val_signer_count = max(int(len(unique_signers) * 0.2), 2)  # At least 2 signers\n",
        "        val_signers = set(random.sample(unique_signers, k=val_signer_count))\n",
        "\n",
        "        # Ensure signers in validation are NEVER in training\n",
        "        val_unseen_signer_keys = []\n",
        "        train_seen_signer_keys = []\n",
        "\n",
        "        for key in valid_keys:\n",
        "            signer_id, _ = extract_signer_sentence_ids(key)\n",
        "            if signer_id in val_signers:\n",
        "                val_unseen_signer_keys.append(key)\n",
        "            else:\n",
        "                train_seen_signer_keys.append(key)\n",
        "\n",
        "        # Verify strict separation\n",
        "        assert set(train_seen_signer_keys).isdisjoint(set(val_unseen_signer_keys)), \"ERROR: Train and unseen signer validation sets overlap!\"\n",
        "\n",
        "        splits['unseen_signers'] = {\n",
        "            'name': 'Unseen Signers Split',\n",
        "            'description': f'Hold out {len(val_signers)} complete signers (never seen during training)',\n",
        "            'train_keys': train_seen_signer_keys,\n",
        "            'val_keys': val_unseen_signer_keys,\n",
        "            'challenge_level': 3,\n",
        "            'val_signers': val_signers  # Store these for reference\n",
        "        }\n",
        "\n",
        "        print(f\"Unseen signers split: {len(train_seen_signer_keys)} train, {len(val_unseen_signer_keys)} validation samples\")\n",
        "        print(f\"Verified: Train and validation sets have NO overlap\")\n",
        "\n",
        "    # Split 4: Test-like challenge (both unseen signers and sentences)\n",
        "    if 'unseen_signers' in splits and 'unseen_sentences' in splits:\n",
        "        # Extract a different set of signers and sentences\n",
        "        # We need to ensure we don't reuse the ones already held out\n",
        "        used_val_signers = set(splits['unseen_signers']['val_signers'])\n",
        "        used_val_sentences = set(splits['unseen_sentences']['val_sentences'])\n",
        "\n",
        "        remaining_signers = [s for s in unique_signers if s not in used_val_signers]\n",
        "        remaining_sentences = [s for s in unique_sentences if s not in used_val_sentences]\n",
        "\n",
        "        if len(remaining_signers) >= 2 and len(remaining_sentences) >= 5:\n",
        "            challenge_signer_count = min(val_signer_count, len(remaining_signers))\n",
        "            challenge_sentence_count = min(val_sentence_count, len(remaining_sentences))\n",
        "\n",
        "            challenge_signers = set(random.sample(remaining_signers, k=challenge_signer_count))\n",
        "            challenge_sentences = set(random.sample(remaining_sentences, k=challenge_sentence_count))\n",
        "\n",
        "            # Find keys that have EITHER challenge signers OR challenge sentences\n",
        "            # This creates a strict train/val separation while ensuring enough validation samples\n",
        "            val_challenge_keys = []\n",
        "            train_challenge_keys = []\n",
        "\n",
        "            for key in valid_keys:\n",
        "                signer_id, _ = extract_signer_sentence_ids(key)\n",
        "                sentence = id_to_sentence_map[key]\n",
        "\n",
        "                # If this key has either a challenge signer or sentence, it goes to validation\n",
        "                if (signer_id in challenge_signers) or (sentence in challenge_sentences):\n",
        "                    val_challenge_keys.append(key)\n",
        "                else:\n",
        "                    train_challenge_keys.append(key)\n",
        "\n",
        "            # Verify strict separation\n",
        "            assert set(train_challenge_keys).isdisjoint(set(val_challenge_keys)), \"ERROR: Train and challenge validation sets overlap!\"\n",
        "\n",
        "            # Only create this split if we have enough validation samples\n",
        "            if len(val_challenge_keys) >= 10:\n",
        "                splits['challenge'] = {\n",
        "                    'name': 'Test-Like Challenge Split',\n",
        "                    'description': 'Both unseen signers and unseen sentences (strictly held out from training)',\n",
        "                    'train_keys': train_challenge_keys,\n",
        "                    'val_keys': val_challenge_keys,\n",
        "                    'challenge_level': 4,\n",
        "                    'val_signers': challenge_signers,\n",
        "                    'val_sentences': challenge_sentences\n",
        "                }\n",
        "\n",
        "                print(f\"Challenge split: {len(train_challenge_keys)} train, {len(val_challenge_keys)} validation samples\")\n",
        "                print(f\"Verified: Train and validation sets have NO overlap\")\n",
        "\n",
        "    return splits, key_info\n",
        "\n",
        "\n",
        "def verify_split_separation(splits):\n",
        "    \"\"\"\n",
        "    Verify that each split maintains strict separation between its own training and validation data.\n",
        "\n",
        "    Args:\n",
        "        splits: Dictionary of validation splits\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all individual splits have strict separation, False otherwise\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Verifying Individual Split Separation ---\")\n",
        "    all_valid = True\n",
        "\n",
        "    # Check each split for strict separation between its own train and validation sets\n",
        "    for name, split_info in splits.items():\n",
        "        train_keys = set(split_info['train_keys'])\n",
        "        val_keys = set(split_info['val_keys'])\n",
        "\n",
        "        # Verify no overlap between train and val within this split\n",
        "        if not train_keys.isdisjoint(val_keys):\n",
        "            overlap = len(train_keys.intersection(val_keys))\n",
        "            print(f\"ERROR: Split '{name}' has {overlap} overlapping keys between train and validation!\")\n",
        "            all_valid = False\n",
        "        else:\n",
        "            print(f\"✓ Split '{name}' has strict separation between train and validation sets\")\n",
        "\n",
        "    # Now provide info about cross-split overlap (not an error, just informational)\n",
        "    print(\"\\n--- Cross-Split Overlap Information (Expected) ---\")\n",
        "    split_names = list(splits.keys())\n",
        "    for i, name1 in enumerate(split_names):\n",
        "        for j in range(i+1, len(split_names)):\n",
        "            name2 = split_names[j]\n",
        "            train_keys1 = set(splits[name1]['train_keys'])\n",
        "            val_keys1 = set(splits[name1]['val_keys'])\n",
        "            train_keys2 = set(splits[name2]['train_keys'])\n",
        "            val_keys2 = set(splits[name2]['val_keys'])\n",
        "\n",
        "            train_train_overlap = len(train_keys1.intersection(train_keys2))\n",
        "            val_val_overlap = len(val_keys1.intersection(val_keys2))\n",
        "\n",
        "            # Show overlap info for training and validation sets\n",
        "            print(f\"Info: Splits '{name1}' and '{name2}':\")\n",
        "            print(f\"  - {train_train_overlap} overlapping training keys\")\n",
        "            print(f\"  - {val_val_overlap} overlapping validation keys\")\n",
        "\n",
        "            # Also show cross-overlap (informational only)\n",
        "            train1_val2_overlap = len(train_keys1.intersection(val_keys2))\n",
        "            train2_val1_overlap = len(train_keys2.intersection(val_keys1))\n",
        "\n",
        "            print(f\"  - {train1_val2_overlap} keys in both '{name1}' train and '{name2}' validation\")\n",
        "            print(f\"  - {train2_val1_overlap} keys in both '{name2}' train and '{name1}' validation\")\n",
        "\n",
        "    if all_valid:\n",
        "        print(\"\\n✓ All individual splits maintain strict separation between their own train and validation sets!\")\n",
        "        print(\"NOTE: The overlap between different splits is normal and expected.\")\n",
        "        print(\"The training will use only ONE split (the most challenging one) for both training and validation.\")\n",
        "    else:\n",
        "        print(\"\\n✗ One or more splits have internal train/validation overlap!\")\n",
        "\n",
        "    return all_valid\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_and_prepare_test_data(test_h5_path, test_json_path, test_h5_keys=None, test_h5_lengths=None):\n",
        "    \"\"\"\n",
        "    Load and prepare the official test dataset\n",
        "\n",
        "    Args:\n",
        "        test_h5_path: Path to test H5 file\n",
        "        test_json_path: Path to test JSON file\n",
        "        test_h5_keys: List of keys from test H5 file (if already loaded)\n",
        "        test_h5_lengths: Mapping from keys to sequence lengths (if already loaded)\n",
        "\n",
        "    Returns:\n",
        "        list: List of valid test keys\n",
        "    \"\"\"\n",
        "    test_ids_from_json = []\n",
        "\n",
        "    # Load test IDs from JSON\n",
        "    if test_json_path and os.path.exists(test_json_path):\n",
        "        try:\n",
        "            with open(test_json_path, \"r\") as f:\n",
        "                test_data_json = json.load(f)\n",
        "                if isinstance(test_data_json, list) and test_data_json and isinstance(test_data_json[0], dict) and \"id\" in test_data_json[0]:\n",
        "                    test_ids_from_json = [item[\"id\"] for item in test_data_json]\n",
        "                    print(f\"Loaded {len(test_ids_from_json)} test IDs from {test_json_path}\")\n",
        "                else:\n",
        "                    print(f\"Warning: Test JSON format not recognized or empty at {test_json_path}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading test JSON {test_json_path}: {e}\")\n",
        "\n",
        "    # If we don't have keys/lengths yet, load them from H5\n",
        "    if test_h5_keys is None or test_h5_lengths is None:\n",
        "        if test_h5_path and os.path.exists(test_h5_path):\n",
        "            test_h5_keys, test_h5_lengths, _ = preload_h5_info(test_h5_path)\n",
        "        else:\n",
        "            print(f\"Warning: Test H5 file not found at {test_h5_path}\")\n",
        "            return []\n",
        "\n",
        "    # Find keys that exist in both H5 and JSON (if JSON was loaded)\n",
        "    valid_test_keys = []\n",
        "    if test_ids_from_json:\n",
        "        valid_test_keys = [key for key in test_h5_keys if key in test_ids_from_json and key in test_h5_lengths]\n",
        "    else:\n",
        "        valid_test_keys = [key for key in test_h5_keys if key in test_h5_lengths]\n",
        "\n",
        "    print(f\"Found {len(valid_test_keys)} valid test keys for evaluation\")\n",
        "    return valid_test_keys\n",
        "\n",
        "# --- Dataset Class ---\n",
        "class SignDataset(Dataset):\n",
        "    def __init__(self, h5_file_path, data_keys_to_use, id_to_sentence_map,\n",
        "                 tokenizer, h5_lengths_map, max_seq_length,\n",
        "                 max_label_length, feature_dim, target_lang, is_test_set=False):\n",
        "        self.h5_file_path = h5_file_path\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.max_label_length = max_label_length\n",
        "        self.feature_dim = feature_dim\n",
        "        self.target_lang = target_lang\n",
        "        self.is_test_set = is_test_set\n",
        "        self.id_to_sentence_map = id_to_sentence_map\n",
        "        self.h5_lengths_map = h5_lengths_map\n",
        "\n",
        "        self.data_info = []\n",
        "        print(f\"Initializing dataset ({'Test' if is_test_set else 'Train/Val'}). Processing {len(data_keys_to_use)} provided keys...\")\n",
        "\n",
        "        skipped_count = 0\n",
        "        skipped_length = 0\n",
        "        skipped_missing_label = 0\n",
        "        skipped_zero_length = 0\n",
        "        for key in tqdm(data_keys_to_use, desc=\"Preparing dataset samples\"):\n",
        "            original_length = self.h5_lengths_map.get(key)\n",
        "            sentence = self.id_to_sentence_map.get(key) if not self.is_test_set else key # Use key as placeholder for test\n",
        "\n",
        "            # Validation (ensure sentence exists unless it's test set)\n",
        "            if sentence is None and not self.is_test_set:\n",
        "                 skipped_missing_label += 1\n",
        "                 skipped_count += 1\n",
        "                 continue\n",
        "            # Skip empty sentences in train/val\n",
        "            if isinstance(sentence, str) and not sentence.strip() and not self.is_test_set:\n",
        "                 skipped_missing_label += 1\n",
        "                 skipped_count += 1\n",
        "                 continue\n",
        "\n",
        "            # Length validation\n",
        "            if original_length is None or original_length == 0 :\n",
        "                skipped_zero_length += 1\n",
        "                skipped_count += 1\n",
        "                continue\n",
        "            if original_length > self.max_seq_length:\n",
        "                skipped_length += 1\n",
        "                skipped_count += 1\n",
        "                continue # Skip sequences longer than max_seq_length\n",
        "\n",
        "            self.data_info.append((key, sentence, original_length))\n",
        "\n",
        "        print(f\"Finished preparing dataset: {len(self.data_info)} samples included.\")\n",
        "        print(f\"  Skipped {skipped_count} samples total ({skipped_length} due to length > {self.max_seq_length}, {skipped_zero_length} zero length, {skipped_missing_label} missing/empty label).\")\n",
        "        if not self.data_info:\n",
        "            print(\"WARNING: Dataset is empty after processing.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_info)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        key, sentence, original_length = self.data_info[idx]\n",
        "        features = None\n",
        "        actual_feature_dim = self.feature_dim # Assume this unless proven otherwise\n",
        "\n",
        "        try:\n",
        "            with h5py.File(self.h5_file_path, \"r\") as f:\n",
        "                item = f[key]\n",
        "                # Check if item is a Group or Dataset\n",
        "                if isinstance(item, h5py.Group):\n",
        "                    data_keys = list(item.keys())\n",
        "                    if data_keys:\n",
        "                        data_to_load = item[data_keys[0]]\n",
        "                    else:\n",
        "                        raise ValueError(f\"Group {key} is empty\")\n",
        "                elif isinstance(item, h5py.Dataset):\n",
        "                    data_to_load = item\n",
        "                else:\n",
        "                    raise ValueError(f\"Key {key} is not a Group or Dataset\")\n",
        "\n",
        "                if data_to_load is not None and isinstance(data_to_load, h5py.Dataset):\n",
        "                    features_np = data_to_load[()] # Load data into numpy array\n",
        "\n",
        "                    # Handle potential dimension mismatch if possible (e.g., flattened)\n",
        "                    current_dim = features_np.shape[1] if len(features_np.shape) > 1 else 0\n",
        "                    if len(features_np.shape) < 2 or current_dim == 0:\n",
        "                        if len(features_np.shape) == 1 and self.feature_dim > 0 and features_np.shape[0] % self.feature_dim == 0:\n",
        "                            seq_len = features_np.shape[0] // self.feature_dim\n",
        "                            features_np = features_np.reshape(seq_len, self.feature_dim)\n",
        "                            original_length = seq_len # Update original length based on reshape\n",
        "                            current_dim = self.feature_dim\n",
        "                        else:\n",
        "                            raise ValueError(f\"Unexpected shape or zero dim for key {key}. Expected (*, {self.feature_dim}), Got {features_np.shape}\")\n",
        "                    elif current_dim != self.feature_dim:\n",
        "                         raise ValueError(f\"Feature dim mismatch for key {key}. Expected {self.feature_dim}, Got {current_dim}\")\n",
        "\n",
        "                    features = torch.tensor(features_np, dtype=torch.float32)\n",
        "                else:\n",
        "                    raise ValueError(f\"No valid dataset found for key {key}\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"Error loading key {key} from {self.h5_file_path}: {e}\")\n",
        "             raise RuntimeError(f\"Failed to load data for key {key}\") from e\n",
        "\n",
        "        assert original_length <= self.max_seq_length, f\"Key {key} has length {original_length} > max_seq_length {self.max_seq_length} despite filtering.\"\n",
        "\n",
        "        # Padding\n",
        "        pad_value = 0.0\n",
        "        num_padding = self.max_seq_length - original_length\n",
        "\n",
        "        if num_padding > 0:\n",
        "            padded_features = F.pad(features, (0, 0, 0, num_padding), value=pad_value)\n",
        "        else: # num_padding should be 0 here\n",
        "            padded_features = features\n",
        "\n",
        "        # Create attention mask for features (1 for real data, 0 for padding)\n",
        "        feature_attention_mask = torch.zeros(self.max_seq_length, dtype=torch.long)\n",
        "        feature_attention_mask[:original_length] = 1\n",
        "\n",
        "        # Prepare labels\n",
        "        if self.is_test_set:\n",
        "            # FIX: Create dummy inputs/labels with proper length instead of just length 1\n",
        "            # Use max_label_length instead of a hardcoded short length\n",
        "            input_ids = torch.full((self.max_label_length,), self.tokenizer.pad_token_id, dtype=torch.long)\n",
        "            label_attention_mask = torch.zeros(self.max_label_length, dtype=torch.long)\n",
        "            # Set first position to non-padding\n",
        "            input_ids[0] = self.tokenizer.bos_token_id if hasattr(self.tokenizer, 'bos_token_id') else self.tokenizer.pad_token_id\n",
        "            label_attention_mask[0] = 1\n",
        "            # Create labels with proper length (-100 for loss masking)\n",
        "            labels_output = input_ids.clone()\n",
        "            labels_output[labels_output == self.tokenizer.pad_token_id] = -100\n",
        "        else:\n",
        "            self.tokenizer.tgt_lang = self.target_lang\n",
        "            tokenized_output = self.tokenizer(\n",
        "                str(sentence), # Ensure it's a string\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "                max_length=self.max_label_length,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            input_ids = tokenized_output[\"input_ids\"].squeeze(0) # Remove batch dim\n",
        "            label_attention_mask = tokenized_output[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "            # Create labels for loss calculation (-100 for padding)\n",
        "            labels_output = input_ids.clone()\n",
        "            labels_output[labels_output == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"input_features\": padded_features,\n",
        "            \"attention_mask\": feature_attention_mask,\n",
        "            \"labels\": labels_output, # Needs to be present, even if dummy for test\n",
        "            \"decoder_attention_mask\": label_attention_mask,\n",
        "            \"key\": key # Keep key for potential debugging/mapping later\n",
        "        }\n",
        "\n",
        "def create_datasets_from_splits(splits, train_h5_path, id_to_sentence_map, train_h5_lengths,\n",
        "                               tokenizer, max_seq_length, max_label_length, feature_dim, target_lang,\n",
        "                               test_h5_path=None, test_h5_lengths=None):\n",
        "    \"\"\"\n",
        "    Create training and validation datasets from the splits.\n",
        "\n",
        "    Args:\n",
        "        splits: Dictionary of validation splits\n",
        "        train_h5_path: Path to training H5 file\n",
        "        id_to_sentence_map: Mapping from keys to sentence translations\n",
        "        train_h5_lengths: Mapping from keys to sequence lengths\n",
        "        tokenizer: The tokenizer\n",
        "        max_seq_length: Maximum sequence length\n",
        "        max_label_length: Maximum label length\n",
        "        feature_dim: Feature dimension\n",
        "        target_lang: Target language\n",
        "        test_h5_path: Path to test H5 file\n",
        "        test_h5_lengths: Mapping from test keys to sequence lengths\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of datasets for each split\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Creating Datasets from Splits ---\")\n",
        "\n",
        "    datasets = {}\n",
        "\n",
        "    for split_name, split_info in splits.items():\n",
        "        print(f\"\\nCreating datasets for '{split_info['name']}' split:\")\n",
        "\n",
        "        train_keys = split_info['train_keys']\n",
        "        val_keys = split_info['val_keys']\n",
        "\n",
        "        # Determine if this is using test data\n",
        "        is_test_split = 'test' in split_name and test_h5_path is not None\n",
        "        h5_path = test_h5_path if is_test_split else train_h5_path\n",
        "        h5_lengths = test_h5_lengths if is_test_split else train_h5_lengths\n",
        "\n",
        "        # For test validation, we don't have translations\n",
        "        empty_map = {}\n",
        "\n",
        "        # Create training dataset\n",
        "        train_dataset = None\n",
        "        if not is_test_split and train_keys:\n",
        "            try:\n",
        "                train_dataset = SignDataset(\n",
        "                    h5_file_path=h5_path,\n",
        "                    data_keys_to_use=train_keys,\n",
        "                    id_to_sentence_map=id_to_sentence_map,\n",
        "                    tokenizer=tokenizer,\n",
        "                    h5_lengths_map=h5_lengths,\n",
        "                    max_seq_length=max_seq_length,\n",
        "                    max_label_length=max_label_length,\n",
        "                    feature_dim=feature_dim,\n",
        "                    target_lang=target_lang,\n",
        "                    is_test_set=False\n",
        "                )\n",
        "                print(f\"  Created training dataset with {len(train_dataset)} samples\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error creating training dataset: {e}\")\n",
        "\n",
        "        # Create validation dataset\n",
        "        val_dataset = None\n",
        "        if val_keys:\n",
        "            try:\n",
        "                val_dataset = SignDataset(\n",
        "                    h5_file_path=h5_path,\n",
        "                    data_keys_to_use=val_keys,\n",
        "                    # Use empty map for test validation to mark as test data\n",
        "                    id_to_sentence_map=empty_map if is_test_split else id_to_sentence_map,\n",
        "                    tokenizer=tokenizer,\n",
        "                    h5_lengths_map=h5_lengths,\n",
        "                    max_seq_length=max_seq_length,\n",
        "                    max_label_length=max_label_length,\n",
        "                    feature_dim=feature_dim,\n",
        "                    target_lang=target_lang,\n",
        "                    is_test_set=is_test_split\n",
        "                )\n",
        "                print(f\"  Created validation dataset with {len(val_dataset)} samples\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Error creating validation dataset: {e}\")\n",
        "\n",
        "        datasets[split_name] = {\n",
        "            'train': train_dataset,\n",
        "            'validation': val_dataset,\n",
        "            'info': split_info\n",
        "        }\n",
        "\n",
        "    return datasets\n",
        "\n",
        "# --- Custom Data Collator --- #\n",
        "@dataclass\n",
        "class CustomDataCollator:\n",
        "    tokenizer: PreTrainedTokenizerBase\n",
        "    padding: Union[bool, str, PaddingStrategy] = True\n",
        "    max_length: Optional[int] = None # Not used for features here, padding done in dataset\n",
        "    pad_to_multiple_of: Optional[int] = None # Applied to labels\n",
        "    label_pad_token_id: int = -100\n",
        "    return_tensors: str = \"pt\"\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        input_features = [f[\"input_features\"] for f in features]\n",
        "        attention_mask = [f[\"attention_mask\"] for f in features]\n",
        "        labels = [f[\"labels\"] for f in features] # These are the ones to pad dynamically\n",
        "        decoder_attention_mask = [f[\"decoder_attention_mask\"] for f in features]\n",
        "\n",
        "        batch_input_features = torch.stack(input_features)\n",
        "        batch_attention_mask = torch.stack(attention_mask)\n",
        "\n",
        "        max_label_len = max(len(lab) for lab in labels)\n",
        "        if self.pad_to_multiple_of is not None:\n",
        "            max_label_len = (\n",
        "                (max_label_len + self.pad_to_multiple_of - 1)\n",
        "                // self.pad_to_multiple_of\n",
        "                * self.pad_to_multiple_of\n",
        "            )\n",
        "\n",
        "        padded_labels = []\n",
        "        padded_decoder_attention_mask = []\n",
        "        for lab, dec_attn in zip(labels, decoder_attention_mask):\n",
        "            remainder = max_label_len - len(lab)\n",
        "            padded_labels.append(F.pad(lab, (0, remainder), value=self.label_pad_token_id))\n",
        "            padded_decoder_attention_mask.append(F.pad(dec_attn, (0, remainder), value=0))\n",
        "\n",
        "        batch_labels = torch.stack(padded_labels)\n",
        "        batch_decoder_attention_mask = torch.stack(padded_decoder_attention_mask)\n",
        "\n",
        "        batch = {\n",
        "            \"input_features\": batch_input_features,\n",
        "            \"attention_mask\": batch_attention_mask,\n",
        "            \"labels\": batch_labels,\n",
        "            \"decoder_attention_mask\": batch_decoder_attention_mask,\n",
        "        }\n",
        "        return batch\n",
        "\n",
        "# --- Model Definition --- #\n",
        "class VisualHead(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(input_dim, output_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "        print(f\"Initialized VisualHead: Linear({input_dim} -> {output_dim})\")\n",
        "\n",
        "    def forward(self, features):\n",
        "        projected_features = self.projection(features)\n",
        "        return projected_features\n",
        "\n",
        "class SignTranslationModel(nn.Module):\n",
        "    def __init__(self, feature_dim, mbert_model_name, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        print(f\"Initializing SignTranslationModel:\")\n",
        "        print(f\"  Loading Base mBART Model From: {mbert_model_name}\")\n",
        "        print(f\"  Feature Dim (Input): {feature_dim}\")\n",
        "\n",
        "        try:\n",
        "            # CRUCIAL CHANGE: Set config before loading model\n",
        "            from transformers import MBartConfig\n",
        "            config = MBartConfig.from_pretrained(mbert_model_name)\n",
        "\n",
        "            # Disable SDPA in config before loading\n",
        "            config._attn_implementation = \"eager\"\n",
        "\n",
        "            # Now load with our config\n",
        "            self.mbart = MBartForConditionalGeneration.from_pretrained(\n",
        "                mbert_model_name,\n",
        "                config=config\n",
        "            )\n",
        "            print(f\"  Base mBART model loaded successfully with eager attention.\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR loading base mBART model from {mbert_model_name}: {e}\")\n",
        "            raise\n",
        "\n",
        "        self.model_hidden_dim = self.mbart.config.d_model\n",
        "        print(f\"  Model Hidden Dim (d_model): {self.model_hidden_dim}\")\n",
        "\n",
        "        self.visual_head = VisualHead(feature_dim, self.model_hidden_dim, dropout_rate)\n",
        "\n",
        "        # Initialize generation_config\n",
        "        self.generation_config = None  # Will be set later in training setup\n",
        "\n",
        "    def forward(self, input_features, attention_mask, labels=None, decoder_attention_mask=None, **kwargs):\n",
        "        input_features = input_features.float()\n",
        "        try:\n",
        "            inputs_embeds = self.visual_head(input_features)\n",
        "        except Exception as e:\n",
        "            print(f\"VisualHead projection error: {e}\\nInput shape: {input_features.shape}, Input dtype: {input_features.dtype}\")\n",
        "            raise\n",
        "\n",
        "        try:\n",
        "            outputs = self.mbart(\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels,\n",
        "                decoder_attention_mask=decoder_attention_mask,\n",
        "                return_dict=True\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"mBART forward error: {e}\")\n",
        "            print(f\"  inputs_embeds shape: {inputs_embeds.shape if inputs_embeds is not None else 'None'}, dtype: {inputs_embeds.dtype if inputs_embeds is not None else 'None'}\")\n",
        "            print(f\"  attention_mask shape: {attention_mask.shape if attention_mask is not None else 'None'}, dtype: {attention_mask.dtype if attention_mask is not None else 'None'}\")\n",
        "            print(f\"  labels shape: {labels.shape if labels is not None else 'None'}, dtype: {labels.dtype if labels is not None else 'None'}\")\n",
        "            print(f\"  decoder_attention_mask shape: {decoder_attention_mask.shape if decoder_attention_mask is not None else 'None'}, dtype: {decoder_attention_mask.dtype if decoder_attention_mask is not None else 'None'}\")\n",
        "            raise\n",
        "        return outputs\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_features, attention_mask, **generate_kwargs):\n",
        "        self.eval()\n",
        "        input_features = input_features.float() # Ensure float input\n",
        "        inputs_embeds = self.visual_head(input_features)\n",
        "\n",
        "        # ATTENTION MASK FIX - Ensure correct shape for generate\n",
        "        # During beam search, the attention mask shape can cause issues\n",
        "        # We need to ensure it's correctly shaped for the mBART model\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Get batch size\n",
        "            batch_size = attention_mask.size(0)\n",
        "\n",
        "            # Ensure attention mask is the correct shape by reshaping\n",
        "            attention_mask = attention_mask.view(batch_size, -1)\n",
        "\n",
        "            # Debug log the shape\n",
        "            # print(f\"Generate: Attention mask reshaped to {attention_mask.shape}\")\n",
        "\n",
        "        decoder_start_token_id = self.mbart.config.forced_bos_token_id # Default\n",
        "        if \"tgt_lang\" in generate_kwargs:\n",
        "            try:\n",
        "                lang_code = generate_kwargs[\"tgt_lang\"]\n",
        "                lang_token_id = global_tokenizer.lang_code_to_id.get(lang_code)\n",
        "                if lang_token_id is not None:\n",
        "                   decoder_start_token_id = lang_token_id\n",
        "                else:\n",
        "                    print(f\"Warning: Target language '{lang_code}' not found in tokenizer lang_code_to_id. Using default BOS: {decoder_start_token_id}.\")\n",
        "            except Exception as e:\n",
        "                 print(f\"Warning: Error accessing tokenizer for target language '{generate_kwargs['tgt_lang']}': {e}. Using default BOS: {decoder_start_token_id}\")\n",
        "\n",
        "        # Use the model's generation_config if available, otherwise use defaults\n",
        "        if hasattr(self, 'generation_config') and self.generation_config is not None:\n",
        "            # Clone and update the generation config\n",
        "            from copy import deepcopy\n",
        "            gen_config = deepcopy(self.generation_config)\n",
        "\n",
        "            # Update with any kwargs provided\n",
        "            for k, v in generate_kwargs.items():\n",
        "                if hasattr(gen_config, k):\n",
        "                    setattr(gen_config, k, v)\n",
        "\n",
        "            # Extra safety check for critical parameters\n",
        "            if \"max_length\" in generate_kwargs:\n",
        "                gen_config.max_length = generate_kwargs[\"max_length\"]\n",
        "            if \"num_beams\" in generate_kwargs:\n",
        "                gen_config.num_beams = generate_kwargs[\"num_beams\"]\n",
        "            if \"decoder_start_token_id\" in generate_kwargs:\n",
        "                gen_config.decoder_start_token_id = generate_kwargs[\"decoder_start_token_id\"]\n",
        "            else:\n",
        "                gen_config.decoder_start_token_id = decoder_start_token_id\n",
        "\n",
        "            # Run generation with the config object\n",
        "            try:\n",
        "                return self.mbart.generate(\n",
        "                    inputs_embeds=inputs_embeds,\n",
        "                    attention_mask=attention_mask,\n",
        "                    generation_config=gen_config\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Generate failed with generation_config: {e}\")\n",
        "                print(\"Falling back to kwargs method...\")\n",
        "\n",
        "        # Fall back to kwargs method\n",
        "        final_generate_kwargs = {\n",
        "            \"max_length\": generate_kwargs.get(\"max_length\", self.mbart.config.max_length),\n",
        "            \"num_beams\": generate_kwargs.get(\"num_beams\", self.mbart.config.num_beams),\n",
        "            \"early_stopping\": generate_kwargs.get(\"early_stopping\", True),\n",
        "            \"decoder_start_token_id\": decoder_start_token_id,\n",
        "        }\n",
        "        for k, v in generate_kwargs.items():\n",
        "            if k not in [\"tgt_lang\", \"max_length\", \"num_beams\", \"early_stopping\", \"decoder_start_token_id\", \"forced_decoder_ids\"]:\n",
        "                 final_generate_kwargs[k] = v\n",
        "\n",
        "        return self.mbart.generate(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            **final_generate_kwargs\n",
        "        )\n",
        "\n",
        "    def save_pretrained(self, save_dir):\n",
        "        \"\"\"Save model to the specified directory.\"\"\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "        # Save mBART config\n",
        "        self.mbart.config.save_pretrained(save_dir)\n",
        "\n",
        "        # Save model state_dict\n",
        "        torch.save(self.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "\n",
        "        # Save generation config if it exists\n",
        "        if hasattr(self, 'generation_config') and self.generation_config is not None:\n",
        "            self.generation_config.save_pretrained(save_dir)\n",
        "\n",
        "        print(f\"Model saved to {save_dir}\")\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, load_dir, device=None):\n",
        "        \"\"\"Load model from the specified directory.\"\"\"\n",
        "        # Load config\n",
        "        from transformers import MBartConfig\n",
        "        config = MBartConfig.from_pretrained(load_dir)\n",
        "\n",
        "        # Determine feature_dim (needed to initialize the model)\n",
        "        # This might be stored in the config or we need to infer it\n",
        "        feature_dim = getattr(config, \"feature_dim\", 208)  # Default to 208 if not found\n",
        "\n",
        "        # Create new model instance\n",
        "        model = cls(\n",
        "            feature_dim=feature_dim,\n",
        "            mbert_model_name=load_dir\n",
        "        )\n",
        "\n",
        "        # Load state_dict\n",
        "        state_dict_path = os.path.join(load_dir, \"pytorch_model.bin\")\n",
        "        if os.path.exists(state_dict_path):\n",
        "            state_dict = torch.load(state_dict_path, map_location=\"cpu\")\n",
        "            model.load_state_dict(state_dict)\n",
        "            print(f\"Model weights loaded from {state_dict_path}\")\n",
        "        else:\n",
        "            print(f\"Warning: No model weights found at {state_dict_path}\")\n",
        "\n",
        "        # Load generation config if it exists\n",
        "        generation_config_path = os.path.join(load_dir, \"generation_config.json\")\n",
        "        if os.path.exists(generation_config_path):\n",
        "            model.generation_config = GenerationConfig.from_pretrained(load_dir)\n",
        "            print(f\"Generation config loaded from {generation_config_path}\")\n",
        "\n",
        "        # Move to device if specified\n",
        "        if device is not None:\n",
        "            model = model.to(device)\n",
        "\n",
        "        return model\n",
        "\n",
        "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
        "        \"\"\"Enable gradient checkpointing for memory efficiency.\"\"\"\n",
        "        print(\"Enabling gradient checkpointing on underlying mBART model...\")\n",
        "        if hasattr(self.mbart, \"gradient_checkpointing_enable\"):\n",
        "            self.mbart.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
        "            print(\"Gradient checkpointing enabled on mBART.\")\n",
        "        else:\n",
        "            print(\"Warning: Underlying mBART model does not have 'gradient_checkpointing_enable' method.\")\n",
        "\n",
        "# --- Evaluation Metrics --- #\n",
        "bleu_metric = evaluate.load(\"sacrebleu\")\n",
        "global_tokenizer = None # To be set after tokenizer initialization\n",
        "\n",
        "def postprocess_text(preds, labels):\n",
        "    \"\"\"Process predictions and labels for evaluation.\"\"\"\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"Compute BLEU score for evaluation.\"\"\"\n",
        "    global global_tokenizer\n",
        "    preds, labels = eval_preds\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0] # Predictions are often tuples\n",
        "\n",
        "    labels = np.where(labels != -100, labels, global_tokenizer.pad_token_id)\n",
        "\n",
        "    try:\n",
        "        decoded_preds = global_tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding predictions: {e}\")\n",
        "        decoded_preds = [\"<DECODE_ERROR>\"] * len(preds)\n",
        "\n",
        "    try:\n",
        "        decoded_labels = global_tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error decoding labels: {e}\")\n",
        "        decoded_labels = [\"<DECODE_ERROR>\"] * len(labels)\n",
        "\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    try:\n",
        "        if not isinstance(decoded_preds, list) or not all(isinstance(p, str) for p in decoded_preds):\n",
        "             raise ValueError(\"Decoded predictions are not in the expected format (List[str]).\")\n",
        "        if not isinstance(decoded_labels, list) or not all(isinstance(ref, list) and all(isinstance(s, str) for s in ref) for ref in decoded_labels):\n",
        "            raise ValueError(\"Decoded labels are not in the expected format (List[List[str]]).\")\n",
        "\n",
        "        result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "        result = {\"bleu\": result[\"score\"]}\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing BLEU: {e}\")\n",
        "        result = {\"bleu\": 0.0} # Default to 0 on error\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != global_tokenizer.pad_token_id) for pred in preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    return result\n",
        "\n",
        "def train_and_evaluate_with_strict_separation(model, datasets, args, tokenizer, data_collator):\n",
        "    \"\"\"\n",
        "    Train and evaluate the model with strict separation between training and validation.\n",
        "\n",
        "    Args:\n",
        "        model: The model to train\n",
        "        datasets: Dictionary of datasets for each split\n",
        "        args: Training arguments\n",
        "        tokenizer: The tokenizer\n",
        "        data_collator: The data collator\n",
        "\n",
        "    Returns:\n",
        "        The trained model\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Training and Evaluation with Strict Separation ===\")\n",
        "\n",
        "    # Create results directory\n",
        "    os.makedirs(args.results_dir, exist_ok=True)\n",
        "    results_file = os.path.join(args.results_dir, \"training_results.txt\")\n",
        "\n",
        "    # Select primary split for training (highest challenge level with valid datasets)\n",
        "    available_splits = [(name, info['info']['challenge_level'])\n",
        "                       for name, info in datasets.items()\n",
        "                       if info['train'] is not None and info['validation'] is not None]\n",
        "\n",
        "    if not available_splits:\n",
        "        print(\"ERROR: No valid splits found for training. Aborting.\")\n",
        "        return model\n",
        "\n",
        "    # Sort by challenge level (descending)\n",
        "    available_splits.sort(key=lambda x: x[1], reverse=True)\n",
        "    primary_split = available_splits[0][0]\n",
        "\n",
        "    primary_train = datasets[primary_split]['train']\n",
        "    primary_validation = datasets[primary_split]['validation']\n",
        "    split_info = datasets[primary_split]['info']\n",
        "\n",
        "    print(f\"\\nUsing '{split_info['name']}' as primary split for training.\")\n",
        "    print(f\"Challenge level: {split_info['challenge_level']}\")\n",
        "    print(f\"Description: {split_info['description']}\")\n",
        "    print(f\"Training samples: {len(primary_train)}\")\n",
        "    print(f\"Validation samples: {len(primary_validation)}\")\n",
        "\n",
        "    # Setup logging\n",
        "    with open(results_file, \"w\") as f:\n",
        "        f.write(\"=== Sign Language Translation Training Results ===\\n\\n\")\n",
        "        f.write(f\"Model: mBART with Visual Head\\n\")\n",
        "        f.write(f\"Primary split: {split_info['name']}\\n\")\n",
        "        f.write(f\"Challenge level: {split_info['challenge_level']}\\n\")\n",
        "        f.write(f\"Description: {split_info['description']}\\n\")\n",
        "        f.write(f\"Training samples: {len(primary_train)}\\n\")\n",
        "        f.write(f\"Validation samples: {len(primary_validation)}\\n\\n\")\n",
        "\n",
        "        f.write(\"Other available splits:\\n\")\n",
        "        for name, info in datasets.items():\n",
        "            if name != primary_split and info['train'] is not None and info['validation'] is not None:\n",
        "                f.write(f\"- {info['info']['name']}: {len(info['train'])} train, {len(info['validation'])} validation\\n\")\n",
        "                f.write(f\"  {info['info']['description']}\\n\")\n",
        "\n",
        "        f.write(\"\\n=== TRAINING PROGRESS ===\\n\")\n",
        "\n",
        "    # Setup target language tokens\n",
        "    lang_token_id = tokenizer.lang_code_to_id.get(args.target_lang)\n",
        "    if lang_token_id is None:\n",
        "        print(f\"Warning: Target language code '{args.target_lang}' not found in tokenizer's lang_code_to_id map.\")\n",
        "        lang_token_id = model.mbart.config.decoder_start_token_id\n",
        "        print(f\"Using fallback decoder_start_token_id: {lang_token_id}\")\n",
        "\n",
        "    # Generation config\n",
        "    generation_config = GenerationConfig(\n",
        "        max_length=args.max_label_length,\n",
        "        num_beams=args.num_beams,\n",
        "        early_stopping=True,\n",
        "        decoder_start_token_id=lang_token_id,\n",
        "        forced_bos_token_id=lang_token_id,\n",
        "    )\n",
        "\n",
        "    model.generation_config = generation_config\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        num_train_epochs=args.epochs,\n",
        "        per_device_train_batch_size=args.train_batch_size,\n",
        "        per_device_eval_batch_size=args.eval_batch_size,\n",
        "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
        "        warmup_steps=args.warmup_steps,\n",
        "        weight_decay=args.weight_decay,\n",
        "        logging_dir=os.path.join(args.output_dir, \"logs\"),\n",
        "        logging_steps=args.logging_steps,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=args.save_total_limit,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"bleu\",\n",
        "        greater_is_better=True,\n",
        "        predict_with_generate=True,\n",
        "        generation_max_length=args.max_label_length,\n",
        "        generation_num_beams=args.num_beams,\n",
        "        fp16=args.fp16 and torch.cuda.is_available(),\n",
        "        gradient_checkpointing=args.gradient_checkpointing,\n",
        "        report_to=[\"tensorboard\"],\n",
        "        dataloader_num_workers=2,\n",
        "        dataloader_pin_memory=True if torch.cuda.is_available() else False,\n",
        "        save_safetensors=False,\n",
        "    )\n",
        "\n",
        "    # Initialize trainer with primary validation dataset\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=primary_train,\n",
        "        eval_dataset=primary_validation,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Early stopping setup\n",
        "    early_stopping_patience = 3\n",
        "    best_epoch = 0\n",
        "    best_bleu = 0\n",
        "    no_improvement_count = 0\n",
        "\n",
        "    # Train the model\n",
        "    print(\"\\n=== Starting Training ===\")\n",
        "\n",
        "    # Check for existing checkpoints\n",
        "    checkpoint_dir = args.output_dir\n",
        "    resume_from_checkpoint = False\n",
        "\n",
        "    # Look for checkpoint directories\n",
        "    if os.path.exists(checkpoint_dir):\n",
        "        checkpoint_dirs = [d for d in os.listdir(checkpoint_dir)\n",
        "                          if os.path.isdir(os.path.join(checkpoint_dir, d))\n",
        "                          and d.startswith(\"checkpoint-\")]\n",
        "\n",
        "        if checkpoint_dirs:\n",
        "            # Sort checkpoints by step number\n",
        "            checkpoint_dirs.sort(key=lambda x: int(x.split(\"-\")[1]))\n",
        "            latest_checkpoint = os.path.join(checkpoint_dir, checkpoint_dirs[-1])\n",
        "            print(f\"\\nFound existing checkpoint: {latest_checkpoint}\")\n",
        "            resume_choice = input(\"Do you want to resume training from this checkpoint? (y/n): \").lower()\n",
        "\n",
        "            if resume_choice == 'y':\n",
        "                resume_from_checkpoint = latest_checkpoint\n",
        "                print(f\"Will resume training from: {resume_from_checkpoint}\")\n",
        "            else:\n",
        "                print(\"Starting training from scratch.\")\n",
        "        else:\n",
        "            print(\"No checkpoints found. Starting training from scratch.\")\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        print(f\"\\n--- Epoch {epoch + 1}/{args.epochs} ---\")\n",
        "\n",
        "        # Train for one epoch - modified to support checkpoint resumption\n",
        "        train_result = trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
        "\n",
        "        # Only use checkpoint for first epoch, then set to False\n",
        "        if resume_from_checkpoint:\n",
        "            resume_from_checkpoint = False\n",
        "\n",
        "        train_metrics = train_result.metrics\n",
        "\n",
        "        print(f\"Training loss: {train_metrics['train_loss']:.4f}\")\n",
        "\n",
        "        # Evaluate on all validation splits\n",
        "        with open(results_file, \"a\") as f:\n",
        "            f.write(f\"\\nEpoch {epoch + 1}\\n\")\n",
        "            f.write(f\"Training loss: {train_metrics['train_loss']:.4f}\\n\")\n",
        "\n",
        "        # Track best model\n",
        "        current_bleu = None\n",
        "\n",
        "        # Evaluate on each split's validation set\n",
        "        for split_name, split_data in datasets.items():\n",
        "            val_dataset = split_data['validation']\n",
        "\n",
        "            if val_dataset is None:\n",
        "                continue\n",
        "\n",
        "            split_desc = split_data['info']['name']\n",
        "            is_test_split = 'test' in split_name\n",
        "\n",
        "            if is_test_split:\n",
        "                # For test validation, we can only generate predictions, not evaluate\n",
        "                print(f\"\\nGenerating predictions for '{split_desc}' validation\")\n",
        "                predictions = trainer.predict(\n",
        "                    test_dataset=val_dataset,\n",
        "                    metric_key_prefix=f\"val_{split_name}\",\n",
        "                )\n",
        "\n",
        "                with open(results_file, \"a\") as f:\n",
        "                    f.write(f\"{split_desc}: Generated predictions only (no reference translations)\\n\")\n",
        "            else:\n",
        "                # For regular validation, evaluate with metrics\n",
        "                print(f\"\\nEvaluating on '{split_desc}' validation\")\n",
        "                val_metrics = trainer.evaluate(\n",
        "                    eval_dataset=val_dataset,\n",
        "                    metric_key_prefix=f\"val_{split_name}\",\n",
        "                )\n",
        "\n",
        "                bleu_score = val_metrics[f\"val_{split_name}_bleu\"]\n",
        "                print(f\"{split_desc} BLEU: {bleu_score:.2f}\")\n",
        "\n",
        "                with open(results_file, \"a\") as f:\n",
        "                    f.write(f\"{split_desc} BLEU: {bleu_score:.2f}\\n\")\n",
        "\n",
        "                # Track primary split for early stopping\n",
        "                if split_name == primary_split:\n",
        "                    current_bleu = bleu_score\n",
        "\n",
        "        # Check early stopping on primary validation\n",
        "        if current_bleu is not None:\n",
        "            if current_bleu > best_bleu:\n",
        "                best_bleu = current_bleu\n",
        "                best_epoch = epoch + 1\n",
        "                no_improvement_count = 0\n",
        "\n",
        "                # Save best model\n",
        "                best_model_dir = os.path.join(args.output_dir, \"best_model\")\n",
        "                os.makedirs(best_model_dir, exist_ok=True)\n",
        "                model.save_pretrained(best_model_dir)\n",
        "                tokenizer.save_pretrained(best_model_dir)\n",
        "\n",
        "                print(f\"New best model saved with BLEU: {best_bleu:.2f}\")\n",
        "            else:\n",
        "                no_improvement_count += 1\n",
        "                print(f\"No improvement for {no_improvement_count} epochs (best: {best_bleu:.2f} at epoch {best_epoch})\")\n",
        "\n",
        "        # Check early stopping\n",
        "        if no_improvement_count >= early_stopping_patience:\n",
        "            print(f\"\\nEarly stopping after {epoch + 1} epochs!\")\n",
        "            with open(results_file, \"a\") as f:\n",
        "                f.write(f\"\\nEarly stopping triggered. Best model was at epoch {best_epoch} with BLEU: {best_bleu:.2f}\\n\")\n",
        "            break\n",
        "\n",
        "    print(\"\\n=== Training Complete ===\")\n",
        "    print(f\"Best model was at epoch {best_epoch} with BLEU: {best_bleu:.2f}\")\n",
        "\n",
        "    # Load best model for final evaluation\n",
        "    best_model_path = os.path.join(args.output_dir, \"best_model\")\n",
        "    if os.path.exists(best_model_path):\n",
        "        print(f\"\\nLoading best model from epoch {best_epoch}...\")\n",
        "        model = model.from_pretrained(best_model_path, device=device)\n",
        "\n",
        "    # Final evaluation on all validation splits\n",
        "    print(\"\\n=== Final Evaluation ===\")\n",
        "\n",
        "    with open(results_file, \"a\") as f:\n",
        "        f.write(\"\\n=== FINAL EVALUATION ===\\n\")\n",
        "\n",
        "    for split_name, split_data in datasets.items():\n",
        "        val_dataset = split_data['validation']\n",
        "\n",
        "        if val_dataset is None:\n",
        "            continue\n",
        "\n",
        "        split_desc = split_data['info']['name']\n",
        "        is_test_split = 'test' in split_name\n",
        "\n",
        "        if is_test_split:\n",
        "            print(f\"\\nFinal prediction generation for '{split_desc}'\")\n",
        "            predict_results = trainer.predict(\n",
        "                test_dataset=val_dataset,\n",
        "                metric_key_prefix=f\"final_{split_name}\",\n",
        "            )\n",
        "\n",
        "            with open(results_file, \"a\") as f:\n",
        "                f.write(f\"Final {split_desc}: Generated predictions only\\n\")\n",
        "        else:\n",
        "            print(f\"\\nFinal evaluation on '{split_desc}'\")\n",
        "            val_metrics = trainer.evaluate(\n",
        "                eval_dataset=val_dataset,\n",
        "                metric_key_prefix=f\"final_{split_name}\",\n",
        "            )\n",
        "\n",
        "            bleu_score = val_metrics[f\"final_{split_name}_bleu\"]\n",
        "            print(f\"Final {split_desc} BLEU: {bleu_score:.2f}\")\n",
        "\n",
        "            with open(results_file, \"a\") as f:\n",
        "                f.write(f\"Final {split_desc} BLEU: {bleu_score:.2f}\\n\")\n",
        "\n",
        "    # Generate predictions for official test set if available\n",
        "    if 'official_test' in datasets and datasets['official_test']['validation'] is not None:\n",
        "        test_dataset = datasets['official_test']['validation']\n",
        "\n",
        "        print(\"\\n=== Generating Official Test Set Predictions ===\")\n",
        "\n",
        "        predict_results = trainer.predict(\n",
        "            test_dataset=test_dataset,\n",
        "            metric_key_prefix=\"test\",\n",
        "        )\n",
        "\n",
        "        # Save predictions\n",
        "        predictions = predict_results.predictions\n",
        "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "        decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "        cleaned_predictions = [pred.strip() for pred in decoded_predictions]\n",
        "\n",
        "        # Create submission format\n",
        "        output_prediction_file = os.path.join(args.results_dir, \"test_predictions.txt\")\n",
        "        with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            writer.write(\"ID\\tPrediction\\n\")\n",
        "            for idx, prediction in enumerate(cleaned_predictions):\n",
        "                if idx < len(test_dataset.data_info):\n",
        "                    sample_key = test_dataset.data_info[idx][0]\n",
        "                    writer.write(f\"{sample_key}\\t{prediction}\\n\")\n",
        "\n",
        "        print(f\"Test predictions saved to {output_prediction_file}\")\n",
        "\n",
        "        # Create JSON format for submission\n",
        "        submission_file = os.path.join(args.results_dir, \"submission.json\")\n",
        "        submission_data = []\n",
        "\n",
        "        for idx, prediction in enumerate(cleaned_predictions):\n",
        "            if idx < len(test_dataset.data_info):\n",
        "                sample_key = test_dataset.data_info[idx][0]\n",
        "                submission_data.append({\n",
        "                    \"id\": sample_key,\n",
        "                    \"translation\": prediction\n",
        "                })\n",
        "\n",
        "        with open(submission_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(submission_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Submission file saved to {submission_file}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "nCyP3FehBCYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === NEW: Main execution function ===\n",
        "def main():\n",
        "    # Create the system\n",
        "    slt_system = SignLanguageTranslationSystem()\n",
        "\n",
        "    # Step 1: Mount Drive\n",
        "    slt_system.mount_drive()\n",
        "\n",
        "    # Step 2: Setup paths and arguments\n",
        "    slt_system.setup()\n",
        "\n",
        "    # Step 3: Load data\n",
        "    slt_system.load_data()\n",
        "\n",
        "    # Step 4: Initialize tokenizer\n",
        "    slt_system.initialize_tokenizer()\n",
        "\n",
        "    # Step 5: Create datasets\n",
        "    slt_system.create_datasets()\n",
        "\n",
        "    # Step 6: Initialize model\n",
        "    slt_system.initialize_model()\n",
        "\n",
        "    # Step 7: Train model (this performs a single training run)\n",
        "    slt_system.train_model()\n",
        "\n",
        "    # Additional steps (uncomment as needed)\n",
        "    slt_system.evaluate_model()  # Additional evaluation\n",
        "    slt_system.generate_predictions()  # Generate test predictions\n",
        "\n",
        "    print(\"All processing completed successfully!\")\n",
        "\n",
        "# === Entry point for direct execution ===\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "S5SBRh_TBlN0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "05b0e412-f06b-4328-bcde-43e644033645"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n",
            "Ignoring unrecognized arguments: ['-f', '/root/.local/share/jupyter/runtime/kernel-38e1520b-38a4-4763-b504-4a0f5a3158e5.json']\n",
            "Effective batch size: 4 (train_bs=4 * grad_accum=1 * devices=1)\n",
            "Using device: cuda\n",
            "Seed set to: 42\n",
            "Found training H5 file: /content/drive/MyDrive/saudi/saudi-signfor-all-competition/SSL.keypoints.train_signers_train_sentences.0.h5\n",
            "Found training CSV file: /content/drive/MyDrive/saudi/saudi-signfor-all-competition/SSL.keypoints.train_signers_train_sentences.csv\n",
            "Found test H5 file: /content/drive/MyDrive/saudi/saudi-signfor-all-competition/SSL.keypoints.test_signers_test_sentences.h5\n",
            "Found test JSON file: /content/drive/MyDrive/saudi/saudi-signfor-all-competition/SSL.keypoints.test_signers_test_sentences.json\n",
            "Loaded training CSV: /content/drive/MyDrive/saudi/saudi-signfor-all-competition/SSL.keypoints.train_signers_train_sentences.csv\n",
            "Training samples in CSV: 24109\n",
            "Created ID-to-sentence map with 24109 entries.\n",
            "Scanning HDF5 /content/drive/MyDrive/saudi/saudi-signfor-all-competition/SSL.keypoints.train_signers_train_sentences.0.h5...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preloading HDF5 info: 100%|██████████| 24109/24109 [00:06<00:00, 3715.66it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 24109 non-empty keys in HDF5.\n",
            "Detected primary feature dimension: 208\n",
            "Scanning HDF5 /content/drive/MyDrive/saudi/saudi-signfor-all-competition/SSL.keypoints.test_signers_test_sentences.h5...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preloading HDF5 info: 100%|██████████| 200/200 [00:00<00:00, 3919.97it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 200 non-empty keys in HDF5.\n",
            "Detected primary feature dimension: 208\n",
            "\n",
            "--- Training Sequence Length Analysis ---\n",
            "Total sequences found in H5: 24109\n",
            "Min length: 29\n",
            "Max length: 1109\n",
            "Mean length: 192.42\n",
            "Median length: 161.0\n",
            "90th percentile: 340.00\n",
            "95th percentile: 408.00\n",
            "99th percentile: 537.00\n",
            "Num sequences > 512: 347 (1.44%)\n",
            "--> Based on this, 23762 sequences (98.56%) will be used with max_seq_length=512.\n",
            "--- End Analysis ---\n",
            "\n",
            "Found 24109 valid keys in training H5 file.\n",
            "Using feature dimension: 208\n",
            "\n",
            "Initializing tokenizer...\n",
            "Tokenizer loaded: facebook/mbart-large-50-many-to-many-mmt\n",
            "\n",
            "--- Creating Validation Splits with STRICT Separation ---\n",
            "Valid keys after filtering: 23762\n",
            "Found 16 unique signers and 1889 unique sentences\n",
            "Standard split: 21385 train, 2377 validation samples\n",
            "Verified: Train and validation sets have NO overlap\n",
            "Unseen sentences split: 21388 train, 2374 validation samples\n",
            "Verified: Train and validation sets have NO overlap\n",
            "Unseen signers split: 18965 train, 4797 validation samples\n",
            "Verified: Train and validation sets have NO overlap\n",
            "Challenge split: 16853 train, 6909 validation samples\n",
            "Verified: Train and validation sets have NO overlap\n",
            "\n",
            "--- Verifying Individual Split Separation ---\n",
            "✓ Split 'standard' has strict separation between train and validation sets\n",
            "✓ Split 'unseen_sentences' has strict separation between train and validation sets\n",
            "✓ Split 'unseen_signers' has strict separation between train and validation sets\n",
            "✓ Split 'challenge' has strict separation between train and validation sets\n",
            "\n",
            "--- Cross-Split Overlap Information (Expected) ---\n",
            "Info: Splits 'standard' and 'unseen_sentences':\n",
            "  - 19248 overlapping training keys\n",
            "  - 237 overlapping validation keys\n",
            "  - 2137 keys in both 'standard' train and 'unseen_sentences' validation\n",
            "  - 2140 keys in both 'unseen_sentences' train and 'standard' validation\n",
            "Info: Splits 'standard' and 'unseen_signers':\n",
            "  - 17034 overlapping training keys\n",
            "  - 446 overlapping validation keys\n",
            "  - 4351 keys in both 'standard' train and 'unseen_signers' validation\n",
            "  - 1931 keys in both 'unseen_signers' train and 'standard' validation\n",
            "Info: Splits 'standard' and 'challenge':\n",
            "  - 15180 overlapping training keys\n",
            "  - 704 overlapping validation keys\n",
            "  - 6205 keys in both 'standard' train and 'challenge' validation\n",
            "  - 1673 keys in both 'challenge' train and 'standard' validation\n",
            "Info: Splits 'unseen_sentences' and 'unseen_signers':\n",
            "  - 17065 overlapping training keys\n",
            "  - 474 overlapping validation keys\n",
            "  - 4323 keys in both 'unseen_sentences' train and 'unseen_signers' validation\n",
            "  - 1900 keys in both 'unseen_signers' train and 'unseen_sentences' validation\n",
            "Info: Splits 'unseen_sentences' and 'challenge':\n",
            "  - 14983 overlapping training keys\n",
            "  - 504 overlapping validation keys\n",
            "  - 6405 keys in both 'unseen_sentences' train and 'challenge' validation\n",
            "  - 1870 keys in both 'challenge' train and 'unseen_sentences' validation\n",
            "Info: Splits 'unseen_signers' and 'challenge':\n",
            "  - 12536 overlapping training keys\n",
            "  - 480 overlapping validation keys\n",
            "  - 6429 keys in both 'unseen_signers' train and 'challenge' validation\n",
            "  - 4317 keys in both 'challenge' train and 'unseen_signers' validation\n",
            "\n",
            "✓ All individual splits maintain strict separation between their own train and validation sets!\n",
            "NOTE: The overlap between different splits is normal and expected.\n",
            "The training will use only ONE split (the most challenging one) for both training and validation.\n",
            "Warning: Test JSON format not recognized or empty at /content/drive/MyDrive/saudi/saudi-signfor-all-competition/SSL.keypoints.test_signers_test_sentences.json.\n",
            "Found 200 valid test keys for evaluation\n",
            "\n",
            "--- Creating Datasets from Splits ---\n",
            "\n",
            "Creating datasets for 'Standard Random Split' split:\n",
            "Initializing dataset (Train/Val). Processing 21385 provided keys...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing dataset samples: 100%|██████████| 21385/21385 [00:00<00:00, 855201.00it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished preparing dataset: 21385 samples included.\n",
            "  Skipped 0 samples total (0 due to length > 512, 0 zero length, 0 missing/empty label).\n",
            "  Created training dataset with 21385 samples\n",
            "Initializing dataset (Train/Val). Processing 2377 provided keys...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing dataset samples: 100%|██████████| 2377/2377 [00:00<00:00, 734915.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished preparing dataset: 2377 samples included.\n",
            "  Skipped 0 samples total (0 due to length > 512, 0 zero length, 0 missing/empty label).\n",
            "  Created validation dataset with 2377 samples\n",
            "\n",
            "Creating datasets for 'Unseen Sentences Split' split:\n",
            "Initializing dataset (Train/Val). Processing 21388 provided keys...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing dataset samples: 100%|██████████| 21388/21388 [00:00<00:00, 1052475.79it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished preparing dataset: 21388 samples included.\n",
            "  Skipped 0 samples total (0 due to length > 512, 0 zero length, 0 missing/empty label).\n",
            "  Created training dataset with 21388 samples\n",
            "Initializing dataset (Train/Val). Processing 2374 provided keys...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing dataset samples: 100%|██████████| 2374/2374 [00:00<00:00, 742969.53it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished preparing dataset: 2374 samples included.\n",
            "  Skipped 0 samples total (0 due to length > 512, 0 zero length, 0 missing/empty label).\n",
            "  Created validation dataset with 2374 samples\n",
            "\n",
            "Creating datasets for 'Unseen Signers Split' split:\n",
            "Initializing dataset (Train/Val). Processing 18965 provided keys...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing dataset samples: 100%|██████████| 18965/18965 [00:00<00:00, 1123675.31it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished preparing dataset: 18965 samples included.\n",
            "  Skipped 0 samples total (0 due to length > 512, 0 zero length, 0 missing/empty label).\n",
            "  Created training dataset with 18965 samples\n",
            "Initializing dataset (Train/Val). Processing 4797 provided keys...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing dataset samples: 100%|██████████| 4797/4797 [00:00<00:00, 1064329.05it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished preparing dataset: 4797 samples included.\n",
            "  Skipped 0 samples total (0 due to length > 512, 0 zero length, 0 missing/empty label).\n",
            "  Created validation dataset with 4797 samples\n",
            "\n",
            "Creating datasets for 'Test-Like Challenge Split' split:\n",
            "Initializing dataset (Train/Val). Processing 16853 provided keys...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing dataset samples: 100%|██████████| 16853/16853 [00:00<00:00, 1078773.07it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished preparing dataset: 16853 samples included.\n",
            "  Skipped 0 samples total (0 due to length > 512, 0 zero length, 0 missing/empty label).\n",
            "  Created training dataset with 16853 samples\n",
            "Initializing dataset (Train/Val). Processing 6909 provided keys...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing dataset samples: 100%|██████████| 6909/6909 [00:00<00:00, 968854.78it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished preparing dataset: 6909 samples included.\n",
            "  Skipped 0 samples total (0 due to length > 512, 0 zero length, 0 missing/empty label).\n",
            "  Created validation dataset with 6909 samples\n",
            "\n",
            "Creating datasets for 'Official Test Set' split:\n",
            "Initializing dataset (Test). Processing 200 provided keys...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preparing dataset samples: 100%|██████████| 200/200 [00:00<00:00, 725030.94it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finished preparing dataset: 198 samples included.\n",
            "  Skipped 2 samples total (2 due to length > 512, 0 zero length, 0 missing/empty label).\n",
            "  Created validation dataset with 198 samples\n",
            "\n",
            "--- Initializing Model ---\n",
            "Initializing SignTranslationModel:\n",
            "  Loading Base mBART Model From: facebook/mbart-large-50-many-to-many-mmt\n",
            "  Feature Dim (Input): 208\n",
            "  Base mBART model loaded successfully with eager attention.\n",
            "  Model Hidden Dim (d_model): 1024\n",
            "Initialized VisualHead: Linear(208 -> 1024)\n",
            "Gradient Checkpointing is DISABLED (intended for A100/faster training).\n",
            "Setting attention implementation to eager on model...\n",
            "Model instantiated and moved to cuda.\n",
            "Model memory footprint (approx): 2.44 GB\n",
            "Running GC and clearing CUDA cache before starting training...\n",
            "Cache cleared.\n",
            "Custom Data Collator created.\n",
            "\n",
            "=== Training and Evaluation with Strict Separation ===\n",
            "\n",
            "Using 'Test-Like Challenge Split' as primary split for training.\n",
            "Challenge level: 4\n",
            "Description: Both unseen signers and unseen sentences (strictly held out from training)\n",
            "Training samples: 16853\n",
            "Validation samples: 6909\n",
            "\n",
            "=== Starting Training ===\n",
            "\n",
            "Found existing checkpoint: /content/drive/MyDrive/saudi/output/checkpoint-63210\n",
            "Will resume training from: /content/drive/MyDrive/saudi/output/checkpoint-63210\n",
            "\n",
            "--- Epoch 1/15 ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63210' max='63210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63210/63210 : < :, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.0000\n",
            "\n",
            "Evaluating on 'Standard Random Split' validation\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard Random Split BLEU: 78.02\n",
            "\n",
            "Evaluating on 'Unseen Sentences Split' validation\n",
            "Unseen Sentences Split BLEU: 83.28\n",
            "\n",
            "Evaluating on 'Unseen Signers Split' validation\n",
            "Unseen Signers Split BLEU: 85.81\n",
            "\n",
            "Evaluating on 'Test-Like Challenge Split' validation\n",
            "Test-Like Challenge Split BLEU: 37.53\n",
            "\n",
            "Generating predictions for 'Official Test Set' validation\n",
            "Model saved to /content/drive/MyDrive/saudi/output/best_model\n",
            "New best model saved with BLEU: 37.53\n",
            "\n",
            "--- Epoch 2/15 ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63210' max='63210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63210/63210 4:40:31, Epoch 15/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.105200</td>\n",
              "      <td>2.193208</td>\n",
              "      <td>30.066300</td>\n",
              "      <td>11.139200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.082300</td>\n",
              "      <td>2.305413</td>\n",
              "      <td>32.398400</td>\n",
              "      <td>11.006400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.058700</td>\n",
              "      <td>2.385316</td>\n",
              "      <td>33.202300</td>\n",
              "      <td>11.033700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.056500</td>\n",
              "      <td>2.485785</td>\n",
              "      <td>33.944700</td>\n",
              "      <td>11.121100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.045900</td>\n",
              "      <td>2.577813</td>\n",
              "      <td>34.846900</td>\n",
              "      <td>11.039400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.017400</td>\n",
              "      <td>2.708436</td>\n",
              "      <td>35.822300</td>\n",
              "      <td>11.097400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.016800</td>\n",
              "      <td>2.680355</td>\n",
              "      <td>37.524200</td>\n",
              "      <td>11.158100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>2.674415</td>\n",
              "      <td>36.820400</td>\n",
              "      <td>11.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.012500</td>\n",
              "      <td>2.756333</td>\n",
              "      <td>38.515600</td>\n",
              "      <td>11.070500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>2.736547</td>\n",
              "      <td>38.768100</td>\n",
              "      <td>11.143300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.003000</td>\n",
              "      <td>2.754784</td>\n",
              "      <td>38.791400</td>\n",
              "      <td>11.049400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>2.782704</td>\n",
              "      <td>39.819400</td>\n",
              "      <td>11.025900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>2.770681</td>\n",
              "      <td>40.859500</td>\n",
              "      <td>11.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>2.774545</td>\n",
              "      <td>41.898900</td>\n",
              "      <td>11.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.768886</td>\n",
              "      <td>41.972900</td>\n",
              "      <td>11.106500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss: 0.0308\n",
            "\n",
            "Evaluating on 'Standard Random Split' validation\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard Random Split BLEU: 81.51\n",
            "\n",
            "Evaluating on 'Unseen Sentences Split' validation\n",
            "Unseen Sentences Split BLEU: 87.99\n",
            "\n",
            "Evaluating on 'Unseen Signers Split' validation\n",
            "Unseen Signers Split BLEU: 88.03\n",
            "\n",
            "Evaluating on 'Test-Like Challenge Split' validation\n",
            "Test-Like Challenge Split BLEU: 41.97\n",
            "\n",
            "Generating predictions for 'Official Test Set' validation\n",
            "Model saved to /content/drive/MyDrive/saudi/output/best_model\n",
            "New best model saved with BLEU: 41.97\n",
            "\n",
            "--- Epoch 3/15 ---\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4215' max='63210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 4215/63210 13:41 < 3:11:49, 5.13 it/s, Epoch 1/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='74' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 74/432 00:47 < 03:54, 1.52 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5035' max='63210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 5035/63210 21:20 < 4:06:35, 3.93 it/s, Epoch 1.19/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.058000</td>\n",
              "      <td>2.453705</td>\n",
              "      <td>35.963100</td>\n",
              "      <td>11.192200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-66328a0036d7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# === Entry point for direct execution ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-66328a0036d7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Step 7: Train model (this performs a single training run)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mslt_system\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Additional steps (uncomment as needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-b6ac875ee842>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;31m# Train and evaluate with our enhanced function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         self.model = train_and_evaluate_with_strict_separation(\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mdatasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-5386f84774fc>\u001b[0m in \u001b[0;36mtrain_and_evaluate_with_strict_separation\u001b[0;34m(model, datasets, args, tokenizer, data_collator)\u001b[0m\n\u001b[1;32m   1265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m         \u001b[0;31m# Train for one epoch - modified to support checkpoint resumption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m         \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m         \u001b[0;31m# Only use checkpoint for first epoch, then set to False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2244\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2245\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2246\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2247\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2558\u001b[0m                     )\n\u001b[1;32m   2559\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2560\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3735\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3736\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3799\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3801\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3802\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3803\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 802\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-5386f84774fc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, labels, decoder_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m             outputs = self.mbart(\n\u001b[0m\u001b[1;32m    901\u001b[0m                 \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1606\u001b[0m                 \u001b[0mdecoder_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_tokens_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1608\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1609\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1492\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1493\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 )\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1352\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;31m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    662\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/mbart/modeling_mbart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    271\u001b[0m             )\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jwFyEEY0BlQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}