{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b9d7995-87d2-4df5-8dff-08d1a09a1931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading training config from: D:/configs\\config_train.yaml\n",
      "Training Config Loaded:\n",
      "Config(ModelArguments=Config(model_name_or_path='D:/YTASL - Checkpoint/t5-base', base_model_name='T5', feature_dim=208, hidden_dim=768, resume=False, resume_checkpoint=''), DatasetArguments=Config(train_dataset_path='SSL.keypoints.train_signers_train_sentences.0.h5', train_labels_dataset_path='SSL.keypoints.train_signers_train_sentences.csv', validation_split_ratio=0.15, max_sequence_length=512, max_label_length=128), TrainingArguments=Config(output_folder='finetune_run', seed=42, num_train_epochs=20, learning_rate=5e-05, weight_decay=0.01, per_device_train_batch_size=1, gradient_accumulation_steps=4, per_device_eval_batch_size=4, fp16=True, gradient_checkpointing=True, optim='adafactor', evaluation_strategy='epoch', logging_strategy='epoch', save_strategy='epoch', save_total_limit=2, load_best_model_at_end=True, metric_for_best_model='eval_bleu', greater_is_better=True, dataloader_num_workers=0, predict_with_generate=True), WandbArguments=Config(report_to_wandb=False, wandb_project='SNL_FineTune'))\n",
      "------------------------------\n",
      "Loading evaluation config from: D:/configs\\config_eval.yaml\n",
      "Evaluation Config Loaded (Paths adjusted):\n",
      "Config(ModelArguments=Config(base_model_name='T5', base_model_hf='google-t5/t5-base', checkpoint_path='D:/YTASL - Checkpoint/t5-base', feature_dim=208, hidden_dim=768), DatasetArguments=Config(test_dataset_path='D:/saudi-signfor-all-competition\\\\SSL.keypoints.test_signers_test_sentences.h5', test_labels_dataset_path='D:/saudi-signfor-all-competition\\\\SSL.keypoints.train_signers_train_sentences.csv'), EvaluationArguments=Config(results_save_path='results_finetuned\\\\results', batch_size=8, dataloader_num_workers=0), GenerationArguments=Config(max_length=128, num_beams=4, early_stopping=True, length_penalty=2.0, no_repeat_ngram_size=3, do_sample=True, top_p=0.95))\n",
      "------------------------------\n",
      "Training Config Paths Adjusted:\n",
      "  Train/Val Data H5: D:/saudi-signfor-all-competition\\SSL.keypoints.train_signers_train_sentences.0.h5\n",
      "  Train/Val Labels CSV: D:/saudi-signfor-all-competition\\SSL.keypoints.train_signers_train_sentences.csv\n",
      "  Validation Split Ratio: 0.15\n",
      "  Starting Fine-tuning From: D:/YTASL - Checkpoint/t5-base\n",
      "  Base Fine-tuning Output Folder: output_finetuned\\finetune_run\n",
      "------------------------------\n",
      "Metrics 'sacrebleu' and 'chrf' loaded using 'evaluate' library.\n",
      "\n",
      "--- Setting up Fine-Tuning ---\n",
      "Fine-tuning Output Directory: output_finetuned\\finetune_run\\SSL_FineTune-Start_t5-base-LR_5e-05-Epochs_Def-Seed_42\n",
      "Loading Tokenizer from: D:/YTASL - Checkpoint/t5-base\n",
      "\n",
      "--- Preparing Train/Validation Split ---\n",
      "Splitting data: 20492 train samples, 3617 validation samples.\n",
      "Scanning HDF5 D:/saudi-signfor-all-competition\\SSL.keypoints.train_signers_train_sentences.0.h5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eeb6f5d9cbc435eb89f631354d610dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading HDF5 info:   0%|          | 0/24109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24109 non-empty keys in HDF5.\n",
      "Filtered keys vs HDF5: 20492 train, 3617 validation\n",
      "\n",
      "Instantiating Training Dataset...\n",
      "Initializing dataset (Train/Val). Processing 20492 provided keys...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca413c40e604e5385452e948d7e2d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset samples:   0%|          | 0/20492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparing dataset: 20190 samples included, 302 samples skipped.\n",
      "\n",
      "Instantiating Validation Dataset...\n",
      "Initializing dataset (Train/Val). Processing 3617 provided keys...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042a75c543aa43de9f135b6bfc8eb6ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset samples:   0%|          | 0/3617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparing dataset: 3572 samples included, 45 samples skipped.\n",
      "\n",
      "Initializing Model for Fine-tuning from: D:/YTASL - Checkpoint/t5-base\n",
      "Initializing SNLTraslationModel:\n",
      "  Loading Base Model From: D:/YTASL - Checkpoint/t5-base\n",
      "  Feature Dim (Input): 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at D:/YTASL - Checkpoint/t5-base and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.final_layer_norm.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.embed_tokens.weight', 'encoder.final_layer_norm.weight', 'lm_head.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Base T5 model loaded successfully.\n",
      "  Model Hidden Dim (d_model): 512\n",
      "  Projection Layer: Linear(208 -> 512)\n",
      "Setting decoder_start_token_id to pad_token_id...\n",
      "Metrics 'sacrebleu' and 'chrf' loaded using 'evaluate' library.\n",
      "\n",
      "--- Setting up Fine-Tuning ---\n",
      "Fine-tuning Output Directory: output_finetuned\\finetune_run\\SSL_FineTune-Start_t5-base-LR_5e-05-Epochs_20-Seed_42\n",
      "Loading Tokenizer from: D:/YTASL - Checkpoint/t5-base\n",
      "\n",
      "--- Preparing Train/Validation Split ---\n",
      "Splitting data: 20492 train samples, 3617 validation samples.\n",
      "Scanning HDF5 D:/saudi-signfor-all-competition\\SSL.keypoints.train_signers_train_sentences.0.h5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e44691eefda4fcdb6c3ccf09d004ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preloading HDF5 info:   0%|          | 0/24109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24109 non-empty keys in HDF5.\n",
      "Filtered keys vs HDF5: 20492 train, 3617 validation\n",
      "\n",
      "Instantiating Training Dataset...\n",
      "Initializing dataset (Train/Val). Processing 20492 provided keys...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18db8fb6b215466f9749125033c4eb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset samples:   0%|          | 0/20492 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparing dataset: 20190 samples included, 302 samples skipped.\n",
      "\n",
      "Instantiating Validation Dataset...\n",
      "Initializing dataset (Train/Val). Processing 3617 provided keys...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809afbaf306c44e6bd2d63c48cd7baf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preparing dataset samples:   0%|          | 0/3617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished preparing dataset: 3572 samples included, 45 samples skipped.\n",
      "\n",
      "Initializing Model for Fine-tuning from: D:/YTASL - Checkpoint/t5-base\n",
      "Initializing SNLTraslationModel:\n",
      "  Loading Base Model From: D:/YTASL - Checkpoint/t5-base\n",
      "  Feature Dim (Input): 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at D:/YTASL - Checkpoint/t5-base and are newly initialized: ['decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.0.layer.2.DenseReluDense.wi.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wi.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.0.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.4.layer.0.SelfAttention.k.weight', 'decoder.block.4.layer.0.SelfAttention.o.weight', 'decoder.block.4.layer.0.SelfAttention.q.weight', 'decoder.block.4.layer.0.SelfAttention.v.weight', 'decoder.block.4.layer.0.layer_norm.weight', 'decoder.block.4.layer.1.EncDecAttention.k.weight', 'decoder.block.4.layer.1.EncDecAttention.o.weight', 'decoder.block.4.layer.1.EncDecAttention.q.weight', 'decoder.block.4.layer.1.EncDecAttention.v.weight', 'decoder.block.4.layer.1.layer_norm.weight', 'decoder.block.4.layer.2.DenseReluDense.wi.weight', 'decoder.block.4.layer.2.DenseReluDense.wo.weight', 'decoder.block.4.layer.2.layer_norm.weight', 'decoder.block.5.layer.0.SelfAttention.k.weight', 'decoder.block.5.layer.0.SelfAttention.o.weight', 'decoder.block.5.layer.0.SelfAttention.q.weight', 'decoder.block.5.layer.0.SelfAttention.v.weight', 'decoder.block.5.layer.0.layer_norm.weight', 'decoder.block.5.layer.1.EncDecAttention.k.weight', 'decoder.block.5.layer.1.EncDecAttention.o.weight', 'decoder.block.5.layer.1.EncDecAttention.q.weight', 'decoder.block.5.layer.1.EncDecAttention.v.weight', 'decoder.block.5.layer.1.layer_norm.weight', 'decoder.block.5.layer.2.DenseReluDense.wi.weight', 'decoder.block.5.layer.2.DenseReluDense.wo.weight', 'decoder.block.5.layer.2.layer_norm.weight', 'decoder.embed_tokens.weight', 'decoder.final_layer_norm.weight', 'encoder.block.0.layer.0.SelfAttention.k.weight', 'encoder.block.0.layer.0.SelfAttention.o.weight', 'encoder.block.0.layer.0.SelfAttention.q.weight', 'encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'encoder.block.0.layer.0.SelfAttention.v.weight', 'encoder.block.0.layer.0.layer_norm.weight', 'encoder.block.0.layer.1.DenseReluDense.wi.weight', 'encoder.block.0.layer.1.DenseReluDense.wo.weight', 'encoder.block.0.layer.1.layer_norm.weight', 'encoder.block.1.layer.0.SelfAttention.k.weight', 'encoder.block.1.layer.0.SelfAttention.o.weight', 'encoder.block.1.layer.0.SelfAttention.q.weight', 'encoder.block.1.layer.0.SelfAttention.v.weight', 'encoder.block.1.layer.0.layer_norm.weight', 'encoder.block.1.layer.1.DenseReluDense.wi.weight', 'encoder.block.1.layer.1.DenseReluDense.wo.weight', 'encoder.block.1.layer.1.layer_norm.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wi.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.3.layer.1.DenseReluDense.wi.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.embed_tokens.weight', 'encoder.final_layer_norm.weight', 'lm_head.weight', 'shared.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Base T5 model loaded successfully.\n",
      "  Model Hidden Dim (d_model): 512\n",
      "  Projection Layer: Linear(208 -> 512)\n",
      "Setting model.config.decoder_start_token_id to pad_token_id...\n",
      "\n",
      "Setting Training Arguments for Fine-tuning...\n",
      "Setting decoder_start_token_id in GenerationConfig...\n",
      "\n",
      "Initializing Seq2SeqTrainer...\n",
      "\n",
      "--- Starting Fine-tuning ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling gradient checkpointing on underlying model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5049' max='100940' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  5049/100940 52:12 < 16:31:54, 1.61 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-tuning Interrupted or Failed: `decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation. ---\n",
      "(No best checkpoint identified before interruption)\n",
      "\n",
      "Cleaning up training objects...\n",
      "\n",
      "--- Fine-tuning Phase Complete ---\n",
      "\n",
      "--- Setting up Evaluation on ACTUAL TEST SET ---\n",
      "Skipping final evaluation on test set as the best fine-tuned checkpoint was not found.\n",
      "\n",
      "--- Full Script Finished ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Fatima\\AppData\\Local\\Temp\\ipykernel_18120\\2461493475.py\", line 742, in <module>\n",
      "    train_result = trainer.train(\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\transformers\\trainer.py\", line 2245, in train\n",
      "    return inner_training_loop(\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\transformers\\trainer.py\", line 2661, in _inner_training_loop\n",
      "    self._maybe_log_save_evaluate(\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\transformers\\trainer.py\", line 3096, in _maybe_log_save_evaluate\n",
      "    metrics = self._evaluate(trial, ignore_keys_for_eval)\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\transformers\\trainer.py\", line 3045, in _evaluate\n",
      "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\transformers\\trainer_seq2seq.py\", line 197, in evaluate\n",
      "    return super().evaluate(eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\transformers\\trainer.py\", line 4154, in evaluate\n",
      "    output = eval_loop(\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\transformers\\trainer.py\", line 4348, in evaluation_loop\n",
      "    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\transformers\\trainer_seq2seq.py\", line 333, in prediction_step\n",
      "    generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Fatima\\AppData\\Local\\Temp\\ipykernel_18120\\2461493475.py\", line 304, in generate\n",
      "    return self.model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, **generate_kwargs)\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2246, in generate\n",
      "    self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n",
      "  File \"C:\\Users\\Fatima\\anaconda3\\envs\\pytorch_cuda\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2082, in _prepare_special_tokens\n",
      "    raise ValueError(\n",
      "ValueError: `decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\n"
     ]
    }
   ],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import sys\n",
    "import evaluate\n",
    "import argparse\n",
    "import random\n",
    "import yaml\n",
    "from tqdm.notebook import tqdm # Use notebook-friendly tqdm\n",
    "import warnings\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split # For validation split\n",
    "import gc # Garbage collector\n",
    "\n",
    "# Data Handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Transformers Library\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    MT5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments, # Using Seq2Seq specific args\n",
    "    Seq2SeqTrainer,\n",
    "    set_seed as transformers_set_seed # Alias to avoid conflict if needed\n",
    ")\n",
    "from safetensors.torch import load_file # Import for loading safetensors\n",
    "\n",
    "# Evaluation Metric\n",
    "from sacrebleu.metrics import BLEU, CHRF # Added CHRF\n",
    "# Using load_metric for simplicity if sacrebleu gives issues with Trainer\n",
    "# from datasets import load_metric\n",
    "\n",
    "# Plotting for EDA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Configuration ---\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define base paths\n",
    "CONFIG_DIR = \"D:/configs\"\n",
    "DATA_DIR = \"D:/saudi-signfor-all-competition\"\n",
    "MODEL_OUTPUT_DIR = \"output_finetuned\" # Changed directory name for clarity\n",
    "EVAL_RESULTS_DIR = \"results_finetuned\" # Changed directory name\n",
    "\n",
    "os.makedirs(MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(EVAL_RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# ===========================================\n",
    "# Cell 2: Configuration Loading Utilities\n",
    "# ===========================================\n",
    "class Config:\n",
    "    \"\"\"Helper class to load YAML configuration files into an object.\"\"\"\n",
    "    def __init__(self, config_dict):\n",
    "        for key, value in config_dict.items():\n",
    "            if isinstance(value, dict):\n",
    "                setattr(self, key, Config(value))\n",
    "            else:\n",
    "                setattr(self, key, value)\n",
    "    def __repr__(self):\n",
    "        items = (f\"{k}={v!r}\" for k, v in self.__dict__.items())\n",
    "        return f\"{self.__class__.__name__}({', '.join(items)})\"\n",
    "    def get(self, attribute_path, default=None):\n",
    "        \"\"\"Access nested attributes using dot notation string, e.g., 'ModelArguments.feature_dim'.\"\"\"\n",
    "        keys = attribute_path.split('.')\n",
    "        value = self;\n",
    "        try:\n",
    "            for key in keys: value = getattr(value, key)\n",
    "            return value\n",
    "        except AttributeError: return default\n",
    "\n",
    "def load_config_from_file(config_path):\n",
    "    \"\"\"Loads a YAML configuration file.\"\"\"\n",
    "    if not os.path.exists(config_path): raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "    with open(config_path, \"r\", encoding='utf-8') as file:\n",
    "        try: config_dict = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as e: print(f\"Error parsing YAML {config_path}: {e}\"); raise\n",
    "    if config_dict is None: print(f\"Warning: Config file {config_path} empty.\"); return Config({})\n",
    "    return Config(config_dict)\n",
    "\n",
    "# --- Load Configurations ---\n",
    "# Modify config_train.yaml to include:\n",
    "# ModelArguments:\n",
    "#   model_name_or_path: \"D:/YTASL - Checkpoint/t5-base\" # <--- PATH TO STARTING CHECKPOINT\n",
    "# DatasetArguments:\n",
    "#   validation_split_ratio: 0.1 # Or your desired ratio (e.g., 0.15)\n",
    "\n",
    "train_config_path = os.path.join(CONFIG_DIR, 'config_train.yaml')\n",
    "eval_config_path = os.path.join(CONFIG_DIR, 'config_eval.yaml') # Used only for final test eval settings\n",
    "\n",
    "print(f\"Loading training config from: {train_config_path}\")\n",
    "train_cfg = load_config_from_file(train_config_path)\n",
    "# Add default validation split ratio if not in config\n",
    "if not train_cfg.get('DatasetArguments.validation_split_ratio'):\n",
    "    print(\"Validation split ratio not found in config, using default 0.1 (10%)\")\n",
    "    if not hasattr(train_cfg, 'DatasetArguments'): train_cfg.DatasetArguments = Config({})\n",
    "    train_cfg.DatasetArguments.validation_split_ratio = 0.1\n",
    "# Ensure starting model path exists in config\n",
    "if not train_cfg.get('ModelArguments.model_name_or_path'):\n",
    "     print(\"WARNING: 'model_name_or_path' not specified in train_cfg.ModelArguments. Defaulting to google-t5/t5-base.\")\n",
    "     if not hasattr(train_cfg, 'ModelArguments'): train_cfg.ModelArguments = Config({})\n",
    "     train_cfg.ModelArguments.model_name_or_path = 'google-t5/t5-base'\n",
    "\n",
    "\n",
    "print(\"Training Config Loaded:\")\n",
    "print(train_cfg)\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Loading evaluation config from: {eval_config_path}\")\n",
    "eval_cfg = load_config_from_file(eval_config_path)\n",
    "\n",
    "# --- Adjust Paths ---\n",
    "# Eval paths (for final test run)\n",
    "test_data_path = eval_cfg.get('DatasetArguments.test_dataset_path')\n",
    "if test_data_path: eval_cfg.DatasetArguments.test_dataset_path = os.path.join(DATA_DIR, test_data_path)\n",
    "test_labels_path_final = eval_cfg.get('DatasetArguments.test_labels_dataset_path') # Path to *actual* test labels\n",
    "if test_labels_path_final: eval_cfg.DatasetArguments.test_labels_dataset_path = os.path.join(DATA_DIR, test_labels_path_final)\n",
    "eval_results_save_rel_path = eval_cfg.get('EvaluationArguments.results_save_path', 'final_test_results') # Subdir name\n",
    "eval_cfg.EvaluationArguments.results_save_path = os.path.join(EVAL_RESULTS_DIR, eval_results_save_rel_path)\n",
    "os.makedirs(eval_cfg.EvaluationArguments.results_save_path, exist_ok=True)\n",
    "\n",
    "# Train paths\n",
    "train_data_path = train_cfg.get('DatasetArguments.train_dataset_path')\n",
    "if train_data_path: train_cfg.DatasetArguments.train_dataset_path = os.path.join(DATA_DIR, train_data_path)\n",
    "train_labels_rel_path = train_cfg.get('DatasetArguments.train_labels_dataset_path')\n",
    "if train_labels_rel_path: train_cfg.DatasetArguments.train_labels_dataset_path = os.path.join(DATA_DIR, train_labels_rel_path)\n",
    "# Validation paths are derived from train split\n",
    "train_output_rel_folder = train_cfg.get('TrainingArguments.output_folder', 'finetune_run') # Base output dir name\n",
    "train_cfg.TrainingArguments.output_folder = os.path.join(MODEL_OUTPUT_DIR, train_output_rel_folder)\n",
    "start_model_path = train_cfg.get('ModelArguments.model_name_or_path') # Get the specified starting path\n",
    "\n",
    "print(\"Evaluation Config Loaded (Paths adjusted):\")\n",
    "print(eval_cfg)\n",
    "print(\"-\" * 30)\n",
    "print(\"Training Config Paths Adjusted:\")\n",
    "print(f\"  Train/Val Data H5: {train_cfg.get('DatasetArguments.train_dataset_path', 'N/A')}\")\n",
    "print(f\"  Train/Val Labels CSV: {train_cfg.get('DatasetArguments.train_labels_dataset_path', 'N/A')}\")\n",
    "print(f\"  Validation Split Ratio: {train_cfg.get('DatasetArguments.validation_split_ratio', 'N/A')}\")\n",
    "print(f\"  Starting Fine-tuning From: {start_model_path}\")\n",
    "print(f\"  Base Fine-tuning Output Folder: {train_cfg.get('TrainingArguments.output_folder', 'N/A')}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ===========================================\n",
    "# Cell 3: EDA (Optional)\n",
    "# ===========================================\n",
    "# Skipped for brevity\n",
    "\n",
    "# ===========================================\n",
    "# Cell 4: Dataset Class (Modified for pre-split data)\n",
    "# ===========================================\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, h5_file_path, data_keys_to_use, id_to_sentence_map,\n",
    "                 tokenizer, h5_lengths_map, max_seq_length=600,\n",
    "                 max_label_length=512, is_test_set=False, feature_dim=208):\n",
    "        self.h5_file_path = h5_file_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.max_label_length = max_label_length\n",
    "        self.is_test_set = is_test_set\n",
    "        self.feature_dim = feature_dim\n",
    "        self.id_to_sentence_map = id_to_sentence_map # Store sentence map\n",
    "\n",
    "        self.data_info = []\n",
    "        print(f\"Initializing dataset ({'Test' if is_test_set else 'Train/Val'}). Processing {len(data_keys_to_use)} provided keys...\")\n",
    "\n",
    "        skipped_count = 0\n",
    "        for key in tqdm(data_keys_to_use, desc=\"Preparing dataset samples\"):\n",
    "            original_length = h5_lengths_map.get(key)\n",
    "            sentence = self.id_to_sentence_map.get(key) # Get sentence\n",
    "\n",
    "            # Validation (ensure sentence exists unless it's test set without labels)\n",
    "            if sentence is None and not self.is_test_set:\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "            if isinstance(sentence, str) and not sentence.strip() and not self.is_test_set:\n",
    "                 skipped_count += 1\n",
    "                 continue\n",
    "\n",
    "            # Length validation\n",
    "            if original_length is None or original_length == 0 or original_length > self.max_seq_length:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "\n",
    "            self.data_info.append((key, sentence, original_length))\n",
    "\n",
    "        print(f\"Finished preparing dataset: {len(self.data_info)} samples included, {skipped_count} samples skipped.\")\n",
    "        if not self.data_info: print(\"WARNING: Dataset is empty after processing.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_info)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        key, sentence, original_length = self.data_info[idx]\n",
    "        features = None\n",
    "        actual_feature_dim = self.feature_dim\n",
    "        try:\n",
    "             with h5py.File(self.h5_file_path, 'r') as f:\n",
    "                 item = f[key]\n",
    "                 data_to_load = item[list(item.keys())[0]] if isinstance(item, h5py.Group) and item.keys() else item if isinstance(item, h5py.Dataset) else None\n",
    "                 if data_to_load is not None: features_np = data_to_load[()]\n",
    "                 else: raise ValueError(f\"No dataset found for key {key}\")\n",
    "                 if features_np.shape[1] != self.feature_dim:\n",
    "                      print(f\"Warning: Feature dim mismatch key {key}. Expected {self.feature_dim}, Got {features_np.shape[1]}. Using actual.\")\n",
    "                      actual_feature_dim = features_np.shape[1]\n",
    "                 features = torch.tensor(features_np, dtype=torch.float32)\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading key {key}: {e}\"); return None # Signal error to collate_fn\n",
    "\n",
    "        pad_value = 0.0; num_padding = self.max_seq_length - original_length\n",
    "        if num_padding < 0: features = features[:self.max_seq_length]; num_padding = 0\n",
    "        if features.shape[1] != actual_feature_dim: print(f\"ERR dim mismatch key {key}\"); return None\n",
    "\n",
    "        padded_features = F.pad(features, (0, 0, 0, num_padding), value=pad_value)\n",
    "        attention_mask = torch.zeros(self.max_seq_length, dtype=torch.long); attention_mask[:original_length] = 1\n",
    "\n",
    "        labels_output = None\n",
    "        if self.is_test_set:\n",
    "             labels_output = sentence if sentence is not None else \"\" # Return string for test set\n",
    "        else: # Train/Val -> Tokenize\n",
    "             tokenized_output = self.tokenizer(\n",
    "                 str(sentence), truncation=True, padding=\"max_length\",\n",
    "                 max_length=self.max_label_length, return_tensors=\"pt\")\n",
    "             input_ids = tokenized_output[\"input_ids\"].squeeze(0)\n",
    "             input_ids[input_ids == self.tokenizer.pad_token_id] = -100\n",
    "             labels_output = input_ids\n",
    "\n",
    "        return {\"features\": padded_features, \"attention_mask\": attention_mask, \"labels\": labels_output, \"key\": key}\n",
    "\n",
    "    @staticmethod\n",
    "    def preload_h5_info(file_path):\n",
    "        # (Same as previous version)\n",
    "        keys = []; lengths = {}\n",
    "        if not file_path or not os.path.exists(file_path): print(f\"Error: H5 path {file_path} invalid.\"); return [], {}\n",
    "        try:\n",
    "            with h5py.File(file_path, 'r') as f:\n",
    "                print(f\"Scanning HDF5 {file_path}...\"); valid_keys = list(f.keys())\n",
    "                for key in tqdm(valid_keys, desc=\"Preloading HDF5 info\"):\n",
    "                    try:\n",
    "                        item = f[key]; seq_len = 0\n",
    "                        if isinstance(item, h5py.Group):\n",
    "                             dk = list(item.keys()); seq_len = item[dk[0]].shape[0] if dk else 0\n",
    "                        elif isinstance(item, h5py.Dataset): seq_len = item.shape[0]\n",
    "                        else: continue\n",
    "                        if seq_len > 0: keys.append(key); lengths[key] = seq_len\n",
    "                    except Exception as e: print(f\"Err key {key}: {e}\")\n",
    "            print(f\"Found {len(keys)} non-empty keys in HDF5.\")\n",
    "            return keys, lengths\n",
    "        except Exception as e: print(f\"Err opening HDF5 {file_path}: {e}\"); return [], {}\n",
    "\n",
    "# ===========================================\n",
    "# Cell 5: Model Definition\n",
    "# ===========================================\n",
    "class SNLTraslationModel(nn.Module):\n",
    "    # Takes model_name_or_path now\n",
    "    def __init__(self, feature_dim, model_name_or_path, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        print(f\"Initializing SNLTraslationModel:\")\n",
    "        print(f\"  Loading Base Model From: {model_name_or_path}\")\n",
    "        print(f\"  Feature Dim (Input): {feature_dim}\")\n",
    "\n",
    "        # Load base model (T5 or mT5 based on config/path)\n",
    "        is_mt5 = 'mt5' in model_name_or_path.lower()\n",
    "        try:\n",
    "            if is_mt5: self.model = MT5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "            else: self.model = T5ForConditionalGeneration.from_pretrained(model_name_or_path)\n",
    "            print(f\"  Base {'MT5' if is_mt5 else 'T5'} model loaded successfully.\")\n",
    "        except Exception as e: print(f\"ERROR loading base model from {model_name_or_path}: {e}\"); raise\n",
    "\n",
    "        self.model_hidden_dim = self.model.config.d_model\n",
    "        print(f\"  Model Hidden Dim (d_model): {self.model_hidden_dim}\")\n",
    "\n",
    "        # Custom linear layer projects features -> model's expected input dim\n",
    "        self.custom_linear = nn.Sequential(\n",
    "            nn.Linear(feature_dim, self.model_hidden_dim),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        print(f\"  Projection Layer: Linear({feature_dim} -> {self.model_hidden_dim})\")\n",
    "\n",
    "    def forward(self, features, attention_mask, labels=None):\n",
    "        try: projected_features = self.custom_linear(features)\n",
    "        except Exception as e: print(f\"Linear proj error: {e}\\nInp shape: {features.shape}\"); raise\n",
    "        try:\n",
    "            # Make sure labels are passed correctly if provided\n",
    "            outputs = self.model(inputs_embeds=projected_features, attention_mask=attention_mask, labels=labels, return_dict=True)\n",
    "        except Exception as e: print(f\"Model fwd error: {e}\\nEmbed shape: {projected_features.shape}\"); raise\n",
    "        return outputs # Contains loss and logits\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, features, attention_mask, **generate_kwargs):\n",
    "        self.eval()\n",
    "        inputs_embeds = self.custom_linear(features)\n",
    "        # Ensure generate_kwargs are passed correctly\n",
    "        return self.model.generate(inputs_embeds=inputs_embeds, attention_mask=attention_mask, **generate_kwargs)\n",
    "\n",
    "    # ---> ADD THIS METHOD <---\n",
    "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n",
    "        \"\"\"Delegates gradient checkpointing enabling to the underlying HF model.\"\"\"\n",
    "        print(\"Enabling gradient checkpointing on underlying model...\")\n",
    "        if hasattr(self.model, 'gradient_checkpointing_enable'):\n",
    "            self.model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n",
    "        else:\n",
    "            print(\"Warning: Underlying model does not have 'gradient_checkpointing_enable' method.\")\n",
    "\n",
    "# ===========================================\n",
    "# Cell 6: Fine-tuning Setup and Execution\n",
    "# ===========================================\n",
    "def set_seed(seed_value):\n",
    "    random.seed(seed_value); np.random.seed(seed_value); torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed_value)\n",
    "    transformers_set_seed(seed_value)\n",
    "\n",
    "def collate_fn_train_val(batch):\n",
    "    \"\"\"Collate function for training/validation DataLoaders.\"\"\"\n",
    "    batch = [sample for sample in batch if sample is not None]\n",
    "    if not batch: return None\n",
    "    try:\n",
    "        features = torch.stack([sample['features'] for sample in batch])\n",
    "        attention_mask = torch.stack([sample['attention_mask'] for sample in batch])\n",
    "        labels = torch.stack([sample['labels'] for sample in batch]) # Expect tensors with -100\n",
    "        return {\"features\": features, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "    except Exception as e: print(f\"Collate error: {e}\"); raise\n",
    "\n",
    "# --- Metrics ---\n",
    "# Load metrics (safer way)\n",
    "# --- Metric Calculation Function ---\n",
    "try:\n",
    "    sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
    "    chrf_metric = evaluate.load(\"chrf\")\n",
    "    metrics_loaded = True\n",
    "    print(\"Metrics 'sacrebleu' and 'chrf' loaded using 'evaluate' library.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load metrics using 'evaluate': {e}\")\n",
    "    print(\"BLEU/CHRF calculation during training/evaluation might fail.\")\n",
    "    metrics_loaded = False\n",
    "   \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load metrics using 'datasets.load_metric': {e}. Using direct import.\")\n",
    "    # Fallback or alternative needed if load_metric fails completely\n",
    "    metrics_loaded = False # Need to handle this in compute_metrics\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define compute_metrics function (needs tokenizer available)\n",
    "def compute_metrics_fn(tokenizer_for_decode):\n",
    "    def compute_metrics(eval_pred):\n",
    "        # eval_pred.predictions often is a tuple if multiple outputs, logits are usually first\n",
    "        logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, tuple) else eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "\n",
    "        if logits is None or labels is None:\n",
    "            print(\"Warning: Logits or labels are None in compute_metrics.\")\n",
    "            return {}\n",
    "\n",
    "        # Get predicted token ids\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "        # Decode predictions\n",
    "        decoded_preds = tokenizer_for_decode.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "        # Decode labels (handle -100)\n",
    "        labels = np.where(labels != -100, labels, tokenizer_for_decode.pad_token_id)\n",
    "        decoded_labels = tokenizer_for_decode.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Format for sacrebleu\n",
    "        decoded_labels_for_bleu = [[label] for label in decoded_labels]\n",
    "\n",
    "        results = {}\n",
    "        if metrics_loaded:\n",
    "            try:\n",
    "                bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels_for_bleu)\n",
    "                chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels_for_bleu, word_match=False) # chrF++\n",
    "\n",
    "                results[\"bleu\"] = round(bleu_result[\"score\"], 4)\n",
    "                results[\"chrf++\"] = round(chrf_result[\"score\"], 4)\n",
    "                # Add BLEU n-gram precisions if desired\n",
    "                # results.update({f\"bleu_{n}\": round(bleu_result['precisions'][i], 4) for i, n in enumerate([1, 2, 3, 4])})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating metrics: {e}\")\n",
    "                results[\"bleu\"] = 0.0\n",
    "                results[\"chrf++\"] = 0.0\n",
    "        else:\n",
    "             print(\"Metrics library not loaded, skipping BLEU/CHRF calculation.\")\n",
    "             results[\"bleu\"] = 0.0 # Return default\n",
    "\n",
    "        # Add generation length metric\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer_for_decode.pad_token_id) for pred in predictions]\n",
    "        results[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "        return {k: round(v, 4) for k, v in results.items()}\n",
    "    return compute_metrics\n",
    "\n",
    "# --- Fine-tuning Configuration ---\n",
    "print(\"\\n--- Setting up Fine-Tuning ---\")\n",
    "seed = train_cfg.get('TrainingArguments.seed', 42); set_seed(seed)\n",
    "val_split_ratio = train_cfg.get('DatasetArguments.validation_split_ratio', 0.1)\n",
    "start_model_path = train_cfg.get('ModelArguments.model_name_or_path') # Use the configured path\n",
    "feature_dim = train_cfg.get('ModelArguments.feature_dim', 208)\n",
    "\n",
    "# --- Experiment Name & Output Dir ---\n",
    "exp_name_parts = [\n",
    "    \"SSL_FineTune\",\n",
    "    f\"Start_{start_model_path.split('/')[-1].replace(':','-')}\", # Sanitize path separators/colons\n",
    "    f\"LR_{train_cfg.get('TrainingArguments.learning_rate', 'Def')}\",\n",
    "    f\"Epochs_{train_cfg.get('TrainingArguments.epochs', 'Def')}\",\n",
    "    f\"Seed_{seed}\"\n",
    "]\n",
    "exp_name_base = \"-\".join(exp_name_parts)\n",
    "exp_name_safe = \"\".join(c if c.isalnum() or c in ('-', '_', '.') else '_' for c in exp_name_base)\n",
    "training_output_dir_base = train_cfg.get('TrainingArguments.output_folder') # Already joined with MODEL_OUTPUT_DIR\n",
    "training_output_dir = os.path.join(training_output_dir_base, exp_name_safe)\n",
    "print(f\"Fine-tuning Output Directory: {training_output_dir}\")\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "print(f\"Loading Tokenizer from: {start_model_path}\")\n",
    "try: tokenizer = AutoTokenizer.from_pretrained(start_model_path)\n",
    "except Exception as e: print(f\"Error loading tokenizer {start_model_path}: {e}\"); exit()\n",
    "\n",
    "# --- Prepare Data Split ---\n",
    "print(\"\\n--- Preparing Train/Validation Split ---\")\n",
    "train_labels_path = train_cfg.get('DatasetArguments.train_labels_dataset_path')\n",
    "if not train_labels_path or not os.path.exists(train_labels_path): print(f\"ERROR: Train labels {train_labels_path} not found.\"); exit()\n",
    "all_train_df = pd.read_csv(train_labels_path); all_train_df['ID'] = all_train_df['ID'].astype(str)\n",
    "all_ids = all_train_df['ID'].tolist(); all_sentences = all_train_df['Translation'].tolist()\n",
    "train_ids, valid_ids, train_sentences, valid_sentences = train_test_split(all_ids, all_sentences, test_size=val_split_ratio, random_state=seed)\n",
    "print(f\"Splitting data: {len(train_ids)} train samples, {len(valid_ids)} validation samples.\")\n",
    "train_id_to_sentence = dict(zip(train_ids, train_sentences)); valid_id_to_sentence = dict(zip(valid_ids, valid_sentences))\n",
    "\n",
    "h5_path = train_cfg.get('DatasetArguments.train_dataset_path')\n",
    "all_h5_keys, h5_lengths_map = VideoDataset.preload_h5_info(h5_path)\n",
    "all_h5_keys_set = set(all_h5_keys)\n",
    "train_keys_final = [id_ for id_ in train_ids if id_ in all_h5_keys_set]\n",
    "valid_keys_final = [id_ for id_ in valid_ids if id_ in all_h5_keys_set]\n",
    "print(f\"Filtered keys vs HDF5: {len(train_keys_final)} train, {len(valid_keys_final)} validation\")\n",
    "\n",
    "# --- Instantiate Datasets ---\n",
    "print(\"\\nInstantiating Training Dataset...\")\n",
    "train_dataset = VideoDataset(\n",
    "    h5_file_path=h5_path, data_keys_to_use=train_keys_final, id_to_sentence_map=train_id_to_sentence,\n",
    "    tokenizer=tokenizer, h5_lengths_map=h5_lengths_map,\n",
    "    max_seq_length=train_cfg.get('DatasetArguments.max_sequence_length', 600),\n",
    "    max_label_length=train_cfg.get('DatasetArguments.max_label_length', 128),\n",
    "    is_test_set=False, feature_dim=feature_dim )\n",
    "\n",
    "print(\"\\nInstantiating Validation Dataset...\")\n",
    "valid_dataset = VideoDataset(\n",
    "    h5_file_path=h5_path, data_keys_to_use=valid_keys_final, id_to_sentence_map=valid_id_to_sentence,\n",
    "    tokenizer=tokenizer, h5_lengths_map=h5_lengths_map,\n",
    "    max_seq_length=train_cfg.get('DatasetArguments.max_sequence_length', 600),\n",
    "    max_label_length=train_cfg.get('DatasetArguments.max_label_length', 128),\n",
    "    is_test_set=False, feature_dim=feature_dim )\n",
    "\n",
    "\n",
    "\n",
    "# --- Initialize Model ---\n",
    "print(f\"\\nInitializing Model for Fine-tuning from: {start_model_path}\")\n",
    "model = SNLTraslationModel(\n",
    "    feature_dim=feature_dim,\n",
    "    model_name_or_path=start_model_path,\n",
    "    dropout_rate=train_cfg.get('TrainingArguments.dropout_rate', 0.1)\n",
    ")\n",
    "\n",
    "# ---> ADD THIS BLOCK <---\n",
    "# Fix missing decoder_start_token_id in the loaded config\n",
    "if model.model.config.decoder_start_token_id is None and tokenizer.pad_token_id is not None:\n",
    "    print(\"Setting decoder_start_token_id to pad_token_id...\")\n",
    "    model.model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "elif model.model.config.decoder_start_token_id is None:\n",
    "    print(\"ERROR: Cannot set decoder_start_token_id because tokenizer.pad_token_id is also None.\")\n",
    "    # Handle error - maybe exit or raise ValueError\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"Decoder_start_token_id already set: {model.model.config.decoder_start_token_id}\")\n",
    "# ---> END OF ADDED BLOCK <---\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# --- Training Arguments ---\n",
    "# ===========================================\n",
    "# Cell 6: Fine-tuning Setup and Execution\n",
    "# ===========================================\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"Sets seed for reproducibility.\"\"\"\n",
    "    random.seed(seed_value); np.random.seed(seed_value); torch.manual_seed(seed_value)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed_value)\n",
    "    transformers_set_seed(seed_value)\n",
    "\n",
    "def collate_fn_train_val(batch):\n",
    "    \"\"\"Collate function for training/validation DataLoaders.\"\"\"\n",
    "    batch = [sample for sample in batch if sample is not None] # Filter out None samples\n",
    "    if not batch: return None\n",
    "    try:\n",
    "        features = torch.stack([sample['features'] for sample in batch])\n",
    "        attention_mask = torch.stack([sample['attention_mask'] for sample in batch])\n",
    "        labels = torch.stack([sample['labels'] for sample in batch]) # Expect tensors with -100\n",
    "        return {\"features\": features, \"attention_mask\": attention_mask, \"labels\": labels}\n",
    "    except Exception as e: print(f\"Collate error: {e}\"); raise\n",
    "\n",
    "# --- Metrics ---\n",
    "# Load metrics (safer way)\n",
    "try:\n",
    "    sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
    "    chrf_metric = evaluate.load(\"chrf\")\n",
    "    metrics_loaded = True\n",
    "    print(\"Metrics 'sacrebleu' and 'chrf' loaded using 'evaluate' library.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load metrics using 'evaluate': {e}\")\n",
    "    print(\"BLEU/CHRF calculation during training/evaluation might fail.\")\n",
    "    metrics_loaded = False\n",
    "\n",
    "# Define compute_metrics function (needs tokenizer available)\n",
    "def compute_metrics_fn(tokenizer_for_decode):\n",
    "    def compute_metrics(eval_pred):\n",
    "        # eval_pred.predictions often is a tuple if multiple outputs, logits are usually first\n",
    "        logits = eval_pred.predictions[0] if isinstance(eval_pred.predictions, tuple) else eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "\n",
    "        if logits is None or labels is None:\n",
    "            print(\"Warning: Logits or labels are None in compute_metrics.\")\n",
    "            return {}\n",
    "\n",
    "        # Get predicted token ids\n",
    "        # Ensure logits is on CPU and numpy for argmax\n",
    "        if isinstance(logits, torch.Tensor):\n",
    "            logits = logits.cpu().numpy()\n",
    "        predictions_ids = np.argmax(logits, axis=-1)\n",
    "\n",
    "        # Decode predictions\n",
    "        # Replace potentially negative indices (if any) before decoding\n",
    "        predictions_ids = np.maximum(predictions_ids, 0)\n",
    "        decoded_preds = tokenizer_for_decode.batch_decode(predictions_ids, skip_special_tokens=True)\n",
    "\n",
    "        # Decode labels (handle -100)\n",
    "        labels = np.where(labels != -100, labels, tokenizer_for_decode.pad_token_id)\n",
    "        decoded_labels = tokenizer_for_decode.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Format for sacrebleu\n",
    "        decoded_labels_for_bleu = [[label] for label in decoded_labels]\n",
    "\n",
    "        results = {}\n",
    "        if metrics_loaded:\n",
    "            try:\n",
    "                # Use compute method from evaluate library\n",
    "                bleu_result = sacrebleu_metric.compute(predictions=decoded_preds, references=decoded_labels_for_bleu)\n",
    "                chrf_result = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels_for_bleu) # word_match=False is default\n",
    "\n",
    "                results[\"bleu\"] = bleu_result[\"score\"] # Main BLEU score\n",
    "                results[\"chrf\"] = chrf_result[\"score\"] # Main CHRF score (likely CHRF++)\n",
    "                # Add BLEU n-gram precisions if desired\n",
    "                # results.update({f\"bleu_{n}_prec\": bleu_result['precisions'][i] for i, n in enumerate([1, 2, 3, 4])})\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error calculating metrics: {e}\")\n",
    "                results[\"bleu\"] = 0.0\n",
    "                results[\"chrf\"] = 0.0\n",
    "        else:\n",
    "             print(\"Metrics library not loaded, skipping BLEU/CHRF calculation.\")\n",
    "             results[\"bleu\"] = 0.0 # Return default\n",
    "\n",
    "        # Add generation length metric\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer_for_decode.pad_token_id) for pred in predictions_ids]\n",
    "        results[\"gen_len\"] = np.mean(prediction_lens) if prediction_lens else 0.0\n",
    "\n",
    "        # Round results\n",
    "        return {k: round(v, 4) for k, v in results.items()}\n",
    "    return compute_metrics\n",
    "\n",
    "# --- Fine-tuning Configuration ---\n",
    "print(\"\\n--- Setting up Fine-Tuning ---\")\n",
    "seed = train_cfg.get('TrainingArguments.seed', 42); set_seed(seed)\n",
    "val_split_ratio = train_cfg.get('DatasetArguments.validation_split_ratio', 0.1)\n",
    "start_model_path = train_cfg.get('ModelArguments.model_name_or_path') # Use the configured path\n",
    "feature_dim = train_cfg.get('ModelArguments.feature_dim', 208)\n",
    "\n",
    "# --- Experiment Name & Output Dir ---\n",
    "exp_name_parts = [\n",
    "    \"SSL_FineTune\",\n",
    "    f\"Start_{start_model_path.split('/')[-1].replace(':','-')}\", # Sanitize path separators/colons\n",
    "    f\"LR_{train_cfg.get('TrainingArguments.learning_rate', 'Def')}\",\n",
    "    f\"Epochs_{train_cfg.get('TrainingArguments.num_train_epochs', 'Def')}\", # Use correct arg name\n",
    "    f\"Seed_{seed}\"\n",
    "]\n",
    "exp_name_base = \"-\".join(exp_name_parts)\n",
    "exp_name_safe = \"\".join(c if c.isalnum() or c in ('-', '_', '.') else '_' for c in exp_name_base)\n",
    "training_output_dir_base = train_cfg.get('TrainingArguments.output_folder') # Already joined with MODEL_OUTPUT_DIR\n",
    "training_output_dir = os.path.join(training_output_dir_base, exp_name_safe)\n",
    "print(f\"Fine-tuning Output Directory: {training_output_dir}\")\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "print(f\"Loading Tokenizer from: {start_model_path}\")\n",
    "try: tokenizer = AutoTokenizer.from_pretrained(start_model_path, legacy=False) # Try setting legacy=False for potential SentencePiece fix\n",
    "except Exception as e: print(f\"Error loading tokenizer {start_model_path}: {e}\"); exit()\n",
    "\n",
    "# --- Prepare Data Split ---\n",
    "print(\"\\n--- Preparing Train/Validation Split ---\")\n",
    "train_labels_path = train_cfg.get('DatasetArguments.train_labels_dataset_path')\n",
    "if not train_labels_path or not os.path.exists(train_labels_path): print(f\"ERROR: Train labels {train_labels_path} not found.\"); exit()\n",
    "all_train_df = pd.read_csv(train_labels_path); all_train_df['ID'] = all_train_df['ID'].astype(str)\n",
    "all_ids = all_train_df['ID'].tolist(); all_sentences = all_train_df['Translation'].tolist()\n",
    "train_ids, valid_ids, train_sentences, valid_sentences = train_test_split(all_ids, all_sentences, test_size=val_split_ratio, random_state=seed)\n",
    "print(f\"Splitting data: {len(train_ids)} train samples, {len(valid_ids)} validation samples.\")\n",
    "train_id_to_sentence = dict(zip(train_ids, train_sentences)); valid_id_to_sentence = dict(zip(valid_ids, valid_sentences))\n",
    "\n",
    "h5_path = train_cfg.get('DatasetArguments.train_dataset_path')\n",
    "all_h5_keys, h5_lengths_map = VideoDataset.preload_h5_info(h5_path)\n",
    "all_h5_keys_set = set(all_h5_keys)\n",
    "train_keys_final = [id_ for id_ in train_ids if id_ in all_h5_keys_set]\n",
    "valid_keys_final = [id_ for id_ in valid_ids if id_ in all_h5_keys_set]\n",
    "print(f\"Filtered keys vs HDF5: {len(train_keys_final)} train, {len(valid_keys_final)} validation\")\n",
    "\n",
    "# --- Instantiate Datasets ---\n",
    "print(\"\\nInstantiating Training Dataset...\")\n",
    "train_dataset = VideoDataset(\n",
    "    h5_file_path=h5_path, data_keys_to_use=train_keys_final, id_to_sentence_map=train_id_to_sentence,\n",
    "    tokenizer=tokenizer, h5_lengths_map=h5_lengths_map,\n",
    "    max_seq_length=train_cfg.get('DatasetArguments.max_sequence_length', 512), # Use updated value\n",
    "    max_label_length=train_cfg.get('DatasetArguments.max_label_length', 128),\n",
    "    is_test_set=False, feature_dim=feature_dim )\n",
    "\n",
    "print(\"\\nInstantiating Validation Dataset...\")\n",
    "valid_dataset = VideoDataset(\n",
    "    h5_file_path=h5_path, data_keys_to_use=valid_keys_final, id_to_sentence_map=valid_id_to_sentence,\n",
    "    tokenizer=tokenizer, h5_lengths_map=h5_lengths_map,\n",
    "    max_seq_length=train_cfg.get('DatasetArguments.max_sequence_length', 512), # Use updated value\n",
    "    max_label_length=train_cfg.get('DatasetArguments.max_label_length', 128),\n",
    "    is_test_set=False, feature_dim=feature_dim )\n",
    "\n",
    "# --- Initialize Model ---\n",
    "print(f\"\\nInitializing Model for Fine-tuning from: {start_model_path}\")\n",
    "model = SNLTraslationModel(\n",
    "    feature_dim=feature_dim,\n",
    "    model_name_or_path=start_model_path,\n",
    "    # Use dropout from TrainingArguments if specified, else default\n",
    "    dropout_rate=train_cfg.get('TrainingArguments.dropout_rate', 0.1)\n",
    ")\n",
    "\n",
    "# --- > FIX DECODER START TOKEN ID IN MODEL CONFIG < ---\n",
    "if model.model.config.decoder_start_token_id is None and tokenizer.pad_token_id is not None:\n",
    "    print(\"Setting model.config.decoder_start_token_id to pad_token_id...\")\n",
    "    model.model.config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "elif model.model.config.decoder_start_token_id is None:\n",
    "    print(\"ERROR! Cannot set decoder_start_token_id: tokenizer.pad_token_id is None.\")\n",
    "    exit() # Stop if we can't fix it\n",
    "else:\n",
    "    print(f\"Model config decoder_start_token_id already set: {model.model.config.decoder_start_token_id}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# --- Training Arguments ---\n",
    "print(\"\\nSetting Training Arguments for Fine-tuning...\")\n",
    "\n",
    "# --- > Prepare GenerationConfig < ---\n",
    "from transformers import GenerationConfig\n",
    "gen_config = GenerationConfig.from_pretrained(\n",
    "    start_model_path, # Load defaults from the starting checkpoint's config\n",
    "    # Override with specific settings for validation generation\n",
    "    max_length=train_cfg.get('DatasetArguments.max_label_length', 128),\n",
    "    num_beams=train_cfg.get('TrainingArguments.generation_num_beams', eval_cfg.get('GenerationArguments.num_beams', 4)), # Allow overriding in train_cfg or use eval_cfg\n",
    "    # Add other relevant params from eval_cfg if desired for validation loop generation\n",
    "    early_stopping=eval_cfg.get('GenerationArguments.early_stopping', True),\n",
    "    length_penalty=eval_cfg.get('GenerationArguments.length_penalty', 1.0),\n",
    "    no_repeat_ngram_size=eval_cfg.get('GenerationArguments.no_repeat_ngram_size', 0),\n",
    "    # Sample params are usually off for BLEU eval, but can be inherited/set if needed\n",
    "    do_sample = eval_cfg.get('GenerationArguments.do_sample', False),\n",
    "    top_p = eval_cfg.get('GenerationArguments.top_p', 1.0) if eval_cfg.get('GenerationArguments.do_sample') else None,\n",
    "    temperature = eval_cfg.get('GenerationArguments.temperature', 1.0) if eval_cfg.get('GenerationArguments.do_sample') else None,\n",
    "\n",
    ")\n",
    "# --- > Explicitly set the decoder_start_token_id in GenerationConfig <---\n",
    "if gen_config.decoder_start_token_id is None and tokenizer.pad_token_id is not None:\n",
    "    print(\"Setting decoder_start_token_id in GenerationConfig...\")\n",
    "    gen_config.decoder_start_token_id = tokenizer.pad_token_id\n",
    "elif gen_config.decoder_start_token_id is None:\n",
    "     print(\"ERROR! Cannot set decoder_start_token_id in GenerationConfig: tokenizer.pad_token_id is None.\")\n",
    "     exit() # Stop if we can't fix it\n",
    "\n",
    "# Filter None sample args if do_sample is false\n",
    "if not gen_config.do_sample:\n",
    "    if hasattr(gen_config, 'top_p'): delattr(gen_config, 'top_p')\n",
    "    if hasattr(gen_config, 'temperature'): delattr(gen_config, 'temperature')\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=training_output_dir,\n",
    "    # Use renamed 'eval_strategy'\n",
    "    eval_strategy=train_cfg.get('TrainingArguments.evaluation_strategy', 'epoch'),\n",
    "    logging_strategy=train_cfg.get('TrainingArguments.logging_strategy', 'epoch'),\n",
    "    save_strategy=train_cfg.get('TrainingArguments.save_strategy', 'epoch'),\n",
    "    learning_rate=train_cfg.get('TrainingArguments.learning_rate', 5e-5),\n",
    "    # Use direct arg names from config where possible\n",
    "    per_device_train_batch_size=train_cfg.get('TrainingArguments.per_device_train_batch_size', 1),\n",
    "    per_device_eval_batch_size=train_cfg.get('TrainingArguments.per_device_eval_batch_size', 4),\n",
    "    gradient_accumulation_steps=train_cfg.get('TrainingArguments.gradient_accumulation_steps', 4),\n",
    "    num_train_epochs=train_cfg.get('TrainingArguments.num_train_epochs', 20),\n",
    "    weight_decay=train_cfg.get('TrainingArguments.weight_decay', 0.01),\n",
    "    fp16=train_cfg.get('TrainingArguments.fp16', True) and torch.cuda.is_available(),\n",
    "    gradient_checkpointing=train_cfg.get('TrainingArguments.gradient_checkpointing', True),\n",
    "    optim=train_cfg.get('TrainingArguments.optim', 'adafactor'),\n",
    "    load_best_model_at_end=train_cfg.get('TrainingArguments.load_best_model_at_end', True),\n",
    "    metric_for_best_model=train_cfg.get('TrainingArguments.metric_for_best_model', 'eval_bleu'),\n",
    "    greater_is_better=train_cfg.get('TrainingArguments.greater_is_better', True),\n",
    "    predict_with_generate=train_cfg.get('TrainingArguments.predict_with_generate', True),\n",
    "    # Pass the GenerationConfig object\n",
    "    generation_config=gen_config,\n",
    "    report_to=\"none\", # Set Wandb arguments if needed\n",
    "    seed=seed,\n",
    "    save_total_limit=train_cfg.get('TrainingArguments.save_total_limit', 2),\n",
    "    dataloader_num_workers=train_cfg.get('TrainingArguments.dataloader_num_workers', 0),\n",
    ")\n",
    "\n",
    "# --- Initialize Trainer ---\n",
    "print(\"\\nInitializing Seq2SeqTrainer...\")\n",
    "# Pass the tokenizer to the compute_metrics function factory\n",
    "compute_metrics_with_tokenizer = compute_metrics_fn(tokenizer)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    tokenizer=tokenizer, # Pass tokenizer for padding/saving\n",
    "    data_collator=collate_fn_train_val,\n",
    "    compute_metrics=compute_metrics_with_tokenizer # Pass the metrics function\n",
    ")\n",
    "\n",
    "# --- Start Fine-tuning ---\n",
    "print(f\"\\n--- Starting Fine-tuning ---\")\n",
    "best_model_checkpoint_path = None # Initialize in case training fails\n",
    "try:\n",
    "    train_result = trainer.train(\n",
    "        resume_from_checkpoint=train_cfg.get('ModelArguments.resume_checkpoint') if train_cfg.get('ModelArguments.resume') else None\n",
    "    )\n",
    "    print(\"Fine-tuning Finished.\")\n",
    "\n",
    "    # --- Save Final Model, State, and Metrics ---\n",
    "    print(\"\\nSaving final best model and tokenizer...\")\n",
    "    # Trainer should have loaded the best model if load_best_model_at_end=True\n",
    "    # The path to the best checkpoint is stored in trainer state\n",
    "    best_model_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "    final_save_path = os.path.join(training_output_dir, \"best_finetuned_checkpoint\")\n",
    "\n",
    "    if best_model_checkpoint_path:\n",
    "         print(f\"Best checkpoint identified at: {best_model_checkpoint_path}\")\n",
    "         # If load_best_model_at_end was true, trainer.model is already the best one\n",
    "         # We just need to save it to our designated final location\n",
    "         trainer.save_model(final_save_path)\n",
    "         print(f\"Best fine-tuned model saved to: {final_save_path}\")\n",
    "         # Also save tokenizer and generation config there\n",
    "         if hasattr(trainer, 'tokenizer') and trainer.tokenizer:\n",
    "             trainer.tokenizer.save_pretrained(final_save_path)\n",
    "             print(f\"Tokenizer saved to: {final_save_path}\")\n",
    "         if hasattr(model.model, 'generation_config'): # Save the generation config used\n",
    "             model.model.generation_config.save_pretrained(final_save_path)\n",
    "             print(f\"Generation config saved to: {final_save_path}\")\n",
    "    else:\n",
    "         print(\"No best checkpoint found (or load_best_model_at_end=False). Saving final model state instead.\")\n",
    "         trainer.save_model(final_save_path) # Save whatever the final state is\n",
    "\n",
    "    trainer.save_state() # Saves trainer state (like optimizer) in the main output dir\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    print(\"\\n--- Final Fine-tuning Metrics (from train_result) ---\")\n",
    "    print(metrics)\n",
    "    if trainer.state.best_metric is not None:\n",
    "        print(f\"Best validation {trainer.args.metric_for_best_model}: {trainer.state.best_metric:.4f} at step {trainer.state.best_model_checkpoint.split('-')[-1] if trainer.state.best_model_checkpoint else 'N/A'}\")\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics) # Saves to train_results.json\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n--- Fine-tuning Interrupted or Failed: {e} ---\"); import traceback; traceback.print_exc()\n",
    "    # Attempt to get best checkpoint path even if training failed mid-epoch after saving one\n",
    "    if trainer.state.best_model_checkpoint:\n",
    "         best_model_checkpoint_path = trainer.state.best_model_checkpoint\n",
    "         print(f\"(Attempting to use best checkpoint found before interruption: {best_model_checkpoint_path})\")\n",
    "    else:\n",
    "         print(\"(No best checkpoint identified before interruption)\")\n",
    "\n",
    "\n",
    "# Clean up memory before final evaluation\n",
    "print(\"\\nCleaning up training objects...\")\n",
    "# We need best_model_checkpoint_path for Cell 7, keep it\n",
    "del model\n",
    "del trainer\n",
    "del train_dataset\n",
    "del valid_dataset\n",
    "gc.collect()\n",
    "if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n--- Fine-tuning Phase Complete ---\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# Cell 7: Final Evaluation on Test Set\n",
    "# ===========================================\n",
    "print(\"\\n--- Setting up Evaluation on ACTUAL TEST SET ---\")\n",
    "\n",
    "# --- Load Best Fine-tuned Model ---\n",
    "if best_model_checkpoint_path and os.path.isdir(best_model_checkpoint_path):\n",
    "    print(f\"Loading best fine-tuned model for final test evaluation from: {best_model_checkpoint_path}\")\n",
    "    # Pass eval_cfg just for structure/defaults, actual config loaded from checkpoint path\n",
    "    model_eval, tokenizer_eval = load_trained_model_for_eval(eval_cfg, best_model_checkpoint_path)\n",
    "\n",
    "    # --- Load ACTUAL Test Dataset ---\n",
    "    print(\"\\nLoading ACTUAL Test Dataset...\")\n",
    "    test_h5_path_eval = eval_cfg.get('DatasetArguments.test_dataset_path')\n",
    "    test_labels_path_eval = eval_cfg.get('DatasetArguments.test_labels_dataset_path')\n",
    "\n",
    "    # Check if REAL test labels file exists\n",
    "    test_labels_exist_eval = False\n",
    "    test_id_to_sentence_map_eval = {}\n",
    "    if test_labels_path_eval and os.path.exists(test_labels_path_eval):\n",
    "        try:\n",
    "            test_df_eval = pd.read_csv(test_labels_path_eval)\n",
    "            test_df_eval['ID'] = test_df_eval['ID'].astype(str)\n",
    "            test_id_to_sentence_map_eval = {row[\"ID\"]: row[\"Translation\"] for _, row in test_df_eval.iterrows()}\n",
    "            test_labels_exist_eval = True\n",
    "            print(f\"Actual test labels file found and loaded: {test_labels_path_eval}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Found test labels file ({test_labels_path_eval}) but couldn't read: {e}. Cannot calculate BLEU.\")\n",
    "            test_labels_exist_eval = False\n",
    "    else:\n",
    "        print(f\"Actual test labels file not found or specified ({test_labels_path_eval}). Cannot calculate BLEU.\")\n",
    "\n",
    "    # Preload HDF5 info for the TEST file\n",
    "    test_h5_keys, test_h5_lengths_map = VideoDataset.preload_h5_info(test_h5_path_eval)\n",
    "    if not test_h5_keys: print(\"ERROR: Could not load keys from test HDF5 file.\"); exit()\n",
    "\n",
    "    # Filter test keys (include all HDF5 keys if no labels exist)\n",
    "    if test_labels_exist_eval:\n",
    "         test_keys_final = [id_ for id_ in test_h5_keys if id_ in test_id_to_sentence_map_eval]\n",
    "         print(f\"Using {len(test_keys_final)} keys present in both test HDF5 and test labels CSV.\")\n",
    "    else:\n",
    "         test_keys_final = test_h5_keys # Use all keys from HDF5 if no labels\n",
    "         print(f\"Using all {len(test_keys_final)} keys found in test HDF5 (no labels provided/loaded).\")\n",
    "\n",
    "\n",
    "    # Use sequence length consistent with training\n",
    "    max_seq_len_eval = train_cfg.get('DatasetArguments.max_sequence_length', 600)\n",
    "\n",
    "    test_dataset_eval = VideoDataset(\n",
    "        h5_file_path=test_h5_path_eval,\n",
    "        data_keys_to_use=test_keys_final,\n",
    "        id_to_sentence_map=test_id_to_sentence_map_eval,\n",
    "        tokenizer=tokenizer_eval, # Use tokenizer loaded from best checkpoint\n",
    "        h5_lengths_map=test_h5_lengths_map,\n",
    "        max_seq_length=max_seq_len_eval,\n",
    "        max_label_length=eval_cfg.get('GenerationArguments.max_length', 128),\n",
    "        is_test_set=True, # Returns strings for labels if they exist\n",
    "        feature_dim=train_cfg.get('ModelArguments.feature_dim', 208) # Match feature dim used in training\n",
    "    )\n",
    "\n",
    "    # --- Create DataLoader ---\n",
    "    eval_batch_size = eval_cfg.get('EvaluationArguments.batch_size', 8)\n",
    "    eval_num_workers = eval_cfg.get('EvaluationArguments.dataloader_num_workers', 0)\n",
    "    test_loader_eval = DataLoader(\n",
    "        test_dataset_eval, batch_size=eval_batch_size, collate_fn=collate_fn_eval,\n",
    "        shuffle=False, num_workers=eval_num_workers )\n",
    "    print(f\"\\nCreated Test DataLoader: {len(test_dataset_eval)} samples, Batch size: {eval_batch_size}\")\n",
    "\n",
    "    # --- Run Evaluation Loop on Test Set ---\n",
    "    print(\"\\n--- Starting Evaluation Loop on TEST SET ---\")\n",
    "    keys_list_test = []; predictions_test = []; references_test = []\n",
    "\n",
    "    # Use generation settings from eval config\n",
    "    gen_args_config_eval = eval_cfg.get('GenerationArguments', Config({}))\n",
    "    generation_config_test = {\n",
    "        \"max_length\": gen_args_config_eval.get('max_length', 128),\n",
    "        \"num_beams\": gen_args_config_eval.get('num_beams', 4),\n",
    "        \"early_stopping\": gen_args_config_eval.get('early_stopping', True),\n",
    "        \"length_penalty\": gen_args_config_eval.get('length_penalty', 1.0),\n",
    "        \"no_repeat_ngram_size\": gen_args_config_eval.get('no_repeat_ngram_size', 0),\n",
    "        \"do_sample\": gen_args_config_eval.get('do_sample', False),\n",
    "        \"top_p\": gen_args_config_eval.get('top_p', 1.0) if gen_args_config_eval.get('do_sample') else None,\n",
    "        \"temperature\": gen_args_config_eval.get('temperature', 1.0) if gen_args_config_eval.get('do_sample') else None,\n",
    "    }\n",
    "    if not generation_config_test.get(\"do_sample\"):\n",
    "         generation_config_test = {k: v for k, v in generation_config_test.items() if k not in [\"top_p\", \"temperature\"]}\n",
    "    print(f\"Test Generation config: {generation_config_test}\")\n",
    "\n",
    "    model_eval.eval() # Ensure model is in eval mode\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader_eval, desc=\"Evaluating on Test Set\"):\n",
    "            if batch is None: continue\n",
    "            try:\n",
    "                output_sequences = model_eval.generate(\n",
    "                    features=batch[\"features\"].to(device), # Ensure data on correct device\n",
    "                    attention_mask=batch[\"attention_mask\"].to(device),\n",
    "                    **generation_config_test )\n",
    "                # Move back to CPU for decoding if needed, though decode handles it\n",
    "                batch_preds = tokenizer_eval.batch_decode(output_sequences.cpu(), skip_special_tokens=True)\n",
    "                keys_list_test.extend(batch[\"keys\"])\n",
    "                predictions_test.extend(batch_preds)\n",
    "                if test_labels_exist_eval: references_test.extend(batch[\"labels\"])\n",
    "            except Exception as e:\n",
    "                 print(f\"\\nError during test set generation: {e}\"); import traceback; traceback.print_exc()\n",
    "                 error_count = len(batch[\"keys\"]); keys_list_test.extend(batch[\"keys\"])\n",
    "                 predictions_test.extend([\"<GENERATION_ERROR>\"] * error_count)\n",
    "                 if test_labels_exist_eval: references_test.extend(batch[\"labels\"])\n",
    "\n",
    "    print(\"\\n--- Test Set Evaluation Loop Finished ---\")\n",
    "\n",
    "    # --- Save Test Set Results ---\n",
    "    # Ensure results_df_test is created even if the loop had issues\n",
    "    if not keys_list_test:\n",
    "        print(\"WARNING: No keys were processed during test set evaluation. Cannot save results.\")\n",
    "    else:\n",
    "        results_df_test = pd.DataFrame({'ID': keys_list_test, 'Prediction': predictions_test})\n",
    "        if test_labels_exist_eval and references_test and len(references_test) == len(predictions_test):\n",
    "            results_df_test['Reference'] = references_test\n",
    "            print(f\"Added Reference column to test results (Total samples: {len(results_df_test)})\")\n",
    "\n",
    "        eval_results_save_dir = eval_cfg.EvaluationArguments.results_save_path\n",
    "        # Use the fine-tuning experiment name for the results file\n",
    "        prediction_save_file_test = os.path.join(eval_results_save_dir, f\"{exp_name_safe}_TEST_predictions.csv\")\n",
    "        print(f\"\\nSaving test set predictions to: {prediction_save_file_test}\")\n",
    "        try:\n",
    "            results_df_test.to_csv(prediction_save_file_test, encoding='utf-8', index=False)\n",
    "            print(\"Test predictions saved successfully.\")\n",
    "        except Exception as e: print(f\"Error saving test predictions: {e}\")\n",
    "\n",
    "        if not results_df_test.empty: print(\"\\nSample Test Predictions:\\n\", results_df_test.head())\n",
    "\n",
    "    # --- Calculate Metrics on Test Set (if references exist) ---\n",
    "    if test_labels_exist_eval and references_test and len(references_test) == len(predictions_test) and metrics_loaded:\n",
    "        print(\"\\n--- Calculating Final Metrics on TEST SET ---\")\n",
    "        references_sacrebleu_test = [[ref] for ref in references_test]\n",
    "        try:\n",
    "            bleu_result_test = bleu_metric.compute(predictions=predictions_test, references=references_sacrebleu_test)\n",
    "            chrf_result_test = chrf_metric.compute(predictions=predictions_test, references=references_sacrebleu_test, word_match=False)\n",
    "\n",
    "            print(f\"\\nCorpus BLEU Score (Test Set): {bleu_result_test['score']:.4f}\")\n",
    "            print(f\"Corpus CHRF++ Score (Test Set): {chrf_result_test['score']:.4f}\")\n",
    "\n",
    "            metrics_summary_test = {\n",
    "                \"BLEU\": round(bleu_result_test[\"score\"], 4),\n",
    "                \"CHRF++\": round(chrf_result_test[\"score\"], 4),\n",
    "            }\n",
    "            print('=' * 30); print(\"Detailed BLEU Scores (Test Set):\"); print('-' * 10)\n",
    "            for i, n in enumerate([1, 2, 3, 4]):\n",
    "                 prec = bleu_result_test['precisions'][i]\n",
    "                 metrics_summary_test[f\"BLEU-{n}\"] = round(prec, 4)\n",
    "                 print(f\"BLEU-{n} Precision: {prec:.2f}\") # Note: Sacrebleu gives precisions, not full BLEU-n scores directly in dict\n",
    "            print('=' * 30)\n",
    "\n",
    "            metrics_save_file_test = os.path.join(eval_results_save_dir, f\"{exp_name_safe}_TEST_metrics.json\")\n",
    "            print(f\"Saving test metrics summary to: {metrics_save_file_test}\")\n",
    "            with open(metrics_save_file_test, 'w', encoding='utf-8') as f: json.dump(metrics_summary_test, f, indent=2)\n",
    "            print(\"Test metrics saved.\")\n",
    "        except Exception as e: print(f\"Error calculating test metrics: {e}\")\n",
    "    else:\n",
    "        print(\"\\nSkipping final metric calculation for test set.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping final evaluation on test set as the best fine-tuned checkpoint was not found.\")\n",
    "\n",
    "print(\"\\n--- Full Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deacc08-1117-44c4-9392-4d3e1724971e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch_cuda]",
   "language": "python",
   "name": "conda-env-pytorch_cuda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
